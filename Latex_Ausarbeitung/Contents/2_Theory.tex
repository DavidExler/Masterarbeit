% !TeX root = ../Thesis.tex

\chapter{Theory}\label{ch:Theory}
%This chapter should introduce to the theoretical background of your thesis. Any (already existing) method you use to obtain the results later should be introduced and explained. The mathematics needed to understand the possibly new methods in the next chapter is also part of this chapter. 

%https://www.ebi.ac.uk/biostudies/BioImages/studies/S-BIAD1518?query=3D%20nuclei%20segmentation

\section{Überblick}
Im nachfolgenden Kapitel wird der theoretische Hintergrund der vorliegenden Thesis behandelt. 
Hierzu werden sowohl die Methoden verwandter Projekte und Studien, als auch die Literatur zum aktuellen Stand der Technik beleuchtet. 
Dem theoretischen Hintergrund wird der Neuheitswert der Arbeit gegenübergestellt, um den Beitrag des vorliegenden Projekts zur Forschung zu verdeutlichen.

\section{Methoden}
\subsection{Unfertiger Zwischendurch-Text!}

\subsection{Benchmark}
Um die Leistungsfähigkeit der Applikation, die im Rahmen der vorliegenden Thesis erstellt wird, zu validieren und verifizieren sind umfangreiche Datensätze notwendig. 
Für jede isolierbare Aufgabe muss ein Datensatz gewählt werden, der eine \ac{gt} entsprechend dieser Aufgabe mitbringt.
Die in diesem Datensatz enthaltenen Daten (Quelldaten) müssen dabei den Daten, für die die Anwendung entworfen wird (Zieldaten) \textit{ähnlich} sein.
So wird sichergestellt, dass sich die auf den Quelldaten gemessene Qualität der Anwendung auf die Zieldaten übertragen lässt \cite{ganin2015domain_big}.
Der Begriff \textit{Ähnlichkeit} ist mehrdeutig und es ist eine Herrausforderung, sinnvolle Merkmale in Daten zu finden, die das Messen von \textit{Ähnlichkeit} ermöglichen.
Metriken die als Maß für \textit{Ähnlichkeit} genutzt werden müssen maßgeschneidert zur Anwendung passen und sind bereits breit erforscht \cite{zhu2020domain_similarity, koohi2018similarity, cai2009similarity}.
Daten können mithilfe passender Metriken \textit{Ähnlichkeits-}gruppen, sogenannten \textit{Domänen}, zugeordnet werden \cite{yuan2005domainsimilarity}.
Beispiele für Domänenunterschiede in biomedizinischen Bildaufnahmen sind verschiedene Marker, Aufnahmegeräte oder Zoom-Stufen (siehe Abb. \ref{fig:domains}). 
%sind verschiedene Darstellungen gleicher Objekte in Bildern wie in Fig. \ref{fig:domains} dargestellt. 
\newline Altes Bild (Katzen) ersetzt \newline
%\begin{figure}[htbp]
%    \centering
%    \includegraphics[width=0.18\textwidth]{Figures/Katze_0.jpg} \hfill
%    \includegraphics[width=0.24\textwidth]{Figures/Katze_1.jpg}\hfill
%    \includegraphics[width=0.24\textwidth]{Figures/Katze_3.jpg}\hfill
%    \includegraphics[width=0.24\textwidth]{Figures/Katze_2.jpg}
%    \caption{Vier Bilder der Klasse \textit{Katze} aus verschiedenen Bilddomänen. Von Links nach Rechts steigt der wahrgenommene Abstraktionsgrad \cite{jrfarm_katze, Egan_Katzen}.}
%    \label{fig:domains}
%\end{figure}
%\begin{figure}[htbp]
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.24\textwidth]{Figures/Cell_domain0.jpg} \hfill
    \includegraphics[width=0.24\textwidth]{Figures/Cell_domain1.jpg}\hfill
    \includegraphics[width=0.24\textwidth]{Figures/Cell_domain2.jpg}\hfill
    \includegraphics[width=0.24\textwidth]{Figures/Cell_domain3.jpg}
    \caption{Vier Aufnahmen von Zellen. Die Bilddomänen sind durch die verschiedenen Marker, Aufnahmegeräte und Zoom-Stufen klar zu unterscheiden \cite{Schermelleh2008, Peng2015, Ali2012}.}
    \label{fig:domains}
\end{figure}
Intuitiv gehören die sichtbaren Objekte zur Klasse \textit{Katze}, aber die Merkmale wie Detailgrad, Abstraktion und Stil unterscheiden sich stark. 
Sie unterscheiden sich also in ihrer Domäne.
In der Bildverarbeitung ist es essenziell die Domäne der Quelldaten im Hinblick auf die Aufgabe der Applikation zu beachten \cite{wang2018domain, peng2017domain}. 
Hierzu kann ein passender Datensatz gewählt werden oder eine Domänenanpassung durchgeführt werden \cite{han2022binsimilarity_domain, pinheiro2018domain, ganin2016domain}. \textit{NOTIZ: Muss ich Domänenanpassung erklären?}
\subsection{Segmentierung}\label{sec:Segmentierung}
%\textbf{KOMMENTAR} ist das notwendig?
\begin{itemize}
    \item Was ist Segmentierung?
    \item Welche klassischen Ansätze gibt es?
    \item Welche KI-Lösungen gibt es?
    %\item \textit{(Hier oder im nächsten Kapitel)} Welche Foundationmodels bzw Backbone/Pyramid Kombinationen gibt es?
\end{itemize}
Segmentierung ist die Aufgabe, Pixel mit semantischen Labels zu klassifizieren (semantische Segmentierung \cite{Hariharan2014}), einzelne Objekte voneinander abzugrenzen (Instanzsegmentierung \cite{Winn2006}) oder beides zu kombinieren (panoptische Segmentierung \cite{kirillov2019PQ}) \cite{Minaee2021}.
In Fig. \ref{fig:Segmentation} \cite{kirillov2019PQ} sind beispielhaft \acp{gt} der verschiedenen Arten von Segmentierung zu sehen.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{Figures/SegmentationOriginal.png}
        \caption{Original}
    \end{subfigure} \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{Figures/Semantic.png}
        \caption{Semantisch}
    \end{subfigure} \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{Figures/Instance.png}
        \caption{Instanz}
    \end{subfigure} \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{Figures/Panoptic.png}
        \caption{Panoptisch}
    \end{subfigure}
    \caption{Die verschiedene Arten der Segmentierung. Links ist das Originalbild zu sehen, rechts folgend sind die zugeordneten pixelweise Labels farblich eingezeichnet. 
    Gleiche Farben bedeuten gleiche Labels. 
    In der semantischen Maske sind gleiche Labels mehrerer Objekte von semantisch gleichen Klassen zu finden. 
    In der Instanz Maske ist jedem Objekt ein individuelles Label zugeordnet, ungeachtet der Klasse des Objekts. 
    In der panoptischen Maske sind auch einzelne Objekte getrennt, den Labels verschiedener Klassen werden allerdings noch semantische Klassen zugeordnet \cite{kirillov2019PQ}.}
    \label{fig:Segmentation}
\end{figure}
Für die verschiedenen Segmentierungsarten werden Architekturen an die Aufgabe angepasst \cite{Arnab2017, Chen2018}. 
Modelle sind dabei in der Regel nach dem Vorbild des \textit{U-Net} \cite{Ronneberger2015} aus einem Merkmalsextraktor (Encoder) und einem Vorhersagenetz (Decoder) aufgebaut \cite{Girshick2014}.
Der Encoder nutzt beispielsweise Bildfaltungen mit Kernen, deren optimale Gewichte anhand von annotierten Daten gelernt werden, zur Merkmalsextraktion \cite{Long2015}.
Dabei verringert der Encoder iterativ die Größe der Eingabe jeder Schicht des Netzes in X und Y Richtung, erhöht dabei aber die Informationstiefe pro verbleibendem Pixel, bis ein hochdimensionaler Merkmalsvektor übrig bleibt. 
Der Decoder hebt, meist durch transponierte Bildfaltungen \cite{Zeiler2010, Zeiler2014}, die räumliche Auflösung schrittweise wieder an, indem er die Bilddimensionen vergrößert und die Merkmalskanäle gleichzeitig reduziert \cite{Noh2015}. 
Über sogenannte Skip-Connections \cite{Ronneberger2015} werden dabei Merkmale aus den entsprechenden Encoder-Schichten mit den Decoder-Stufen verknüpft, sodass sowohl globale Kontextinformationen als auch feine Strukturen für die Segmentierung erhalten bleiben \cite{Mostajabi2015, Gu2018}. 
%Long2015 - semantische Segmentierung - FCN
%Ronneberger2015 - U-Net
%Arnab2017 - Pixelwise instance segmentation with a dynamically instantiated network
%Chen2018 - Masklab: {Instance} segmentation by refining object detection with semantic and direction features
%%   Hariharan2014 - Simultaneous detection and segmentation
\subsection{Klassifikation}\label{sec:Klassifikation}
\begin{itemize}
    \item Was ist Klassifikation generell?
    \item Was ist Klassifikation durch panoptische Segmentierung?
    \item Welche klassischen Ansätze gibt es? 
    \item Welche KI-Lösungen gibt es?
    %\item 
\end{itemize}
Klassifikation beschreibt das Zuordnen einer Kategorie oder Klasse zu der eine gegebene Stichprobe gehört \cite{Fukunaga1993}.
Hierzu werden die Merkmale des Objekts, das in der Stichprobe präsentiert wird, durch Beobachtung oder Messung gewonnen \cite{Kulkarni1998}.
Nach wiederholter Extraktion der Merkmale von Objekten verschiedener Klassen werden Muster in den Merkmalen gesucht, um Regeln für die Zuweisung von Objekten zu Klassen auf Basis der Muster festzulegen \cite{Jain1999, ShalevShwartz2014}.
Sowohl die Algorithmen zur Merkmalsextraktion, als auch zum Ableiten der Muster und Regeln können mit unterschiedlich hohem Rechenaufwand, Abstraktionsgrad und Maß an Generalisierbarkeit implementiert werden \cite{Loog2018}. 
Zur Merkmalsextraktion werden klassisch Metriken zu einem Vektor kombiniert \cite{Guyon2006}.
Diese Metriken können beispielsweise die Verteilung der Farbkanäle, eine Charakterisierung der Textur, die Fläche des Objekts und vieles mehr sein \cite{Kunaver2005, Mutlag2020}.
Eine weitere verbreitete Metrik ist eine Kombination von Parametern der Fourier-Entwicklung einer Kontur, die aus der diskreten komplexen Zahlenfolge
\begin{equation}
  c[n] = x[n] + i \cdot y[n]
\end{equation}
mithilfe der diskreten Fourier-Transformation 
\begin{equation}
  F[k] = \sum_{n=0}^{N-1} c[n] \cdot e^{-2 \pi i \frac{kn}{N}}
\end{equation} gebildet werden \cite{Zahn1972, Kuhl1982}.
Merkmalsvektoren werden häufig abstrahiert und in ihrer Dimension reduziert, beispielsweise durch eine Principal Component Analysis \cite{Pearson1901, Hotelling1933}
Sind keine Beispiele von Stichproben der verfügbaren Klassen verfügbar, werden diese Metriken zum Clustering verwendet \cite{Jain1999, Xu2005}. 
Sind nur wenige \acp{gt} vorhanden können semi supervised Verfahren angewandt werden, die besonders die Ähnlichkeit der Stichproben ohne Labels herausarbeiten \cite{Boser1992, Yarowsky1995}.
Eine pominente Methode des semi supervised Lernens ist das label spreading, die mithilfe einer Kernfunktion \cite{Schoelkopf1997, Smola1998} die Dimension von Merkmalsvektoren ändert und in einen alternativen Merkmalsraum transformiert \cite{Zhou2003}.
Verschiedene Kernfunktionen wie Radiale Basis Funktionen \cite{Lowe1988}
\begin{equation}
    \phi(x, y) = \exp\!\left(-\gamma \|x - y\|^2\right), \quad \gamma > 0
    %\phi(\mathbf{x}) = \exp\Big(-\frac{\|\mathbf{x}-\mathbf{c}\|^2}{2\sigma^2}\Big)
\end{equation}
werden für das label spreading eingesetzt \cite{Delalleau2005}.
Die meist genutzten Methoden der Klassifikation sind Logik-basierte Ansätze wie Entscheidungsbäume, Perzeptron-basierte Ansätze wie Neuronale Netze, statistische Ansätze wie Bayes'sche Netzwerke oder Nächster-Nachbar Verfahren und Support Vector Maschinen \cite{Kotsiantis2007}.
Diese Methoden basieren direkt auf Ähnlichkeiten zwischen den Metriken von unbekannten Stichproben und Stichproben mit bekannter Klasse \cite{Hughes1968}.
Moderne Anwendungen nutzen zur Merkmalsextraktion verschiedene Deep Learning basierte Methoden \cite{Khan2023}.
%Recurrent Neural Networks \cite{Lecun1998} und vor allem \acp{cnn} \cite{Krizhevsky2012} und \acp{vit} \cite{Bain2021} sind in der Lage aus Bildern aussagekräftige, abstrakte Merkmale zu generieren \cite{Plested2022}.
Vor allem \acp{cnn} \cite{Krizhevsky2012} und \acp{vit} \cite{Bain2021} sind in der Lage aus Bildern aussagekräftige, abstrakte Merkmale zu generieren \cite{Plested2022}.
Ein Netz, das zur Merkmalsextraktion eingesetzt wird, wird als \textbf{Encoder} bezeichnet.
Als \textbf{Decoder} wird der klassifizierende Teil des Klassifikators bezeichnet.
Der State-of-the-Art für den Decoder ist ein Neuronales Netz, das auf Basis der abstrakten Merkmale des Encoder eine Zuversichtlichkeit für jede Klasse ausgibt \cite{Ghods2019}.
Hierzu lernt in der Regel ein Multi-Layer-Perzeptron auf Basis von Trainingsdaten mit zugehöriger \ac{gt} den Zusammenhang zwischen den Merkmalen und der assoziierten Klasse \cite{Schmidhuber2015}.\newline.
%Diesen Zus in Gewichtungen von der Ausgabe des Encoders und nicht-linearen Aktivierungsfunktionen auszudrücken 
%Yuan2022 - Reinforcement Learning
%Bain2021 - ViT Encoder
%Krizhevsky2012- ImageNet CNN
%Lecun1998 - BASE RNN
%Elngar2021 - CNN Survey - unsupervised/supervised
%Optionen für klassifikatoren: \begin{verbatim}
%    https://monai.io/model-zoo.html#/model/pathology_nuclei_classification 
%    pre-trained model for classifying nuclei cells as the following types - Other - Inflammatory - Epithelial - Spindle-Shaped
%    This model is trained using DenseNet121 over ConSeP dataset.
%    
%    https://monai.io/model-zoo.html#/model/hf_exaonepath-crc-msi-predictor
%    MSI classification of CRC tumors using EXAONEPath 1.0.0 Patch-level Foundation Model for Pathology
%
%    https://monai.io/model-zoo.html#/model/hf_exaonepath
%    EXAONEPath 1.0 Patch-level Foundation Model for Pathology
%
%    https://github.com/BMEII-AI/RadImageNet
%\end{verbatim} 
\section{Literaturrecherche}

\subsection{Benchmark}
Da das manuelle Erstellen der \ac{gt} für die Segmentierung von Zelldaten mit erheblichem manuellem Aufwand verbunden ist und zusätzlich Expertenwissen voraussetzt, sind Datensätze hierfür selten. 
Einige prominente Datensätze, deren Domänen zu den Zieldaten der Anwendung dieser Arbeit \textit{ähnlich} sind, sind:  
\begin{itemize}
    \item LiveCell \cite{edlund2021livecell}, ein manuell annotierter und Experten-validierter Datensatz aus 5,239 2D-Bildern. 
    Die Daten sind mit incucyter HD Phasenkontrastmikroskopie gesammelt und enthalten 1,686,352 individuelle Zellen von acht verschiedenen Zelltypen.
    \item YeaZ \cite{dietler2020YeaZ}, ein zweiteiliger Datensatz von Hefe Zellen aus 87 Phasenkontrast-Bildern mit insgesamt 10,422 Zellen und 614 Hellfeld-Bildern mit insgesamt 3,841 Zellen in 6 Beleuchtungsstufen aufgenommen. Die Annotationen sind semi-manuell erstellt, da die Phasenkontrast-Bilder manuell, und die Lichtfeld-Bilder aus den Phasenkonstrast-Segmentierungsmasken annotiert wurden. 
    \item DeepBas \cite{holden2021deepbacs, cspahn2021deepbacs}, ein Datensatz von \textit{B. subtilis strain SH130} Bakterien. Er besteht aus Weitfeldaufnahmen (Fluoreszenz), aufgenommen mit einem inversen Mikroskop, bestehend aus sieben manuell annotierten Bildern mit je zwischen 46 und 335 Zellen.
    \item die Cell Tracking Challenge \cite{ulman2017cellTrackingChallenge}, eine Sammlung aus 13 Datensätzen verschiedener Mikroskopiemodalitäten, die sich zum Messen der Segmentierungs- und Verfolgungsfähigkeiten für verschiedene Zelltypen eignen.
    \item MoNuSeg \cite{kumar2017MoNuSeg}, eine Zusammenstellung manuell annotierter Gewebeschnitte von sieben verschiedenen Organen. Über 21,000 Zellen sind pro Bild in den 30 Bildern mit verschiedenen Färbungen und Aufnahmetechniken verteilt.
    \item TissueNet \cite{greenwald2022Tissuenet} ein umfassender Datensatz mit über 1,000,000 Zellen mit diversen Gewebearten und unterschiedlichen Aufnahmetechniken.
    \item S-BIAD1518 \cite{Kromp2020_Dataset, chen20223_Dataset}, ein Datensatz der neben manuell annotierten Bildern von acht verschiedenen Zellarten sind synthetisch erzeugte Daten enthält. Mit Hilfe von SpCycleGAN \cite{fu2018cycleGAN} wurden dazu auf Basis von simulierten \ac{gt}s Bilder generiert, die anstreben die Merkmale der realen Bilder zu reproduzieren. Es handelt sich hierbei um 3D-multispektrale Daten, aufgenommen mittels Fluoreszenzbildgebung. 
\end{itemize}
Aufgrund des geringen Volumen frei zugänglicher Daten sind diese Sammlungen auch für das Training von Segmentierungsnetzen begehrt. 
Jeder Datensatz, der bereits im Trainingssatz eines gewählten Segmentierungsnetzes enthalten war eignet sich nicht mehr zur unabhängigen Bewertung der Netze. 
Neben annotierten Datensätzen bietet die Literatur auch Methoden zum eigenständigen Erzeugen domänenspezifischer Datensätze \cite{Abay2019, Raghunathan2021, Nikolenko2021, Choi2017, Lu2023}.
Das cell synthesis Projekt \cite{bruch2025} ermöglicht das synthetische Generieren von 3D-Trainingsdaten mit realistischer Zellform und -ausrichtung und umgebenden Markern. 
Zudem umfasst das Framework eine Trainingsroutine, um ein \ac{gan} zu trainieren, das Bilddaten und passende Annotationen automatisch erzeugt und an eine gewünschte Bilddomäne anpasst. 
%
%\begin{itemize}
%    \item \cite{zhu2020domain_similarity} Speech recognition - Herrausforderung der Anpassung an Audio domänen
%    \item \cite{han2022binsimilarity_domain} Domain adaption für Bilder (in domain "bins")
%    \item \cite{koohi2018similarity} Maß für Ähnlichkeit in Workflows
%    \item \cite{cai2009similarity} Maß für Ähnlichkeit in Netzwerken
%    \item \cite{wang2018domain}
%    \item \cite{peng2017domain}
%    \item \cite{pinheiro2018domain}
%    \item \cite{ganin2015domain_big}
%    \item \cite{yuan2005domainsimilarity}
%    \item \cite{ganin2016domain}
%\end{itemize}

\subsection{Segmentierungsnetz}
\textit{Foundation-models} sind für viele moderne \ac{ki}-Anwendungen unerlässlich \cite{bommasani2021foundationmodels}. 
Sie werden zunächst für allgemeine Aufgaben vortrainiert und anschließend auf spezifische Anwendungen angepasst (\textit{fine-tuning}), meist unter Einfrieren von Teilen der Gewichte \cite{Yosinski2014}.
Auch Segmentierungsmodelle profitieren stark von umfangreichem Vortraining \cite{dippel2022segmentation}. 
In der aktuellen Forschung werden verschiedene \textit{foundation-models} für Segmentierung angewandt \cite{wang2021max,zou2023segment,jain2023oneformer,li2023mask}.
Ein prominentes Exemplar ist das \ac{sam} \cite{kirillov2023sam} von Meta AI. 
Es besteht aus einem Bild Encoder, einem Prompt Encoder und einem Masken Decoder (siehe Fig. \ref{fig:SAM_Architektur}). 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/SAM_Architecture.png}
    \caption{\cite{kirillov2023sam}. Architektur des \ac{sam}. 
    Eingabebilder werden durch einen Bild Encoder zu Repräsentationen umgeformt. 
    Zusätzliche optionale Hinweise auf das zu segmentierende Objekt werden durch Bildfaltungen oder einen Prompt Encoder repräsentiert. 
    Anschließend prädiziert der Decoder mehrere mögliche Masken und zugehörige Zuversichtlichkeiten.}
    \label{fig:SAM_Architektur}
\end{figure}
Als Bild Encoder dient ein Vision Transformer \cite{dosovitskiy2020ViT}, mit Vortraining als Masked Auto Encoder \cite{he2022mae} und zusätzlichem Training für höhere Bildauflösung.
Der Prompt Encoder ist mehrstufig.
Ein angelernter Positional Encoder generiert Repräsentationen aus Positions-Nutzereingaben wie Punkten und Boxen.
Für textuelle Prompts wird der Encoder des CLIP \cite{Radford2021} Model genutzt.
Außerdem werden Bildfaltungen als Encoder auf Masken-Nutzereingaben angewandt.
Mithilfe dieser Encoder wird dem Modell eine Repräsentation von dem zu segmentierenden Bild und optionalen, manuellen Hinweisen auf das erwünschte Ergebnis gegeben, die bereits semantische Informationen und abstrakte Bildmerkmale beinhalten.
Aus diesen Repräsentation generiert dann der Decoder mehrere mögliche Masken mit zugehörigen Zuversichtlichkeiten, aus denen ein finales Segmentierungsergebnis ausgewählt wird.\newline
\ac{sam} wurde bereits für viele Mikroskopie-Zelldaten downstream Anwendungen \textit{fine-tuned} \cite{archit2025samfine, israel2023samfine, vandeloo2025samfine}. 
Auch für bestehende biologische Segmentierungsanwendungen, wie etwa Cellpose\cite{stringer2021cellpose}, wurde \ac{sam} auf Zelldaten angepasst \cite{pachitariu2025samcellpose}.
Dieser \textit{fine-tune} nennt sich CellposeSAM.
Er kombiniert den Bild-Encoder von \ac{sam} mit dem \textit{Flow}-Segmentierungsansatz von Cellpose.
Dabei generiert der Bild-Encoder direkt Vekotoren, die Zwischenrepräsentationen, die sogenannten \textit{Flows}, darstellen.
Diese \textit{Flows} werden pixelweise zu einem Gradientenfeld überführt.
Mithilfe der Gradienten werden Objektinstanzen vorhergesagt.
Fig. \ref{fig:CellposeSAM} zeigt diesen Ablauf als Diagramm. \newline
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/CellposeSAM_Diagramm.png}
    \caption{\cite{pachitariu2025samcellpose}. Ablauf des CellposeSAM Modell. Eingabebilder werden durch einen Bild Encoder direkt zu \textit{Flows} umgeformt. Die Gradienten der Flows werden verfolgt und aus dem entstehenden Gradientenfeld werden Segmentierungsinstanzen vorhergesagt.}
    \label{fig:CellposeSAM}
\end{figure}
Deepcell \cite{van2016deepcell,bannon2021deepcell,greenwald2022deepcell} bietet weitere Zellsegmentierungsnetze.
Das Deepcell-Caliban \cite{moen2019DeepcellCaliban}-Netz nutzt als Encoder eine EfficientNetV2L-Architektur \cite{Tan2021}, an deren Ausgabeschichten (C1\–C5) eine Pyramiden-Struktur zur Merkmalsfusion (P1\–P7) angeschlossen ist. 
Eine Besonderheit des Netzes ist, dass Eingabebildern zusätzlich Koordinatenkarten hinzugefügt werden.
Als Decoder dienen drei Segmentierungsköpfe, die verschiedene Transformationen der gelabelten Trainingsmasken vorhersagen.\newline
In der Literatur sehr verbreitet ist außerdem das nnU-Net \cite{isensee2021nnu}, ein Segmentierungsframework das sich automatisch an neue biomedizinische Aufgaben anpasst.
Es konfiguriert Vorverarbeitung, Netzwerkarchitektur, Training und Nachbearbeitung dynamisch auf Basis der Eigenschaften des jeweiligen Datensatzes. 
Die Leistungsfähigkeit des Ansatzes ergibt sich nicht aus einer neuen Architektur oder Lernmethode, sondern aus der konsequenten Automatisierung und Systematisierung von Entwurfsentscheidungen.

\subsection{Klassifikator}
%\textbf{QUELLEN NOCH!}\newline
Für Klassifikatoren werden in der Regel nur Encoder vortrainiert, der Decoder muss den Klassen der downstream Anwendung angepasst werden \cite{yosinski_how_2014, Dippel2022, plested_deep_2022}.
State-of-the-Art für Bild Encoder sind \acp{cnn} oder \acp{vit}, die auf dem ImageNet \cite{Russakovsky2015} Datensatz vortrainiert werden \cite{You2018, Kornblith2019}.
Klassifikatoren profitieren stark von ImageNet-Vortraining \cite{Beyer2020, Recht2019}.\newline
\textbf{ResNet} ist ein Residual Neural Network, ein \ac{cnn} mit sogenannten residual connections \cite{He2016}.
Diese residual connections verbinden den Ein- und Ausgang modularer Faltungsschichten (siehe Fig. \ref{fig:ResCon}).
Es wurde gezeigt, dass residual connections dem Degradierungsproblem \cite{He2015, srivastava2015highway} von Netzen mit steigender Tiefe entgegenwirken \cite{He2016}.
%\begin{figure}
%    \centering
%    \includegraphics[width=0.4\textwidth]{Figures/ResidualConns.png}
%    \caption{Residual connection. Der Eingang des modularen Blocks ist mit dem Eingang direkt verbunden \cite{He2016}.}
%    \label{fig:ResCon}
%\end{figure}
In ihrem paper stellen die Autoren fünf unterschiedlich tiefe Varianten der \textbf{ResNet} Architektur vor.
Jede Variante enthält fünf Blöcke mit residual connections.
Die Blöcke bestehen aus Faltungen mit verschiedenen Kernelgrößen und Strides, Batch normalization \cite{Ioffe2017} und der ReLU \cite{Nair2010} Aktivierungsfunktion.\newline
\textbf{EfficientNetV2} ist ein Nachfolger der EfficientNet Modellfamilie \cite{Tan2019, Tan2021}.
Die Architektur basiert auf einer Kombination aus MBConv- \cite{Sandler2018, Tan2019} und Fused-MBConv-Blöcken \cite{Gupta2019}.
Beide MBConv-Arten nutzen dabei Squeeze-and-Excitation und 1x1 Faltungsschichten.
Während MBConv aber die Dimension der Daten mithilfe einer weiteren 1x1 Faltung und 3x3 Depthwise-Faltung erhöht, nutzt der Fused-MBConv-Block hierzu eine reguläre 3x3 Faltung (siehe Fig. \ref{fig:MBConv}).\newline
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=1\textwidth]{Figures/ResidualConns.png}
        \caption{Residual connection. Der Eingang des modularen Blocks ist mit dem Eingang direkt verbunden \cite{He2016}.}
        \label{fig:ResCon}
    \end{subfigure} \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=1\textwidth]{Figures/MBConv.png}
        \caption{MBConv- und Fused-MBConv-Blöcke. Die Blöcke nutzt neben einer residual connection auch Faltungsschichten und Squeeze-and-Excitation um die Datendimension zu erhöhen \cite{Tan2021}.}
        \label{fig:MBConv}
    \end{subfigure}
    \caption{Diagramme der Residual Connections, MBConv-Blöcke und Fused-MBConv-Blöcke.}
    \label{fig:ResDiagramms}
\end{figure}
\textbf{ConvNeXt} \cite{Liu2022} ist eine \ac{cnn}-Modellfamilie mit besonders großen Faltungskernen. 
Die \textbf{ConvNeXt} Architektur umfasst fünf modulare Blöcke mit Faltungen und residual connections, wie die ResNet Architektur \cite{He2016}.
Allerdings verändert dabei \textbf{ConvNeXt} einige Details der ResNet Architektur, wie beispielsweise die GeLU Aktivierungsfunktion \cite{Hendrycks2016} und Layer normalization \cite{Ba2016}.\newline
Swin Transformer \cite{liu2021} sind eine beliebte Modellfamilie von \acp{vit}.
Ihr Nachfolger, die \textbf{Swin Transformer V2} \cite{Liu2022a} vergrößert die Modelle noch.
Die Architektur \acp{vit} kombiniert Bildausschnitte mit einem Positions-Bias.
Hierzu wird ein Bildfenster $z$ und dessen relativen Koordinaten im Bild $\Delta x$ und $\Delta y$ in einem Attention Mechanismus zusammengeführt.
Die Positionen werden in einem MLP-Netz verarbeitet, währen das Bildfenster mit drei verschiedenen Gewichtsmatrizen multipliziert wird.
Mithilfe einer Kosinus-Ähnlichkeitsfunktion, der Softmax Funktion \cite{Bridle1989} und elementweiser Multiplikation sowie Addition werden diese Ergebnisse in einen Merkmalsraum überführt.
Zwei Layer normalization \cite{Ba2016} Schichten, ein weiteres MLP-Netz und residual connections vervollständigen anschließend den modularen \textbf{Swin Transformer V2} Block.
Dieser Aufbau ist in Fig. \ref{fig:SwinArchitecture} dargestellt. 

%Backbones:
%\cite{He2016} ResNet
%\cite{Tan2021} EfficientNetV2L
%\cite{Liu2022} convnextxl
%\cite{Liu2022a} SwinV2
\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/SWINV2_Architecture.png}
    \caption{\textbf{Swin Transformer V2} Architektur \cite{Liu2022a}. 
    Ein Bildfenster $z$ und dessen relative Koordinaten im Bild $\Delta x$ und $\Delta y$ werden in einem Attention Mechanismus zusammengeführt.
    Mithilfe einer Kosinus-Ähnlichkeitsfunktion, der Softmax Funktion \cite{Bridle1989} und elementweiser Multiplikation sowie Addition werden diese Ergebnisse in einen Merkmalsraum überführt.
    Zwei Layer normalization \cite{Ba2016} Schichten, ein weiteres MLP-Netz und residual connections vervollständigen anschließend den modularen \textbf{Swin Transformer V2} Block.}
    \label{fig:SwinArchitecture}
\end{figure}

\section{Offene Probleme}

\section{Zielsetzung}