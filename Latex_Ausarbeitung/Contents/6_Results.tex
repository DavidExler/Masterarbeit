% !TeX root = ../Thesis.tex

\chapter{Ergebnisse} \label{ch:results}


\section{Überblick}
Das nachfolgende Kapitel beschreibt die Ergebnisse der durchgeführten Experimente.
Die Experimente werden in mehreren Durchläufen des neu entwickelten Konzepts \glqq 3D-Zelldaten-Pipeline\grqq zur Auswertung von 3D-Zelldaten durchgeführt.
Auf Grundlage der Messergebnisse der 3D-Zelldaten-Pipeline wird eine statistische Auswertung vorgenommen. 
Zuerst werden die Bewertungen der Instanzsegmentierungsmasken anhand der \ac{ipq}-Metrik und anschließend die der Klassifikatoren anhand ihrer Genauigkeit betrachtet.
Die \ac{ipq}-Ergebnisse werden in die drei Faktoren: \acf{sq}, \acf{rq} und die neu eingeführte \acf{iq} gegliedert (siehe Kap. \ref{sec:Kriterien}).
Daraus wird ersichtlich, dass jedes der Segmentierungsmodelle in einem der Faktoren dominiert, insgesamt aber CellposeSAM den anderen Modellen signifikant überlegen ist. 
Auch die Ergebnisse der Klassifikation werden unterteilt, um den Einfluss einzelner Methoden auf die Genauigkeit des Klassifikators zu identifizieren.
Vergleiche der Ergebnisse verschiedener Methoden zeigen, dass die neu eingeführten Methoden wie der Pseudo-Labler (siehe Kap. \ref{sec:Pretrain}) je nach Anforderung dem Stand der Technik überlegen sind.
Außerdem zeigen die Ergebnisse, dass das Anwenden der 3D-Zelldaten-Pipeline optimale Methoden für die Segmentierung und Klassifikation von Zelldaten effizient findet und gegenüber nicht-optimalen Methoden einen signifikanten Qualitätsunterschied bringt.
Mithilfe ausführlicher Analysen sind des Weiteren grundlegende Erkenntnisse über das Verhalten von Klassifikatoren im Umgang mit biologischen 3D-Bildstapeln möglich.

\section{Hardware}
Für die Anwendung wird eine NVIDIA GeForce RTX 3090 Ti mit 24 GB VRAM verwendet.
Der verwendete Server verfügt über eine 12th-Gen-Intel(R) Core(TM) i9-12900KF-CPU mit 16 Kernen und 64 GB RAM.

\section{Segmentierung}
Für die Wahl eines Segmentierungsmodells wird in Kapitel \ref{ch:NewMethods} das Bewertungskriterium \ac{ipq} eingeführt.
Außerdem wird in Kapitel \ref{ch:NewMethods} der annotierte S\_BIAD1518-Datensatz vorgestellt.
Die \ac{ipq} wird auf dem Datensatz mit den Masken der drei vortrainierten Segmentierungsmodelle und zur Validierung mit den Annotationen durchgeführt.
Alle Annotationen erreichen einen \ac{ipq}-Wert von genau Eins.
Abb. \ref{fig:example_masks} zeigt exemplarisch einen 2D-Schnitt eines 3D-Bildes mit roten Linien als Konturen der Annotation oder der vorhergesagten Segmentierungsmaske je Modell.
Die prädizierten Instanzsegmentierungsmasken für die dreidimensionalen Benchmarkdaten der drei Segmentierungsmodelle unterscheiden sich optisch deutlich (siehe Abb. \ref{fig:example_masks}).
Masken des nnUNet-Modells sehen kantiger aus als die Masken anderer Modelle und weisen oft eine raue Kontur mit hervorstehenden Extremitäten oder Einkerbungen auf.
Deepcell-Masken sehen glatt und ausgebreitet aus.
Durch die Watershed-Nachverarbeitung sind teilweise sichtbare Artefakte entstanden, an Stellen, an denen das Trennen der Instanzen für Watershed nicht möglich ist.
Die CellposeSAM-Masken sehen intuitiv am besten aus, sie haben oft nahezu eliptische Konturen und trennen Instanzen so wie es für das menschliche Auge sinnvoll erscheint. 
Das spiegelt sich auch in deutlichen Unterschieden in der \ac{ipq} wider.
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \includegraphics[width=\linewidth]{Figures/Deepcell_Mask_Example.png}
        \caption{Deepcell Maske.}
        \label{fig:DeepcellMaske}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \includegraphics[width=\linewidth]{Figures/nnUNet_Mask_Example.png}
        \caption{nnUNet Maske.}
        \label{fig:nnUNetMaske}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \includegraphics[width=\linewidth]{Figures/CellposeSam_Mask_Example.png}
        \caption{CellposeSam Maske.}
        \label{fig:CellposeSamMaske}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \includegraphics[width=\linewidth]{Figures/gt_Mask_Example.png}
        \caption{Maske der Annotation.}
        \label{fig:gtMaske}
    \end{subfigure}
    \caption{Darstellung der Segmentierungsmasken der verschiedenen Segmentierungsmodelle sowie der Annotation als Konturen auf einem zweidimensionalen Durchschnitt einer dreidimensionalen Stichprobe des S\_BIAD1518-Datensatzes.}
    \label{fig:example_masks}
\end{figure}
\noindent
Die Ergebnisse jedes Segmentierungsmodell sind einzeln und für jedes Bild im Anhang \ref{supp:ipq} angehängt. 
Eine Zusammenfassung der \ac{ipq}-Ergebnisse ist in den Boxplots in Abb. \ref{fig:boxplots_ipq} gegeben.
Das zentrale Ergebnis ist, dass CellposeSAM die besten \ac{ipq}-Werte liefert. 
Mit einem Mittelwert von \num{0.64} ist die \ac{ipq} von CellposeSAM höher als der Mittelwert bei nnUNet (\num{0.04}) und Deepcell (\num{0.02}), bestätigt durch höchst signifikante einseitige t-Tests. 
Dennoch zeigt sich, dass CellposeSAM lediglich in der Kategorie \ac{sq} den höchsten Mittelwert aufweist. 
Die \ac{rq} der nnUNet-Masken ist höher als die der CellposeSAM-Masken. 
Ebenso ist die \ac{iq} der Deepcell-Masken höher als die \ac{iq} der CellposeSAM-Maske.
Beide Beobachtungen sind durch höchst signifikante t-Tests gestützt. \\[0.5\baselineskip]
Obwohl nnUNet und Deepcell jeweils eine Metrik dominieren, wird ihr \ac{ipq}-Wert durch die beiden niedrigsten Faktoren stark heruntergezogen, während CellposeSAM in jeder Metrik gut, wenn auch nicht am besten, abschneidet.
Außerdem zeigen die Boxplots viele Ausreißer in den Daten, was den Unterschieden zwischen den Bildkategorien, die der Datensatz enthält, geschuldet sein kann.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/Segmentation_Boxplots.pdf}
    \caption{Ergebnisse der \ac{ipq}-Berechnungen mit den Faktoren $k_i = 1$ (siehe Formel (\ref{eq:ipq})). 
    Die X-Achse unterteilt die Daten in die Kriterien \acf{sq}, \acf{rq}, \acf{iq} und \acf{ipq}, wie in der Formel (\ref{eq:ipq}) beschrieben. 
    Für jede Metrik sind drei farbige Boxplots zu sehen, jeweils einer pro Segmentierungsmodell.
    Die Sterne zeigen die Signifikanz relevanter t-Tests.
    }
    \label{fig:boxplots_ipq}
\end{figure}
\noindent
In Abb. \ref{fig:ipq_examples} sind Beispiele für die einzelnen Fehlerarten zu sehen.
Das \ac{iq}-Beispiel zeigt die Verletzung injektiver Abbildung der Nuclei auf die Annotation anhand einer nnUNet-Maske.
Zentral ist hier ein nicht-eliptischer Nucleus zu sehen.
\newline\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{Figures/IPQ_examples.pdf}
    \caption{Exemplische Darstellung einzelner Ausprägungen verschiedener Fehlerarten.
    Hierzu sind jeweils, in dieser Reihenfolge von links nach rechts, ein Ausschnitt eines 2D-Durchschnitts einer Stichprobe, die Annotation als Kontur und die vorhergesagte Maske eines Modells zu sehen.
    Die erste Zeile zeigt ein Beispiel für einen schlechten \ac{iq}-Wert anhand einer nnUNet-Maske.
    In der zweiten Zeile ist ein \ac{rq}-Fehler anhand einer CellposeSAM-Maske zu sehen und unten ist ein Beispiel für einen schlechten \ac{sq}-Wert anhand einer Deepcell-Maske zu sehen. 
    }
    \label{fig:ipq_examples}
\end{figure}
\noindent
Dieser Nucleus wird vom nnUNet-Modell in mehrere Segmente unterteilt, wodurch die räumliche Konzentration der Nuclei überschätzt wird.
Nicht-eliptische Nuclei führen häufig zu dieser Art von Fehlern.\\[0.5\baselineskip]
Mit dem Fehler gehen \ac{rq}-Fehler einher, weil zwei der Segmente im Vergleich zur Annotation zu klein sind, um als \ac{tp} erkannt zu werden.
In der zweiten Zeile ist anhand einer CellposeSAM-Maske ein \ac{rq}-Beispiel gegeben.
Das Segmentierungsmodell hat hier einen Nucleus halluziniert. 
Der Nucleus unten links, direkt unter dem Linken der beiden großen Nuclei, ist in der Annotation nicht zu finden.
Dieser Fehler geht nicht mit einem schlechteren \ac{sq}- oder \ac{iq}-Wert einher.
Durch den Fehler wird die Anzahl der Nuclei überschätzt.\\[0.5\baselineskip]
Zuletzt ist ein \ac{sq}-Beispiel dargestellt.
Neben und unter dem sichtbaren Nucleus oben sind vermutlich Schatten von Nuclei aus anderen Z-Ebenen zu sehen, die vom Deepcell-Modell als Nuclei erfasst wurden.
Dadurch ist das Segment, das den sichtbaren Nucleus abbildet, deutlich zu groß, was die \ac{iou}-Werte beeinträchtigt.
Mit diesem Fehler geht auch ein schlechterer \ac{rq}-Wert einher, da der Schatten zu mehreren Halluzinationen geführt hat.
Da diese Halluzinationen nicht mit den Annotationssegmenten überschneiden, ist der \ac{iq}-Wert hier nicht betroffen.
Der Fehler führt zu einer Fehleinschätzung des Nucleivolumens.\\[0.5\baselineskip]
Abb. \ref{fig:contour_examples} zeigt ein Beispiel für eine Instanzsegmentierungsmaske der Zieldaten mit dem CellposeSAM-Modell.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/contour_examples.pdf}
    \caption{Exemplische Darstellung einer Zieldaten-Instanzsegmentierungsmaske des CellposeSAM-Modells.
    Links ist eine 2D-Schicht des Original-Bildes zu sehen, daneben die Instanzsegmentierungsmaske und rechts eine Überlagerung von den Konturen der Instanzen über die Marker-Kanäle des Original. 
    Zu jeder der drei Ansichten sind auch ein 2D-Schnitt der X-Z-Ebene und der Y-Z-Ebene gegeben.
    }
    \label{fig:contour_examples}
\end{figure}
\noindent
Links sind exemplarisch 2D-Schichten eines 3D-Eingabe-Volumens zu sehen.
Die Schichten sind Schnitte der drei Koordinaten-Ebenen des Volumens.
Daneben ist die Instanzsegmentierungsmaske dargestellt als Maskenbild und als Konturen auf den Marker-Kanälen des Eingabebilds.
Zu sehen ist, dass die CellposeSAM-Masken auch intuitiv sehr gut zur Eingabe passen.
\section{Klassifikation}\label{sec:resultsClassifier}
\subsection{Überblick}
Die in Abschnitt \ref{sec:MethodsClassifier} eingeführten Methoden zur Klassifikation werden anhand eines separaten Anteils des manuell annotierten Datensatzes durch die neu entwickelte 3D-Zelldaten-Pipeline getestet.
Der manuell annotierte Datensatz enthält 125 Stichproben von \newline Myotuben-Zellkernen, 95 Stichproben von der Klasse Debris, 148 Stichproben der Klasse \glqq Andere\grqq und nur 16 Stichproben der Klasse Schwannzellen-Nucleus.
Die Klassen Eins bis Vier sind hier der Reihe nach Myotuben-Kerne, Debris, Andere und Schwannzellen-Kerne.
Der Abschnitt \ref{supp:Klassifikation} des Anhang zeigt alle Ergebnisse der verschiedenen Methodenkombinationen tabellarisch.
Um die Genauigkeit der Klassifikatoren anhand des Test-Anteil möglichst repräsentativ zu bestimmen, werden manuell 50\% statt 20\% der Schwannzellen-Nuclei in den Test-Anteil der Daten eingesetzt.
Die Genauigkeit, also der prozentuale Anteil richtiger Vorhersagen, auf dem Test-Anteil des Zieldatensatzes wird als Kriterium verwendet.\\[0.5\baselineskip]
Da die Modelle während des Trainings rauschbehaftete Verläufe der Genauigkeit aufweisen und es zu Overfitting kommen kann, wird Early-Stopping implementiert, mit der mittleren Genauigkeit des Klassifikators auf dem Test-Anteil der manuell annotierten Daten über fünf Epochen.
Dazu wird die geglättete Genauigkeit betrachtet.
Wenn diese Vier mal in Folge sinkt oder gleich bleibt, wird das Training unterbrochen.
Abb. \ref{fig:Trainingsverlauf} zeigt exemplarisch zwei Trainingsverläufe, die dieses Phänomen belegen.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.68\linewidth]{Figures/Trainingskurven.pdf}
    \caption{Die Diagramme zeigen den Trainingsverlauf eines Klassifikators. 
    Links ist der Verlauf des Loss zu sehen.
    In der Mitte und rechts sind die Verläufe der Validierungsgenauigkeit zu sehen.
    Rechts ist die Kurve zur Anschaulichkeit durch Mittelung von je fünf Werten geglättet.
    Der obere Verlauf zeigt einen Klassifikator ohne Overfitting, im unteren Verlauf ist Overfitting zu erkennen.
    }
    \label{fig:Trainingsverlauf}
\end{figure}
\noindent
Im oberen Verlauf ist zu sehen, dass die Genauigkeit des Klassifikators tendenziell steigt.
Die geglättete Genauigkeit zeigt hier zwar rauschbehaftetes Verhalten mit einigen lokalen Minima, aber nach ein bis zwei Berechnungsschritten steigt der Wert wieder.
Der Verlauf ist also zwar nicht monoton steigend, aber es liegt nahe, dass bei Fortsetzung des Trainings weitere Verbesserungen zu erwarten sind.
Im unteren Verlauf ist Overfitting zu erkennen, da die geglättete Genauigkeit über vier Berechnungsschritte hinweg gesunken oder gleich geblieben ist.
Der Trainings-Loss nimmt in den selben Epochen weiter ab, das Modell passt sich also weiter an die Trainingsdaten an, ohne dass es besser darin wird, ungesehene Nuclei zu erkennen.
Die Genauigkeit auf dem Test-Anteil des Datensatzes sinkt etwa ab Epoche 30 und erreicht ihr Maximum in Epoche 32. 
Dementsprechend wird die Genauigkeit des Klassifikators in Epoche 32 für den Vergleich herangezogen. \\[0.5\baselineskip]
Mit der Prädiktion erweitern die Klassifikatoren die Instanzsegmentierungsmasken zu panoptischen Segmentierungsmasken.
Abb. \ref{fig:contours_classes} zeigt exemplarisch eine panoptische Maske.
Die Klassen der Nuclei sind als Farben der Konturen gegeben.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.68\linewidth]{Figures/contours_and_classes.pdf}
    \caption{Panoptische Segmentierungsmaske, erstellt durch die Kombination der Instanzsegmentierungsmaske des CellposeSAM-Segmentierungsmodell und den prädizierten Klassen eines Klassifikators.
    }
    \label{fig:contours_classes}
\end{figure}
\newpage

\subsection{Encoder}\label{subsec:RES_encoder}
In Abb. \ref{fig:bars_encoders} ist die Genauigkeit der Klassifikatoren pro Encoder gegeben.
Zu sehen ist sowohl das Maximum, als auch der Durchschnitt der Genauigkeitswerte der verschiedenen Methodenkombinationen jedes Klassifikators.
Die Klassifikatoren sind nach dem aufsteigenden Maximalwert sortiert.
\begin{figure}[H]
    \centering\includegraphics[width=0.8\linewidth]{Figures/encoders_compare.pdf}
    \caption{Das Balkendiagramm zeigt an der Y-Achse die Genauigkeit der Klassifikatoren bei Verwendung der verschiedenen Encoder.
    Auf der X-Achse sind die Encoder in Gruppen aufgetragen.
    Jede Gruppe enthält einen Maximalwert (Orange) und einen Durchschnittswert (Blau), da jeder Encoder mit verschiedenen Kombinationen von Methoden getestet wird.
    Die Encoder sind nach aufsteigendem Maximalwert von links nach rechts sortiert.
    }
    \label{fig:bars_encoders}
\end{figure}
\noindent
Zu sehen ist, dass der beste Wert vom ResNet18-Encoder stammt.
Mit 85,9\% Genauigkeit auf dem Test-Anteil des Datensatzes ist diese Kombination das gefundene Optimum.
Auch im Mittelwert haben die Klassifikatoren mit dem ResNet18-Encoder mit 79,1\% die höchste Genauigkeit.
CellposeSAM und ResNet101 haben als Encoder die zweit- und dritthöchste durchschnittliche Genauigkeit mit 77,4\% und 77,3\%, jeweils.
Während CellposeSAM allerdings die zweithöchste maximale Genauigkeit aufweist, hat ResNet101 die geringste.
Besonders auffällig ist Swin V2.
Mit 64,6\% hat es die niedrigste durchschnittliche Genauigkeit, aber der Maximalwert von 83,9\% ist der dritthöchste.
\subsection{Vortraining}\label{subsec:RES_pretrain}
Abb. \ref{fig:heatmap_pretrain} zeigt eine Übersicht über die Effektivität der verwendeten Vortrainingsmethoden.
\begin{figure}[H]
    \centering\includegraphics[width=0.7\linewidth]{Figures/encodersVpretrain.pdf}
    \caption{Die Heatmap zeigt die Genauigkeiten der Klassifikatoren bei Verwendung der verschiedenen Vortrainingsmethoden.
    Auf der Y-Achse sind die verschiedenen Encoder aufgeführt.
    Die X-Achse zeigt die getesteten Methoden des Vortrainings an.
    Die Farbe der Felder und der Wert darin zeigen die Durchschnittsgenauigkeiten aller Klassifikatoren, die die entsprechende Vortrainingsmethode nutzen.
    Die letzte Zeile zeigt den Durchschnitt der Werte aller Encoder, abhängig von der Vortrainingsmethode.}
    \label{fig:heatmap_pretrain}
\end{figure}
\noindent
Um die Ergebnisse möglichst unabhängig von den anderen Klassifikatormethoden zu betrachten, werden hier nur die Klassifikatoren mit der besten Vorverarbeitungsmethode für den Encoder und dem Volumen-Klassifikator als Klassifikations-Kopf betrachtet.
Auf der vertikalen Achse sind die verschiedenen Encoder sowie der Durchschnitt aller Encoder zu sehen.
Auf der horizontalen Achse sind die vier Vortrainingsmethoden aufgeführt.
Links ist nur semi-supervised-Training zu sehen.
Daneben steht das semi-supervised-Vortraining, also das Vortraining mit den Pseudo-Labler-Annotationen, Einfrieren der Encoder-Gewichte und Fortsetzen des Trainings mit vollständig annotierten Daten. 
Rechts ist das fully-supervised ImageNet-Vortraining mit eingefrorenen Encoder-Gewichten und kein Vortraining aufgetragen (siehe Kap. \ref{sec:Pretrain}).
Zu sehen ist, dass das Vortraining mit semi-supervised-Daten durchschnittlich deutlich schlechtere Ergebnisse liefert als das fully-supervised ImageNet-Vortraining und etwas schlechtere Ergebnisse als kein Vortraining.
Im Durchschnitt führt das Fortführen des Trainings mit den annotierten Daten nach dem \newline semi-supervised-Training zu einer Steigerung der Genauigkeit um 13 Prozentpunkte.
Für den ConvNeXt-Encoder ist der Vorteil des fortgesetzten Trainings besonders hoch mit 39 Prozentpunkten.
Der ConvNeXt-Encoder ist außerdem mit dem semi-supervised-Vortraining um sieben Prozentpunkte genauer als ohne Vortraining.
Auch die beiden ResNet-Modelle erzielen mit dem semi-supervised-Vortraining ähnlich gute Ergebnisse wie mit anderen Vortrainingsmethoden.
Gegenüber dem ImageNet-Vortraining, das deutlich mehr Rechenaufwand erfordert, sind die Klassifikatoren mit ResNet18 und ResNet101 nur um zwei bzw. drei Prozentpunkte schlechter, wenn sie die Annotationen des Pseudo-Lablers als Vortraining nutzen.
Beide haben außerdem eine höhere Genauigkeit als der Durchschnitt ohne Vortraining.
Sie sind beide auch ohne Fortsetzung des Trainings nach dem semi-supervised-Vortraining bereits 64\% und 62\% genau. \\[0.5\baselineskip]
Besonders auffällig ist auch hier das Swin V2-Modell, das ohne fully-supervised-Vortraining nur eine durchschnittliche Genauigkeit von 44,0\% erreicht.
Vortraining auf dem ImageNet-Datensatz führt im Vergleich dazu bei diesem Encoder zu einer Steigerung der Genauigkeit um 40 Prozentpunkte.
Wie in Abschnitt \ref{subsec:RES_encoder} beschrieben hat die Genauigkeit des Swin V2-Encoder einen hohen Maximalwert, aber einen geringen Durchschnittswert.
Der Vergleich der Vortrainingsmethoden zeigt, dass das an der schlechten Leistung des Swin V2-Encoder ohne ImageNet-Vortraining liegt.
Abb. \ref{fig:gradCAM_Swin} zeigt zwei Scatterplots.
Beide visualisieren Projektionen des Merkmalsraums des Swin V2-Ecoders auf zwei Dimensionen mittels \ac{tsne}.
Die linke Abbildung zeigt den Merkmalsraum des Encoders ohne Vortraining und die Rechte den Merkmalsraum nach ImageNet-Vortraining.
Zu sehen ist, dass der Swin V2-Encoder keinen sinnvollen Merkmalsraum lernt.
Das Vortraining auf dem ImageNet-Datensatz ist umfangreicher und diverser als auf den Zieldaten und führt dadurch zu einem generalisierteren Merkmalsraum.
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \includegraphics[width=\linewidth]{Figures/swin_bad.pdf}
        \caption{Deepcell Maske.}
        \label{fig:DeepcellMaske}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \includegraphics[width=\linewidth]{Figures/swin_good.pdf}
        \caption{}
        \label{fig:nnUNetMaske}
    \end{subfigure}
    \caption{2D-Projektionen des Merkmalraums von zwei Klassifikatoren durch \ac{tsne}.
    Der linke Scatterplot zeigt den Merkmalsraum eines Swin V2-Encoders ohne Vortraining, der Rechte den eines ImageNet-vortrainierten Swin V2-Encoders.
    Die Werte der X- und Y-Achse sind arbiträre Aktivierungsintensitäten ohne physische Interpretation.}
    \label{fig:gradCAM_Swin}
\end{figure}
\noindent
ImageNet-Vortraining liefert im Durchschnitt die besten Ergebnisse mit einer Genauigkeit von 84\%. 
Außerdem ist die Varianz entlang der Encoder mit dem ImageNet-Vortraining (\num{5.6e-5}) deutlich geringer als ohne Vortraining (0,016). 
Dennoch erreicht ResNet18 ohne Vortraining die beste Genauigkeit von 86\%.
Des Weiteren wird hier die durchschnittliche Genauigkeit pro Klasse betrachtet.
Abb. \ref{fig:per_class_acc} zeigt die durchschnittliche Genauigkeit pro Klasse und Sterne als Hinweise auf signifikante Ergebnisse von paarweisen t-Tests.
Wie eingangs beschrieben, sind nur 16 Beispiele der Klasse Schwannzellen-Nucleus im Datensatz enthalten, acht hiervon im Trainings- und acht im Test-Anteil.
Zu sehen ist, dass insbesondere die Schwannzellen-Kerne schlecht erkannt werden.
Wie in Kap. \ref{sec:Preproc} beschrieben, wird dieses Problem der ungleichen Verteilung der Klassen durch den Retreiver durch eine Gewichtung der Gradienten unterrepräsentierter Klassen angegangen.
In der Abbildung ist jedoch zu sehen, dass dieser Ansatz das Problem nicht vollständig gelöst hat. 
Die unterrepräsentierte Klasse der Schwannzellen-Nuclei wird höchst signifikant schlechter erkannt als alle anderen Klassen.
Beinahe alle Methoden ergeben isoliert betrachtet eine ähnliche Verteilung der Genauigkeit pro Klassen, nur das Vortraining hat einen Einfluss.
Vortraining mit semi-supervised-Annotationen und anschließendes Fortsetzen des Trainings mit manuell annotierten Daten führen zu Klassifikatoren, die teilweise wesentlich ausgeglichener die Klassen erkennen.
Mit dem Volumen-Klassifikator und der Masken-Methode zur Vorverarbeitung erreichen der ResNet101-Encoder 83,2\%, der ResNet18-Encoder 73,5\% und der  ConvNeXt-Encoder 56,3\% Genauigkeit für die Klasse der Schwannzellen-Nuclei.
Das ist im Vergleich zu den 21,7\% durchschnittlicher Genauigkeit für die Schwannzellen-Klasse ein starker Gewinn.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/per_class_acc.pdf}
    \caption{Durchschnittliche Genauigkeit pro Klasse für alle Klassifikatoren.
    Die Klassen Eins bis Vier sind der Reihe nach Myotuben-Kerne, Debris, Andere und Schwannzellen-Kerne. 
    }
    \label{fig:per_class_acc}
\end{figure}
\noindent
Abb. \ref{fig:class_examples} zeigt je ein Beispiel der Klassen, ohne Vorverarbeitung.
\begin{figure}[H]
     \centering
     \includegraphics[width=0.9\linewidth]{Figures/class_examples.pdf}
    \caption{Beispiele der Nucleus-Klassen.
    Die Klassen Eins bis Vier sind der Reihe nach Myotuben-Kerne, Debris, \glqq Andere\grqq und Schwannzellen-Kerne. 
    }
    \label{fig:class_examples}
\end{figure} 
\noindent
In Rot sind die Nucleus-Kanäle dargestellt, Grün und Blau sind Marker-Kanäle.
Die Nuclei der Klasse Eins (Myotuben-Kern) und Vier (Schwannzellen-Kern) sind optisch sehr ähnlich, beide sind lang und dünn.
Die Unterscheidung ist nur durch die Betrachtung umliegender Strukturen möglich. \\[0.5\baselineskip] 
Für den ResNet18-, ResNet101- und ConvNeXt-Encoder, die drei Encoder die erfolgreich mit dem semi-supervised-Vortraining trainiert sind, ist die durchschnittliche Genauigkeit für die Schwannzellen-Klasse 71\%
Die durchschnittliche Genauigkeit ist nach dem semi-supervised-Vortraining also geringer, aber dafür wird die unterrepräsentierte Klasse der Schwannzellen-Kerne nicht wie bei anderen Vortrainingsmethoden schlechter erkannt.


\subsection{Klassifikations-Kopf}\label{subsec:RES_decoder}
Abb. \ref{fig:bars_decoders} zeigt eine Gegenüberstellung der Genauigkeit beider Klassifikations-Köpfe in einem Balkendiagramm mit Fehlerbalken.
\begin{figure}[H]
    \centering\includegraphics[width=0.8\linewidth]{Figures/encodersVdecoder.pdf}
    \caption{Genauigkeitswert der Klassifikatoren unter Verwendung eines bestimmten Klassifikations-Kopfes.
    Auf der Y-Achse ist der Durchschnitt der Genauigkeiten der besten Trainingsepoche aller Klassifikatoren, die diesen Klassifikations-Kopf nutzen, aufgetragen. 
    Die X-Achse zeigt Gruppen von Encodern, jeweils besetzt mit einem Wert für den Schichten- und den Volumen-Klassifikator.
    Rechts zu sehen sind Balken, die den Durchschnitt der Werte aller Encoder, abhängig von dem Klassifikations-Kopf, zeigen.}
    \label{fig:bars_decoders}
\end{figure}
Dabei werden nur die Klassifikatoren berücksichtigt, bei denen sich die Methodenkombination ausschließlich im verwendeten Klassifikations-Kopf unterscheidet.
Das ist notwendig, da nicht alle möglichen Kombinationen trainiert wurden und ein Vergleich mit unvollständigen Kombinationen keine aussagekräftigen Ergebnisse liefert.  
Die blauen Balken des Balkendiagramms zeigen jeweils die Ergebnisse der Architekturen, die den Schichten-Klassifikator nutzen, die orangenen Balken die Ergebnisse des Volumen-Klassifikators.
Auf der X-Achse sind als Gruppen zuerst die Encoder und zuletzt der Durchschnitt aufgetragen, und auf der Y-Achse die durchschnittliche Genauigkeit der Modelle mit dem jeweiligen Klassifikations-Kopf.
An den Daten lässt sich erkennen, dass der Einfluss des Klassifikations-Kopfes nicht homogen ist.
Mit dem ConvNeXt-Encoder ist der Schichten-Klassifikator performanter als der Volumen-Klassifikator, während es bei allen anderen Encodern umgekehrt ist.
Für den CellposeSAM Encoder ist der Genauigkeitsunterschied 12,6 Prozentpunkte groß, für EfficientNetV2 einen halben Prozentpunkt.
Die Daten zeigen also, dass die Intensität des Unterschieds in der Genauigkeit der beiden Klassifikations-Köpfe stark vom Encoder abhängt.
Die Balken rechts, die je den Durchschnitt aller Modelle mit einem Klassifikations-Kopf darstellen, zeigen mit einem signifikant, dass der Volumen-Klassifikator insgesamt bessere Ergebnisse liefert.


\subsection{Vorverarbeitung}\label{subsec:RES_preproc}
In Abb. \ref{fig:bars_channel1} sind die durchschnittlichen Genauigkeiten der Modelle gezeigt, die jeweils eine der Vorverarbeitungsmethoden nutzen.
\begin{figure}[H]
    \centering\includegraphics[width=0.8\linewidth]{Figures/encodersVchannel1.pdf}
    \caption{Genauigkeitswert der Klassifikatoren bei Verwendung einer bestimmten Vorverarbeitungsart.
    Auf der Y-Achse ist der Durchschnitt der Genauigkeiten der besten Trainingsepoche aller Klassifikatoren, die diese Vorverarbeitungsart nutzen, aufgetragen. 
    Die X-Achse zeigt Gruppen von Encodern, jeweils besetzt mit einem Wert für die Masken-Methode, die den Nucleus-Kanal des Bildes durch die Segmentierungsmaske ersetzt, und die Distanz-Methode, die eine Distanztransformation auf den Nucleus-Kanal anwendet.
    Rechts zu sehen sind Balken, die den Durchschnitt der Werte aller Encoder, abhängig von der Vorverarbeitungsmethode, zeigen.}
    \label{fig:bars_channel1}
\end{figure}
\noindent
Wie für den Klassifikations-Kopf-Vergleich sind auch hier die Klassifikatoren ausgeschlossen, deren Methodenkombination mit nur einer der beiden Vorverarbeitungsmethoden trainiert wurde.
Auf der Y-Achse ist die Genauigkeit aufgetragen, auf der X-Achse sind die Encoder und der Durchschnitt aller Encoder als Gruppen.
Die Ergebnisse zeigen deutlich, dass die Masken-Methode, also das Ersetzen des Nucleus-Kanals mit der Segmentierungsmaske, zu einer höheren Genauigkeit führt.
Mit der Distanz-Methode, also dem Anwenden einer Distanztransformation von der Segmentierungsmaske auf den Nucleus-Kanal, beträgt die Genauigkeit durchschnittlich 77,2\%, mit der Masken-Methode sind es 81,5\%.
Die Differenz ist hoch signifikant.
Zu sehen ist aber, dass der Einfluss der Vorverarbeitung unterschiedlich stark ist, je nach Encoder.
Während die Genauigkeit für Modelle mit dem ResNet101-Encoder mit der Masken-Methode um 0,8 Prozentpunkte gegenüber der Verwendung von der Distanz-Methode steigt, macht die Vorverarbeitung beim EfficientNetV2 Encoders 11,5 Prozentpunkte aus. \\[0.5\baselineskip]
Mithilfe von GradCAM \cite{Selvaraju2017} wird außerdem das Verhalten der Klassifikatoren exemplarisch visualisiert.
Abb. \ref{fig:gradCAM} zeigt den Nucleus-Kanal und einen Marker-Kanal von zwei 2D-Schichten.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/gradCAM.pdf}
    \caption{GradCAM-Visualisierung für zwei exemplarische 2D-Schichten, jeweils mit einem Nucleus-Kanal und einem Marker-Kanal.
    Links ist ein Beispiel einer Stichprobe gegeben, die mit der Masken-Methode trainiert wurde, rechts mit der Distanz-Methode.
    Die Nucleus-Kanäle zeigen die Segmentierungsmasken der Nuclei in weiß, überlagert mit der gradientenbasierten Heatmap, die die Wichtigkeit räumicher Regionen für die Klassifikation darstellen.
    In beiden Nucleus-Kanälen ist die Wichtigkeit der Nucleus-Oberfläche nur sehr gering.
    In den Marker-Kanälen ist zu sehen, dass Regionen mit erkennbaren Strukturen besonders wichtig sind.}
    \label{fig:gradCAM}
\end{figure}
\noindent
Die beiden 2D-Schichten sind aus unterschiedlichen 3D-Stapeln, links einer der mit der Masken-Methode, und rechts einer der mit der Distanz-Methode vorverarbeitet wurde.
Zu sehen ist jeweils eine Heatmap für zwei Klassifikatoren
Beide Heatmaps zeigen gradientenbasiert die Aktivierungen in einer Schicht des Merkmalsraums zurück auf die Eingangsdaten projeziert.
Die Heatmaps sind jeweils zur besseren Interpretierbarkeit einer 2D-Schicht der Segmentierungsmaske des Nucleus und einem Marker-Kanal überlagert. 
Dadurch wird räumlich aufgezeigt, welche Regionen der Eingangsdaten zur Entscheidung für eine bestimmte Klasse besonders beigetragen haben.
Die beiden Nucleus-Kanal-Ansichten zeigen, dass Regionen direkt um den Nucleus herum für die Klassifikationsentscheidung wichtiger sind als die Oberfläche des Nucleus.
Selbst unter Verwendung der Distanz-Methode, die zum Ziel hat, Oberflächen-Merkmale zu erhalten und dafür klare Kanten und eine eindeutige Geometrie der Segmentierungsmaske augibt, wird die Oberfläche kaum betrachtet.
Des Weiteren sind in den Heatmaps der Marker-Kanäle zu sehen, dass auch abseits des Nucleus-Kanals eine intuitiv sinnvolle Lokalisierung der Aufmerksamkeit des Klassifikators erfolgt.
Die Maxima der Heatmaps sind auf sichtbare Strukturen im Marker-Kanal platziert.

\subsection{Wichtigkeit der Marker}\label{subsec:RES_allg}
Eine weitere angestellte Untersuchung ist die Quantifizierung der Wichtigkeiten der verschiedenen Marker-Kanäle. 
Auch für Expert*Innen ist es schwer, eindeutige Beziehungen zwischen den Strukturen, die durch bestimmte Marker gefärbt werden, und den verschiedenen Klassen der Nuclei herzustellen. 
Zur Abschätzung der globalen Markerrelevanz werden die Gradienten der Modellvorhersage bis auf die Eingangsebene zurückprojiziert und die betragsmäßigen Gradienten über die räumlichen Dimensionen gemittelt. 
Die Wichtigkeitswerte, die sich ergeben, sind nahezu gleich verteilt. \newline%, was darauf hindeutet, dass das Modell keine einzelnen Marker stark bevorzugt, sondern  kombinierte Informationen mehrerer Kanäle nutzt, um Entscheidungen zu treffen.
Abb. \ref{fig:marker_gradCAM} zeigt exemplarisch Gradientenfelder einer 2D-Schicht aus einer Stichprobe der dreidimensionalen Zieldaten und deren Kosinus-Ähnlichkeit.
In den rohen Gradienten sind die sensitivsten Bereiche des Modells direkt aus den Ableitungen der Vorhersage bezüglich der Eingabe ablesbar.
Diese Sensitivitäten zeigen intensives Rauschen, weil die Effekte von punktweisen Variationen für jeden Pixel der Eingabedaten dargestellt sind.
In der mittleren Darstellung sind die integrierten Gradienten dargestellt.
Durch die Integration sind Rauscheffekte deutlich reduziert, und eine stabilere, interpretierbarere Verteilung der relevanten Regionen im Gradientenfeld ist sichtbar.
Zur Beurteilung der Konsistenz des Klassifikators werden die Ähnlichkeiten zwischen den Gradienten-Feldern quantitativ verglichen. 
Die Felder haben eine Kosinus-Ähnlichkeit von 0,913 und damit eine hohe Übereinstimmung in der Richtung der Wichtigkeitsverteilungen. 
Der Klassifikator reagiert also auf ähnliche Muster in beiden Feldern. 
Die Region mit besonders dichten Maxima im Gradientenfeld ist durch rote Pfeile gekennzeichnet.
Die Kosinus-Ähnlichkeit ist pixelweise in der rechten Abbildung dem Eingabebild überlagert.
Der hohe Wert spricht für eine stabile und konsistente Aufmerksamkeitslokalisierung des Klassifikators, was wiederum auf eine robuste interne Repräsentation der relevanten Merkmale hindeutet.
\begin{figure}[H]
    \centering\includegraphics[width=0.8\linewidth]{Figures/gradients.pdf}
    \caption{Die Abbildung zeigt zwei Gradienten-Felder einer 2D-Schicht einer Stichprobe der dreidimensionalen Zieldaten und deren Kosinus-Ähnlichkeit.
    Die Kosinus-Ähnlichkeit ist em der 2D-Schicht überlagert.
    Das rechte Gradientenfeld ist integriert, um hochfrequente lokale Unterschiede zu entfernen.
    Die Untersuchung der Kosinus-Ähnlichkeit soll zeigen, ob der Klassifikator konsistent auf bestimmte Strukturen der Eingabe reagiert.
    Mit den roten Pfeilen ist die Region mit besonders dichten lokalen Maxima der integrierten Maxima gekennzeichnet.
    Diese Region stellt eine Struktur dar, die für den Klassifikator konsistent zur Entscheidung beiträgt.
    }
    \label{fig:marker_gradCAM}
\end{figure}
