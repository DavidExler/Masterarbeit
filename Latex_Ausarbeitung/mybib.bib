@article{dosovitskiy2020ViT,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{he2022mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@misc{jrfarm_katze,
  author       = {{JR Farm}},
  title        = {Katzenratgeber – Alles Wissenswerte über Katzen},
  year         = 2025,
  howpublished = {\url{https://www.jr-farm.de/ratgeber/tiere/katze}},
  note         = {Accessed: 2025-06-26}
}

@misc{Egan_Katzen,
author = {Egan, Ben and Redden, Alex and {XWAVE} and {SilentAntagonist}},
month = may,
title = {{Dalle3 1 Million+ High Quality Captions}},
url = {https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions},
year = {2024}
}

%nuclei datensatz papers
@article{edlund2021livecell,
  title={LIVECell—A large-scale dataset for label-free live cell segmentation},
  author={Edlund, Christoffer and Jackson, Timothy R and Khalid, Nabeel and Bevan, Nicola and Dale, Timothy and Dengel, Andreas and Ahmed, Sheraz and Trygg, Johan and Sj{\"o}gren, Rickard},
  journal={Nature methods},
  volume={18},
  number={9},
  pages={1038--1045},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{dietler2020YeaZ,
  title={A convolutional neural network segments yeast microscopy images with high accuracy},
  author={Dietler, Nicola and Minder, Matthias and Gligorovski, Vojislav and Economou, Augoustina Maria and Joly, Denis Alain Henri Lucien and Sadeghi, Ahmad and Chan, Chun Hei Michael and Kozi{\'n}ski, Mateusz and Weigert, Martin and Bitbol, Anne-Florence and others},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={5723},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{cutler2022omnipose,
  title={Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation},
  author={Cutler, Kevin J and Stringer, Carsen and Lo, Teresa W and Rappez, Luca and Stroustrup, Nicholas and Brook Peterson, S and Wiggins, Paul A and Mougous, Joseph D},
  journal={Nature methods},
  volume={19},
  number={11},
  pages={1438--1448},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@misc{holden2021deepbacs,
  author       = {Holden, S. and Conduit, M.},
  title        = {{DeepBacs – Bacillus subtilis fluorescence segmentation dataset}},
  year         = {2021},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.5550968},
  url          = {https://doi.org/10.5281/zenodo.5550968},
  note         = {Data set}
}

@article{cspahn2021deepbacs,
  title={DeepBacs: Bacterial image analysis using open-source deep learning approaches},
  author={Spahn, Christoph and Laine, Romain F. and Matos Pereira, Pedro and von Chamier, Lucas and Conduit, Mia and G{\'o}mez-de-Mariscal, Estibaliz and Gomes de Pinho, Mariana and Jacquemet, Guillaume and Holden, S{\'{e}}amus and Heilemann, Mike and Henriques, Ricardo},
  journal={bioRxiv},
  year={2021},
  doi = {10.1101/2021.11.03.467152},
  publisher = {Cold Spring Harbor Laboratory},
  URL = {https://www.biorxiv.org/content/early/2021/11/03/2021.11.03.467152}
}

@article{kumar2017MoNuSeg,
  title={A dataset and a technique for generalized nuclear segmentation for computational pathology},
  author={Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
  journal={IEEE transactions on medical imaging},
  volume={36},
  number={7},
  pages={1550--1560},
  year={2017},
  publisher={IEEE}
}

@article{greenwald2022Tissuenet,
  title={Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning},
  author={Greenwald, Noah F and Miller, Geneva and Moen, Erick and Kong, Alex and Kagel, Adam and Dougherty, Thomas and Fullaway, Christine Camacho and McIntosh, Brianna J and Leow, Ke Xuan and Schwartz, Morgan Sarah and others},
  journal={Nature biotechnology},
  volume={40},
  number={4},
  pages={555--565},
  year={2022},
  publisher={Nature Publishing Group US New York}
}


%Domain adaption und similarity
@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and March, Mario and Lempitsky, Victor},
  journal={Journal of machine learning research},
  volume={17},
  number={59},
  pages={1--35},
  year={2016}
}

@inproceedings{yuan2005domainsimilarity,
  title={An empirical study on language model adaptation using a metric of domain similarity},
  author={Yuan, Wei and Gao, Jianfeng and Suzuki, Hisami},
  booktitle={International Conference on Natural Language Processing},
  pages={957--968},
  year={2005},
  organization={Springer}
}

@inproceedings{ganin2015domain_big,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={International conference on machine learning},
  pages={1180--1189},
  year={2015},
  organization={PMLR}
}

@inproceedings{pinheiro2018domain,
  title={Unsupervised domain adaptation with similarity learning},
  author={Pinheiro, Pedro O},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8004--8013},
  year={2018}
}

@article{peng2017domain,
  title={Visda: The visual domain adaptation challenge},
  author={Peng, Xingchao and Usman, Ben and Kaushik, Neela and Hoffman, Judy and Wang, Dequan and Saenko, Kate},
  journal={arXiv preprint arXiv:1710.06924},
  year={2017}
}

@article{wang2018domain,
  title={Deep visual domain adaptation: A survey},
  author={Wang, Mei and Deng, Weihong},
  journal={Neurocomputing},
  volume={312},
  pages={135--153},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{cai2009similarity,
  title={Efficient algorithm for computing link-based similarity in real world networks},
  author={Cai, Yuanzhe and Cong, Gao and Jia, Xu and Liu, Hongyan and He, Jun and Lu, Jiaheng and Du, Xiaoyong},
  booktitle={2009 Ninth IEEE International Conference on Data Mining},
  pages={734--739},
  year={2009},
  organization={IEEE}
}

@article{koohi2018similarity,
  title={Cross-domain graph based similarity measurement of workflows},
  author={Koohi-Var, Tahereh and Zahedi, Morteza},
  journal={Journal of Big Data},
  volume={5},
  pages={1--16},
  year={2018},
  publisher={Springer}
}

@article{han2022binsimilarity_domain,
  title={Bin similarity-based domain adaptation for fine-grained image classification},
  author={Han, Tianyu and Zhang, Lifeng and Jia, Shixiang},
  journal={International Journal of Intelligent Systems},
  volume={37},
  number={3},
  pages={2319--2334},
  year={2022},
  publisher={Wiley Online Library}
}

@article{zhu2020domain_similarity,
  title={Domain adaptation using class similarity for robust speech recognition},
  author={Zhu, Han and Zhao, Jiangjiang and Ren, Yuling and Wang, Li and Zhang, Pengyuan},
  journal={arXiv preprint arXiv:2011.02782},
  year={2020}
}

@article{bruch2025,
  title={Improving 3D deep learning segmentation with biophysically motivated cell synthesis},
  author={Bruch, Roman and Vitacolonna, Mario and N{\"u}rnberg, Elina and Sauer, Simeon and Rudolf, R{\"u}diger and Reischl, Markus},
  journal={Communications Biology},
  volume={8},
  number={1},
  pages={43},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

%Segmentierung foundation model
@inproceedings{li2023mask,
  title={Mask dino: Towards a unified transformer-based framework for object detection and segmentation},
  author={Li, Feng and Zhang, Hao and Xu, Huaizhe and Liu, Shilong and Zhang, Lei and Ni, Lionel M and Shum, Heung-Yeung},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3041--3050},
  year={2023}
}

@article{isensee2021nnu,
  title={nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},
  author={Isensee, Fabian and Jaeger, Paul F and Kohl, Simon AA and Petersen, Jens and Maier-Hein, Klaus H},
  journal={Nature methods},
  volume={18},
  number={2},
  pages={203--211},
  year={2021},
  publisher={Nature Publishing Group}
}

%Segmentierung model
@article{moen2019DeepcellCaliban,
  title={Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning},
  author={Moen, Erick and Borba, Enrico and Miller, Geneva and Schwartz, Morgan and Bannon, Dylan and Koe, Nora and Camplisson, Isabella and Kyme, Daniel and Pavelchek, Cole and Price, Tyler and others},
  journal={Biorxiv},
  pages={803205},
  year={2019},
  publisher={Cold Spring Harbor Laboratory}
}

@article{bannon2021deepcell,
  title={DeepCell Kiosk: scaling deep learning--enabled cellular image analysis with Kubernetes},
  author={Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba, Enrico and Kudo, Takamasa and Greenwald, Noah and Vijayakumar, Vibha and Chang, Brian and Pao, Edward and Osterman, Erik and others},
  journal={Nature methods},
  volume={18},
  number={1},
  pages={43--45},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{greenwald2022deepcell,
  title={Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning},
  author={Greenwald, Noah F and Miller, Geneva and Moen, Erick and Kong, Alex and Kagel, Adam and Dougherty, Thomas and Fullaway, Christine Camacho and McIntosh, Brianna J and Leow, Ke Xuan and Schwartz, Morgan Sarah and others},
  journal={Nature biotechnology},
  volume={40},
  number={4},
  pages={555--565},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@article{van2016deepcell,
  title={Deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments},
  author={Van Valen, David A and Kudo, Takamasa and Lane, Keara M and Macklin, Derek N and Quach, Nicolas T and DeFelice, Mialy M and Maayan, Inbal and Tanouchi, Yu and Ashley, Euan A and Covert, Markus W},
  journal={PLoS computational biology},
  volume={12},
  number={11},
  pages={e1005177},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}


%Segmentierung foundation model
@inproceedings{jain2023oneformer,
  title={Oneformer: One transformer to rule universal image segmentation},
  author={Jain, Jitesh and Li, Jiachen and Chiu, Mang Tik and Hassani, Ali and Orlov, Nikita and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2989--2998},
  year={2023}
}

%Segmentierung foundation model
@article{zou2023segment,
  title={Segment everything everywhere all at once},
  author={Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={19769--19782},
  year={2023}
}

%Segmentierung foundation model
@inproceedings{wang2021max,
  title={Max-deeplab: End-to-end panoptic segmentation with mask transformers},
  author={Wang, Huiyu and Zhu, Yukun and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5463--5474},
  year={2021}
}


%Image foundation model, nicht ganz für Segmentierung gedacht
@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International conference on machine learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}

@article{stringer2021cellpose,
  title={Cellpose: a generalist algorithm for cellular segmentation},
  author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
  journal={Nature methods},
  volume={18},
  number={1},
  pages={100--106},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{pachitariu2025samcellpose,
  title={Cellpose-SAM: superhuman generalization for cellular segmentation},
  author={Pachitariu, Marius and Rariden, Michael and Stringer, Carsen},
  journal={bioRxiv},
  pages={2025--04},
  year={2025},
  publisher={Cold Spring Harbor Laboratory}
}

@article{vandeloo2025samfine,
  title={SAMCell: Generalized Label-Free Biological Cell Segmentation with Segment Anything},
  author={VandeLoo, Alexandra D and Malta, Nathan J and Aponte, Emilio and van Zyl, Caitlin and Xu, Danfei and Forest, Craig R},
  journal={bioRxiv},
  year={2025}
}

@article{archit2025samfine,
  title={Segment anything for microscopy},
  author={Archit, Anwai and Freckmann, Luca and Nair, Sushmita and Khalid, Nabeel and Hilt, Paul and Rajashekar, Vikas and Freitag, Marei and Teuber, Carolin and Buckley, Genevieve and von Haaren, Sebastian and others},
  journal={Nature Methods},
  pages={1--13},
  year={2025},
  publisher={Nature Publishing Group US New York}
}

@article{israel2023samfine,
  title={A foundation model for cell segmentation},
  author={Israel, Uriah and Marks, Markus and Dilip, Rohit and Li, Qilin and Schwartz, Morgan and Pradhan, Elora and Pao, Edward and Li, Shenyi and Pearson-Goulart, Alexander and Perona, Pietro and others},
  journal={arXiv preprint arXiv:2311.11004},
  year={2023}
}

@inproceedings{kirillov2023sam,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4015--4026},
  year={2023}
}

@article{dippel2022segmentation,
  title={Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder},
  author={Dippel, Jonas and Lenga, Matthias and Goerttler, Thomas and Obermayer, Klaus and H{\"o}hne, Johannes},
  journal={arXiv preprint arXiv:2207.14508},
  year={2022}
}

@article{bommasani2021foundationmodels,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{li2024PartPQ,
  title={Panoptic-PartFormer++: A unified and decoupled view for panoptic part segmentation},
  author={Li, Xiangtai and Xu, Shilin and Yang, Yibo and Yuan, Haobo and Cheng, Guangliang and Tong, Yunhai and Lin, Zhouchen and Yang, Ming-Hsuan and Tao, Dacheng},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2024},
  publisher={IEEE}
}

@inproceedings{fu2018cycleGAN,
  title={Three dimensional fluorescence microscopy image synthesis and segmentation},
  author={Fu, Chichen and Lee, Soonam and Joon Ho, David and Han, Shuo and Salama, Paul and Dunn, Kenneth W and Delp, Edward J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={2221--2229},
  year={2018}
}

@article{Kromp2020_Dataset,
  author = {Kromp, Florian and Bozsaky, Eva and Rifatbegovic, Fikret and Fischer, Lukas and Ambros, Magdalena and Berneder, Maria and Weiss, Tamara and Lazic, Daria and Doerr, Wolfgang and Hanbury, Allan and Beiske, Klaus and Ambros, Peter F. and Ambros, Inge M. and Taschner-Mandl, Sabine},
  journal = {Nature Scientific Data},
  title = {An annotated fluorescence image dataset for training nuclear segmentation methods},
  year = {2020},
  volume = {7},
  number = {262},
  pages = {1--8},
  doi = {10.1038/s41597-020-00608-w}
}

@article{chen20223_Dataset,
  title={3d ground truth annotations of nuclei in 3d microscopy volumes},
  author={Chen, Alain and Wu, Liming and Winfree, Seth and Dunn, Kenneth W and Salama, Paul and Delp, Edward J},
  journal={bioRxiv},
  pages={2022--09},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{ulman2017cellTrackingChallenge,
  title={An objective comparison of cell-tracking algorithms},
  author={Ulman, Vladim{\'\i}r and Ma{\v{s}}ka, Martin and Magnusson, Klas EG and Ronneberger, Olaf and Haubold, Carsten and Harder, Nathalie and Matula, Pavel and Matula, Petr and Svoboda, David and Radojevic, Miroslav and others},
  journal={Nature methods},
  volume={14},
  number={12},
  pages={1141--1152},
  year={2017},
  publisher={Nature Publishing Group UK London}
}











@book{Boyd2009,
	author		= "Stephen Boyd and Lieven Vandenberghe",
	title		= "Convex {O}ptimization",
	edition		= "7th ed.",
	year		= "2009",
	publisher	= "Cambridge University Press",
	hyphenation     = "english",
}  

@online{EMVA1288,
	author		= "EMVA",
	title		= "{EMVA} {S}tandard 1288",
	subtitle	= "Standard for Characerization of Image Sensors and Cameras. Release 3.1.",
	url		= "http://www.emva.org/standards-technology/emva-1288/",
	version		= "3.1",
	date		= "2016",
	month		= "12",
	year		= "2016",
	hyphenation	= "ngerman",
} 

@online{Hashemi2018,
	title		= {Asymmetric Similarity Loss Function to Balance Precision and Recall in Highly Unbalanced Deep Medical Image Segmentation},
	author		= {Hashemi, S. R. and Salehi, S. S. M. and Erdogmus, D. and Prabhu, S. P. and Warfield S. K. and Gholipour, A.},
	year		= {2018},
	archivePrefix	= "arXiv",
	eprint		= {1803.11078v3},
	hyphenation     = "ngerman",
}

@book{Jaehne2012,
	author		= "Bernd Jähne",
	title		= "Digitale Bildverarbeitung und Bildgewinnung",
	edition		= "7. Auflage",
	year		= "2012",
	publisher	= "Springer",
	doi		= "10.1007/978-3-642-04952-1",
	hyphenation	= "ngerman",
}

@thesis{Scherr2017,
	author		= "Tim Scherr",
	title		= "Gradient-Based Surface Reconstruction and the Application to Wind Waves",
	type		= "Master's Thesis",
	institution	= "Ruprecht-Karls University Heidelberg",
	date		= "2017",
	doi		= "10.11588/heidok.00023653",
	hyphenation	= "english",
}

@thesis{Scherr2017-2,
	author		= "Tim Scherr",
	title		= "Gradient-Based Surface Reconstruction and the Application to Wind Waves",
	type		= "Master's Thesis",
	institution	= "Ruprecht-Karls University Heidelberg",
	date		= "2017",
	doi		= "10.11588/heidok.00023653",
	hyphenation	= "ngerman",
}

@article{Schott2018,
	author 		= "Schott, B. AND Traub, M. AND Schlagenhauf, C. AND Takamiya, M. AND Antritter, T. AND Bartschat, A. AND Löffler, K. AND Blessing, D. AND Otte, J. C. AND Kobitski, A. Y. AND Nienhaus, G. U. AND Strähle, U. AND Mikut, R. AND Stegmaier, J.",
	journal		= "PLOS Computational Biology",
	publisher	= "Public Library of Science",
	title		= "Embryo{M}iner: {A} {N}ew {F}ramework for {I}nteractive {K}nowledge {D}iscovery in {L}arge-{S}cale {C}ell {T}racking {D}ata of {D}eveloping {E}mbryos",
	year		= "2018",
	volume		= "14",
	pages		= "1-18",
	doi		= "10.1371/journal.pcbi.1006128",
	hyphenation	= "english",
}

@InProceedings{Szegedy2015,
	author		= {Szegedy, C. and Liu, W. and Jia, Y. and Sermanet, P. and Reed, S. and Anguelov, D. and Erhan, D. and Vanhoucke, V. and Rabinovich, A.},
	title		= {Going Deeper With Convolutions},
	booktitle	= {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages		= {1-9}, 
	year		= {2015},
	doi		= {10.1109/CVPR.2015.7298594}, 
	hyphenation	= "ngerman",
}  

@InProceedings{Radford2021,
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date      = {2021-07},
  title     = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
  language  = {en},
  pages     = {8748--8763},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v139/radford21a.html},
  urldate   = {2025-07-10},
  abstract  = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  file      = {Full Text PDF:http\://proceedings.mlr.press/v139/radford21a/radford21a.pdf:application/pdf;Supplementary PDF:http\://proceedings.mlr.press/v139/radford21a/radford21a-supp.pdf:application/pdf},
  issn      = {2640-3498},
}

@Article{Weisrock2024,
  author       = {Weisrock, Antoine and Wüst, Rebecca and Olenic, Maria and Lecomte-Grosbras, Pauline and Thorrez, Lieven},
  date         = {2024-10},
  journaltitle = {Tissue Engineering Part A},
  title        = {{MyoFInDer}: {An} {AI}-{Based} {Tool} for {Myotube} {Fusion} {Index} {Determination}},
  doi          = {10.1089/ten.tea.2024.0049},
  issn         = {1937-3341},
  number       = {19-20},
  pages        = {652--661},
  url          = {https://www.liebertpub.com/doi/10.1089/ten.tea.2024.0049},
  urldate      = {2025-07-16},
  volume       = {30},
  file         = {Full Text PDF:https\://www.liebertpub.com/doi/pdf/10.1089/ten.tea.2024.0049:application/pdf},
  publisher    = {Mary Ann Liebert, Inc., publishers},
  shorttitle   = {{MyoFInDer}},
}

@TechReport{Lair2025,
  author      = {Lair, Benjamin and Cazorla, Clément and Lobeto, Alicia and Labour, Axel and Laurens, Claire and Rustan, Arild C. and Weiss, Pierre and Flores-Flores, Remy and Moro, Cedric},
  date        = {2025-02},
  institution = {bioRxiv},
  title       = {{MyoFuse}: {A} fully {AI}-based workflow for automated quantification of skeletal muscle cell fusion in vitro},
  chapter     = {New Results},
  doi         = {10.1101/2025.02.17.638596},
  language    = {en},
  note        = {Type: article},
  pages       = {2025.02.17.638596},
  url         = {https://www.biorxiv.org/content/10.1101/2025.02.17.638596v1},
  urldate     = {2025-07-16},
  abstract    = {Background The myogenic fusion index (FI) is commonly used in skeletal muscle cell culture to assess the ability of myoblasts to form myotubes, as the ratio of myoblast nuclei fused with myotubes over the total number of myoblasts. The manual quantification of the FI from 2D microscopy images is tedious and biased, thus several automated methods have been developed. However, they still face challenges such as efficient nucleus segmentation and classification of fused and isolated myoblast nuclei. Here, we developed a novel workflow entirely based on AI for fully automated and unbiased quantification of the FI.
Results Using current methods, we show that myoblast nuclei located above or below myotubes can significantly corrupt accurate FI computation. To circumvent this issue, we developed MyoFuse which enables an accurate and high-throughput segmentation and classification of myonuclei. It comprises a nuclei segmentation step using Cellpose, followed by a classification network trained with Svetlana. MyoFuse demonstrated strong accuracy when tested against manual annotation in mouse C2C12 and human primary myotubes. The trained classifier is able to differentiate myotube nuclei from myoblast nuclei based on myotube cytoplasm staining only. Experimental comparisons also highlighted that the previously developed methods lead to a significant overestimation of the FI.
Conclusion In summary, we underscore the lack of accuracy of traditional methods for automated FI quantification. MyoFuse enables a direct and accurate segmentation of nuclei even in nuclei clusters frequently observed in myotubes. This workflow thus offers a new and more reliable method to evaluate the FI. It also limits the selection bias by processing large images.},
  copyright   = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file        = {Full Text PDF:https\://www.biorxiv.org/content/biorxiv/early/2025/02/17/2025.02.17.638596.full.pdf:application/pdf},
  shorttitle  = {{MyoFuse}},
}

@Article{Jeong2025,
  author       = {Jeong, Kyungchang and Park, Sanghun and Jo, Gyuchan and Seo, Hanbit and Choi, Nayoung and Jang, Soyoung and Park, Gyutae and Seo, Young-Duk and Brad Kim, Yuan H. and Jeong, Ji-Hoon and Hyun, Sang-Hwan and Choi, Jungseok and Lee, Euijong},
  date         = {2025-03},
  journaltitle = {Computers in Biology and Medicine},
  title        = {{SEPO}-{FI}: {Deep}-learning based software to calculate fusion index of muscle cells},
  doi          = {10.1016/j.compbiomed.2025.109706},
  issn         = {0010-4825},
  pages        = {109706},
  url          = {https://www.sciencedirect.com/science/article/pii/S0010482525000563},
  urldate      = {2025-07-16},
  volume       = {186},
  abstract     = {The fusion index is a critical metric for quantitatively assessing the transformation of in vitro muscle cells into myotubes in the biological and medical fields. Traditional methods for calculating this index manually involve the labor-intensive counting of numerous muscle cell nuclei in images, which necessitates determining whether each nucleus is located inside or outside the myotubes, leading to significant inter-observer variation. To address these challenges, this study proposes a three-stage process that integrates the strengths of pattern recognition and deep-learning to automatically calculate the fusion index. The experimental results demonstrate that the proposed process achieves significantly higher performance in cell nuclei detection and classification, with an F1-score of 0.953, whereas traditional object detection methods achieve less than 0.5. In addition, the fusion index obtained using the proposed method is closely aligned with the human-assessed values, showing minimal discrepancy and strong agreement with human evaluations. This process is incorporated into the development of “SEPO-FI” as public software, automating cell detection and classification to enable effective fusion index calculation and broaden access to this methodology within the scientific community.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0010482525000563/pdfft?download=true:application/pdf},
  keywords     = {Fusion index calculation, Computer vision, Pattern recognition, Deep-learning, Cell detection and classification, Convolutional neural network},
  shorttitle   = {{SEPO}-{FI}},
}

@article{lewis1917behavior,
  title={Behavior of cross striated muscle in tissue cultures},
  author={Lewis, Warren H and Lewis, Margaret R},
  journal={American Journal of Anatomy},
  volume={22},
  number={2},
  pages={169--194},
  year={1917},
  publisher={Wiley Subscription Services, Inc., A Wiley Company Hoboken}
}

@Article{Pogogeff1946,
  author       = {Pogogeff, Irene A. and Murray, Margaret R.},
  date         = {1946},
  journaltitle = {The Anatomical Record},
  title        = {Form and behavior of adult mammalian skeletal muscle in vitro},
  doi          = {10.1002/ar.1090950308},
  issn         = {1097-0185},
  language     = {en},
  number       = {3},
  pages        = {321--335},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ar.1090950308},
  urldate      = {2025-07-16},
  volume       = {95},
  copyright    = {Copyright © 1946 Wiley-Liss, Inc.},
  file         = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ar.1090950308:application/pdf},
}

@Article{Enwere2014,
  author       = {Enwere, Emeka and LaCasse, Eric and Adam, Nadine and Korneluk, Robert},
  date         = {2014-02},
  journaltitle = {Frontiers in Immunology},
  title        = {Role of the {TWEAK}-{Fn14}-{cIAP1}-{NF}-κ{B} {Signaling} {Axis} in the {Regulation} of {Myogenesis} and {Muscle} {Homeostasis}},
  doi          = {10.3389/fimmu.2014.00034},
  pages        = {34},
  volume       = {5},
  abstract     = {Mammalian skeletal muscle maintains a robust regenerative capacity throughout life, largely due to the presence of a stem cell population known as “satellite cells” in the muscle milieu. In normal conditions, these cells remain quiescent; they are activated upon injury to become myoblasts, which proliferate extensively and eventually differentiate and fuse to form new multinucleated muscle fibers. Recent findings have identified some of the factors, including the cytokine TNFα-like weak inducer of apoptosis (TWEAK), which govern these cells’ decisions to proliferate, differentiate, or fuse. In this review, we will address the functions of TWEAK, its receptor Fn14, and the associated signal transduction molecule, the cellular inhibitor of apoptosis 1 (cIAP1), in the regulation of myogenesis. TWEAK signaling can activate the canonical NF-κB signaling pathway, which promotes myoblast proliferation and inhibits myogenesis. In addition, TWEAK activates the non-canonical NF-κB pathway, which, in contrast, promotes myogenesis by increasing myoblast fusion. Both pathways are regulated by cIAP1, which is an essential component of downstream signaling mediated by TWEAK and similar cytokines. This review will focus on the seemingly contradictory roles played by TWEAK during muscle regeneration, by highlighting the interplay between the two NF-κB pathways under physiological and pathological conditions. We will also discuss how myogenesis is negatively affected by chronic conditions, which affect homeostasis of the skeletal muscle environment.},
  file         = {Full Text PDF:https\://www.researchgate.net/journal/Frontiers-in-Immunology-1664-3224/publication/260254398_Role_of_the_TWEAK-Fn14-cIAP1-NF-kB_Signaling_Axis_in_the_Regulation_of_Myogenesis_and_Muscle_Homeostasis/links/66544fd022a7f16b4f4ed833/Role-of-the-TWEAK-Fn14-cIAP1-NF-kB-Signaling-Axis-in-the-Regulation-of-Myogenesis-and-Muscle-Homeostasis.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/260254398_Role_of_the_TWEAK-Fn14-cIAP1-NF-kB_Signaling_Axis_in_the_Regulation_of_Myogenesis_and_Muscle_Homeostasis:},
}

@Article{Scharner2011,
  author       = {Scharner, Juergen and Zammit, Peter S},
  date         = {2011-08},
  journaltitle = {Skeletal Muscle},
  title        = {The muscle satellite cell at 50: the formative years},
  doi          = {10.1186/2044-5040-1-28},
  issn         = {2044-5040},
  pages        = {28},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3177780/},
  urldate      = {2025-07-16},
  volume       = {1},
  abstract     = {In February 1961, Alexander Mauro described a cell 'wedged' between the plasma membrane of the muscle fibre and the surrounding basement membrane. He postulated that it could be a dormant myoblast, poised to repair muscle when needed. In the same month, Bernard Katz also reported a cell in a similar location on muscle spindles, suggesting that it was associated with development and growth of intrafusal muscle fibres. Both Mauro and Katz used the term 'satellite cell' in relation to their discoveries. Today, the muscle satellite cell is widely accepted as the resident stem cell of skeletal muscle, supplying myoblasts for growth, homeostasis and repair., Since 2011 marks both the 50th anniversary of the discovery of the satellite cell, and the launch of Skeletal Muscle, it seems an opportune moment to summarise the seminal events in the history of research into muscle regeneration. We start with the 19th-century pioneers who showed that muscle had a regenerative capacity, through to the descriptions from the mid-20th century of the underlying cellular mechanisms. The journey of the satellite cell from electron microscope curio, to its gradual acceptance as a bona fide myoblast precursor, is then charted: work that provided the foundations for our understanding of the role of the satellite cell. Finally, the rapid progress in the age of molecular biology is briefly discussed, and some ongoing debates on satellite cell function highlighted.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC3177780/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC3177780/pdf/2044-5040-1-28.pdf:application/pdf},
  pmcid        = {PMC3177780},
  pmid         = {21849021},
  shorttitle   = {The muscle satellite cell at 50},
}

@Book{Gilbert2014,
  author    = {Gilbert, Scott F},
  date      = {2014},
  title     = {Developmental biology},
  edition   = {10},
  language  = {english},
  location  = {Sunderland, Massachusetts},
  publisher = {Sinauer Associates},
}

@Article{Brunetti2021,
  author       = {Brunetti, Jessica and Koenig, Stéphane and Monnier, Arthur and Frieden, Maud},
  date         = {2021-05},
  journaltitle = {Skeletal Muscle},
  title        = {Nanopattern surface improves cultured human myotube maturation},
  doi          = {10.1186/s13395-021-00268-3},
  issn         = {2044-5040},
  number       = {1},
  pages        = {12},
  url          = {https://doi.org/10.1186/s13395-021-00268-3},
  urldate      = {2025-07-16},
  volume       = {11},
  abstract     = {In vitro maturation of human primary myoblasts using 2D culture remains a challenging process and leads to immature fibers with poor internal organization and function. This would however represent a valuable system to study muscle physiology or pathophysiology from patient myoblasts, at a single-cell level.},
  file         = {Full Text PDF:https\://skeletalmusclejournal.biomedcentral.com/counter/pdf/10.1186/s13395-021-00268-3:application/pdf},
  keywords     = {Human primary myoblasts, Cell alignment, Myotube maturation, Acetylcholine receptor clusters, Ca2+ signals},
}

@Article{NogalesGadea2010,
  author       = {Nogales-Gadea, Gisela and Mormeneo, Emma and García-Consuegra, Inés and Rubio, Juan C. and Orozco, Anna and Arenas, Joaquin and Martín, Miguel A. and Lucia, Alejandro and Gómez-Foix, Anna M. and Martí, Ramon and Andreu, Antoni L.},
  date         = {2010-10},
  journaltitle = {PLOS ONE},
  title        = {Expression of {Glycogen} {Phosphorylase} {Isoforms} in {Cultured} {Muscle} from {Patients} with {McArdle}'s {Disease} {Carrying} the p.{R771PfsX33} {PYGM} {Mutation}},
  doi          = {10.1371/journal.pone.0013164},
  issn         = {1932-6203},
  language     = {en},
  number       = {10},
  pages        = {e13164},
  url          = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0013164},
  urldate      = {2025-07-16},
  volume       = {5},
  abstract     = {Background Mutations in the PYGM gene encoding skeletal muscle glycogen phosphorylase (GP) cause a metabolic disorder known as McArdle's disease. Previous studies in muscle biopsies and cultured muscle cells from McArdle patients have shown that PYGM mutations abolish GP activity in skeletal muscle, but that the enzyme activity reappears when muscle cells are in culture. The identification of the GP isoenzyme that accounts for this activity remains controversial. Methodology/Principal Findings In this study we present two related patients harbouring a novel PYGM mutation, p.R771PfsX33. In the patients' skeletal muscle biopsies, PYGM mRNA levels were ∼60\% lower than those observed in two matched healthy controls; biochemical analysis of a patient muscle biopsy resulted in undetectable GP protein and GP activity. A strong reduction of the PYGM mRNA was observed in cultured muscle cells from patients and controls, as compared to the levels observed in muscle tissue. In cultured cells, PYGM mRNA levels were negligible regardless of the differentiation stage. After a 12 day period of differentiation similar expression of the brain and liver isoforms were observed at the mRNA level in cells from patients and controls. Total GP activity (measured with AMP) was not different either; however, the active GP activity and immunoreactive GP protein levels were lower in patients' cell cultures. GP immunoreactivity was mainly due to brain and liver GP but muscle GP seemed to be responsible for the differences. Conclusions/Significance These results indicate that in both patients' and controls' cell cultures, unlike in skeletal muscle tissue, most of the protein and GP activities result from the expression of brain GP and liver GP genes, although there is still some activity resulting from the expression of the muscle GP gene. More research is necessary to clarify the differential mechanisms of metabolic adaptations that McArdle cultures undergo in vitro.},
  file         = {Full Text PDF:https\://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0013164&type=printable:application/pdf},
  keywords     = {Skeletal muscles, Glycogens, Cell cultures, Muscle proteins, Muscle differentiation, Muscle cells, Biopsy, Frameshift mutation},
  publisher    = {Public Library of Science},
}

@Article{Chua2019,
  author       = {Chua, Min‐Wen Jason and Yildirim, Ege Deniz and Tan, Jun‐Hao Elwin and Chua, Yan‐Jiang Benjamin and Low, Suet‐Mei Crystal and Ding, Suet Lee Shirley and Li, Chun‐wei and Jiang, Zongmin and Teh, Bin Tean and Yu, Kang and Shyh‐Chang, Ng},
  date         = {2019-03},
  journaltitle = {Cell Proliferation},
  title        = {Assessment of different strategies for scalable production and proliferation of human myoblasts},
  doi          = {10.1111/cpr.12602},
  issn         = {0960-7722},
  number       = {3},
  pages        = {e12602},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6536385/},
  urldate      = {2025-07-16},
  volume       = {52},
  abstract     = {Objectives
Myoblast transfer therapy (MTT) is a technique to replace muscle satellite cells with genetically repaired or healthy myoblasts, to treat muscular dystrophies. However, clinical trials with human myoblasts were ineffective, showing almost no benefit with MTT. One important obstacle is the rapid senescence of human myoblasts. The main purpose of our study was to compare the various methods for scalable generation of proliferative human myoblasts.

Methods
We compared the immortalization of primary myoblasts with hTERT, cyclin D1 and CDK4R24C, two chemically defined methods for deriving myoblasts from pluripotent human embryonic stem cells (hESCs), and introduction of viral MyoD into hESC‐myoblasts.

Results
Our results show that, while all the strategies above are suboptimal at generating bona fide human myoblasts that can both proliferate and differentiate robustly, chemically defined hESC‐monolayer‐myoblasts show the most promise in differentiation potential.

Conclusions
Further efforts to optimize the chemically defined differentiation of hESC‐monolayer‐myoblasts would be the most promising strategy for the scalable generation of human myoblasts, for applications in MTT and high‐throughput drug screening.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC6536385/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC6536385/pdf/CPR-52-e12602.pdf:application/pdf},
  pmcid        = {PMC6536385},
  pmid         = {30891802},
}

@Article{Shahini2018,
  author       = {Shahini, Aref and Vydiam, Kalyan and Choudhury, Debanik and Rajabian, Nika and Nguyen, Thy and Lei, Pedro and Andreadis, Stelios T.},
  date         = {2018-07},
  journaltitle = {Stem cell research},
  title        = {Efficient and high yield isolation of myoblasts from skeletal muscle},
  doi          = {10.1016/j.scr.2018.05.017},
  issn         = {1873-5061},
  pages        = {122--129},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6090567/},
  urldate      = {2025-07-16},
  volume       = {30},
  abstract     = {Skeletal muscle (SkM) regeneration relies on the activity of myogenic progenitors that reside beneath the basal lamina of myofibers. Here, we describe a protocol for the isolation of the SkM progenitors from young and old mice by exploiting their outgrowth potential from SkM explants on matrigel coated dishes in the presence of high serum, chicken embryo extract and basic fibroblast growth factor. Compared to other protocols, this method yields a higher number of myoblasts (10–20 million) by enabling the outgrowth of these cells from tissue fragments. The majority of outgrowth cells ({\textasciitilde}90\%) were positive for myogenic markers such as α7-integrin, MyoD, and Desmin. The myogenic cell population could be purified to 98\% with one round of pre-plating on collagen coated dishes, where differential attachment of fibroblasts and other non-myogenic progenitors separates them from myoblasts. Moreover, the combination of high serum medium and matrigel coating provided a proliferation advantage to myogenic cells, which expanded rapidly ({\textasciitilde}24 h population doubling), while non-myogenic cells diminished over time, thereby eliminating the need for further purification steps such as FACS sorting. Finally, myogenic progenitors gave rise to multinucleated myotubes that exhibited sarcomeres and spontaneous beating in the culture dish.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC6090567/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC6090567/pdf/nihms1500572.pdf:application/pdf},
  pmcid        = {PMC6090567},
  pmid         = {29879622},
}

@Article{Nolan2024,
  author       = {Nolan, Andy and Heaton, Robert A. and Adamova, Petra and Cole, Paige and Turton, Nadia and Gillham, Scott H. and Owens, Daniel J. and Sexton, Darren W.},
  date         = {2024-05},
  journaltitle = {Cytometry. Part A: The Journal of the International Society for Analytical Cytology},
  title        = {Fluorescent characterization of differentiated myotubes using flow cytometry},
  doi          = {10.1002/cyto.a.24822},
  issn         = {1552-4930},
  language     = {eng},
  number       = {5},
  pages        = {332--344},
  volume       = {105},
  abstract     = {Flow cytometry is routinely used in the assessment of skeletal muscle progenitor cell (myoblast) populations. However, a full gating strategy, inclusive of difficult to interpret forward and side scatter data, which documents cytometric analysis of differentiated myoblasts (myotubes) has not been reported. Beyond changes in size and shape, there are substantial metabolic and protein changes in myotubes allowing for their potential identification within heterogenous cell suspensions. To establish the utility of flow cytometry for determination of myoblasts and myotubes, C2C12 murine cell populations were assessed for cell morphology and metabolic reprogramming. Laser scatter, both forward (FSC; size) and side (SSC; granularity), measured cell morphology, while mitochondrial mass, reactive oxygen species (ROS) generation and DNA content were quantified using the fluorescent probes, MitoTracker green, CM-H2DCFDA and Vybrant DyeCycle, respectively. Immunophenotyping for myosin heavy chain (MyHC) was utilized to confirm myotube differentiation. Cellular viability was determined using Annexin V/propidium iodide dual labelling. Fluorescent microscopy was employed to visualize fluorescence and morphology. Myotube and myoblast populations were resolvable through non-intuitive interpretation of laser scatter-based morphology assessment and mitochondrial mass and activity assessment. Myotubes appeared to have similar sizes to the myoblasts based on laser scatter but exhibited greater mitochondrial mass (159\%, p {\textless} 0.0001), ROS production (303\%, p {\textless} 0.0001), DNA content (18\%, p {\textless} 0.001) and expression of MyHC (147\%, p {\textless} 0.001) compared to myoblasts. Myotube sub-populations contained a larger viable cluster of cells which were unable to be fractionated from myoblast populations and a smaller population cluster which likely contains apoptotic bodies. Imaging of differentiated myoblasts that had transited through the flow cytometer revealed the presence of intact, 'rolled-up' myotubes, which would alter laser scatter properties and potential transit through the laser beam. Our results indicate that myotubes can be analyzed successfully using flow cytometry. Increased mitochondrial mass, ROS and DNA content are key features that correlate with MyHC expression but due to myotubes 'rolling up' during flow cytometric analysis, laser scatter determination of size is not positively correlated; a phenomenon observed with some size determination particles and related to surface properties of said particles. We also note a greater heterogeneity of myotubes compared to myoblasts as evidenced by the 2 distinct sub-populations. We suggest that acoustic focussing may prove effective in identifying myotube sub populations compared to traditional hydrodynamic focussing.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/38092660:text/html},
  keywords     = {Flow Cytometry, Muscle Fibers, Skeletal, Animals, Mice, Cell Differentiation, Reactive Oxygen Species, Cell Line, Myoblasts, Mitochondria, Fluorescent Dyes, Myosin Heavy Chains, Cell Survival, C2C12, flow cytometry, myoblasts, myotubes},
  pmid         = {38092660},
}

@Article{Agley2012,
  author       = {Agley, Chibeza C. and Velloso, Cristiana P. and Lazarus, Norman R. and Harridge, Stephen D. R.},
  date         = {2012-06},
  journaltitle = {Journal of Histochemistry and Cytochemistry},
  title        = {An {Image} {Analysis} {Method} for the {Precise} {Selection} and {Quantitation} of Fluorescently {Labeled} {Cellular} {Constituents}},
  doi          = {10.1369/0022155412442897},
  issn         = {0022-1554},
  number       = {6},
  pages        = {428--438},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3393072/},
  urldate      = {2025-07-16},
  volume       = {60},
  abstract     = {The accurate measurement of the morphological characteristics of cells with nonuniform
conformations presents difficulties. We report here a straightforward method using
immunofluorescent staining and the commercially available imaging program Adobe Photoshop,
which allows objective and precise information to be gathered on irregularly shaped cells.
We have applied this measurement technique to the analysis of human muscle cells and their
immunologically marked intracellular constituents, as these cells are prone to adopting a
highly branched phenotype in culture. Use of this method can be used to overcome many of
the long-standing limitations of conventional approaches for quantifying muscle cell size
in vitro. In addition, wider applications of Photoshop as a quantitative and
semiquantitative tool in immunocytochemistry are explored.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC3393072/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC3393072/pdf/10.1369_0022155412442897.pdf:application/pdf},
  pmcid        = {PMC3393072},
  pmid         = {22511600},
}

@Article{Velica2011,
  author       = {Veliça, Pedro and Bunce, Chris M.},
  date         = {2011-09},
  journaltitle = {Muscle \& Nerve},
  title        = {A quick, simple and unbiased method to quantify {C2C12} myogenic differentiation},
  doi          = {10.1002/mus.22056},
  issn         = {1097-4598},
  language     = {eng},
  number       = {3},
  pages        = {366--370},
  volume       = {44},
  abstract     = {INTRODUCTION: C2C12 myoblasts undergo in vitro myogenesis to form protein-rich multinucleated myotubes. Determining the fraction of total nuclei incorporated into myotubes is a commonly used method to quantify the extent of differentiation, but it is labor-intensive and susceptible to operator bias.
METHODS: We have developed a simple method to quantify myotube formation using micrographs of Jenner-Giemsa-stained C2C12 cultures. Because myotubes are darkly stained by Jenner-Giemsa dyes, the extent of myotube formation correlates with an increase in pixels attributed to the darkest tones. Thus, image histograms were obtained from photographs using ImageJ software, and the sum of the darkest tones was used as a measure of myotube density.
RESULTS: Measurements of myotube density mirrored those of fusion index during C2C12 differentiation and after treatment with prostaglandin D(2) , an inhibitor of C2C12 myogenesis.
CONCLUSIONS: We propose this inexpensive, quick, and unbiased method to quantify C2C12 differentiation as a complement of the fusion index analysis.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/21996796:text/html},
  keywords     = {Animals, Cell Differentiation, Cell Line, Image Processing, Computer-Assisted, Mice, Microscopy, Muscle Development, Muscle Fibers, Skeletal, Muscle, Skeletal, Myoblasts, Skeletal, Software},
  pmid         = {21996796},
}

@Article{Noe2022,
  author       = {Noë, Simon and Corvelyn, Marlies and Willems, Sarah and Costamagna, Domiziana and Aerts, Jean-Marie and Van Campenhout, Anja and Desloovere, Kaat},
  date         = {2022-06},
  journaltitle = {Skeletal Muscle},
  title        = {The {Myotube} {Analyzer}: how to assess myogenic features in muscle stem cells},
  doi          = {10.1186/s13395-022-00297-6},
  issn         = {2044-5040},
  language     = {en},
  number       = {1},
  pages        = {12},
  url          = {https://doi.org/10.1186/s13395-022-00297-6},
  urldate      = {2025-07-16},
  volume       = {12},
  abstract     = {The analysis of in vitro cultures of human adult muscle stem cells obtained from biopsies delineates the potential of skeletal muscles and may help to understand altered muscle morphology in patients. In these analyses, the fusion index is a commonly used quantitative metric to assess the myogenic potency of the muscle stem cells. Since the fusion index only partly describes myogenic potency, we developed the Myotube Analyzer tool, which combines the definition of the fusion index with extra features of myonuclei and myotubes obtained from satellite cell cultures.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1186%2Fs13395-022-00297-6.pdf:application/pdf},
  keywords     = {Electromyography, Muscle stem cells, Muscle, Muscle Physiology, Myosin, Skeletal Muscle, Cerebral palsy, Satellite cell cultures, Fusion index, Image analysis},
  shorttitle   = {The {Myotube} {Analyzer}},
}

@Article{Abdelmoez2020,
  author       = {Abdelmoez, Ahmed M. and Sardón Puig, Laura and Smith, Jonathon A. B. and Gabriel, Brendan M. and Savikj, Mladen and Dollet, Lucile and Chibalin, Alexander V. and Krook, Anna and Zierath, Juleen R. and Pillon, Nicolas J.},
  date         = {2020-03},
  journaltitle = {American Journal of Physiology - Cell Physiology},
  title        = {Comparative profiling of skeletal muscle models reveals heterogeneity of transcriptome and metabolism},
  doi          = {10.1152/ajpcell.00540.2019},
  issn         = {0363-6143},
  number       = {3},
  pages        = {C615--C626},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7099524/},
  urldate      = {2025-07-16},
  volume       = {318},
  abstract     = {Rat L6, mouse C2C12, and primary human skeletal muscle cells (HSMCs) are commonly used to study biological processes in skeletal muscle, and experimental data on these models are abundant. However, consistently matched experimental data are scarce, and comparisons between the different cell types and adult tissue are problematic. We hypothesized that metabolic differences between these cellular models may be reflected at the mRNA level. Publicly available data sets were used to profile mRNA levels in myotubes and skeletal muscle tissues. L6, C2C12, and HSMC myotubes were assessed for proliferation, glucose uptake, glycogen synthesis, mitochondrial activity, and substrate oxidation, as well as the response to in vitro contraction. Transcriptomic profiling revealed that mRNA of genes coding for actin and myosin was enriched in C2C12, whereas L6 myotubes had the highest levels of genes encoding glucose transporters and the five complexes of the mitochondrial electron transport chain. Consistently, insulin-stimulated glucose uptake and oxidative capacity were greatest in L6 myotubes. Insulin-induced glycogen synthesis was highest in HSMCs, but C2C12 myotubes had higher baseline glucose oxidation. All models responded to electrical pulse stimulation-induced glucose uptake and gene expression but in a slightly different manner. Our analysis reveals a great degree of heterogeneity in the transcriptomic and metabolic profiles of L6, C2C12, or primary human myotubes. Based on these distinct signatures, we provide recommendations for the appropriate use of these models depending on scientific hypotheses and biological relevance.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC7099524/:text/html},
  pmcid        = {PMC7099524},
  pmid         = {31825657},
}

@Article{Inoue2018,
  author       = {Inoue, Haruki and Kunida, Katsuyuki and Matsuda, Naoki and Hoshino, Daisuke and Wada, Takumi and Imamura, Hiromi and Noji, Hiroyuki and Kuroda, Shinya},
  date         = {2018-08},
  journaltitle = {Cell Structure and Function},
  title        = {Automatic {Quantitative} {Segmentation} of {Myotubes} {Reveals} {Single}-cell {Dynamics} of {S6} {Kinase} {Activation}},
  doi          = {10.1247/csf.18012},
  issn         = {1347-3700},
  language     = {eng},
  number       = {2},
  pages        = {153--169},
  volume       = {43},
  abstract     = {Automatic cell segmentation is a powerful method for quantifying signaling dynamics at single-cell resolution in live cell fluorescence imaging. Segmentation methods for mononuclear and round shape cells have been developed extensively. However, a segmentation method for elongated polynuclear cells, such as differentiated C2C12 myotubes, has yet to be developed. In addition, myotubes are surrounded by undifferentiated reserve cells, making it difficult to identify background regions and subsequent quantification. Here we developed an automatic quantitative segmentation method for myotubes using watershed segmentation of summed binary images and a two-component Gaussian mixture model. We used time-lapse fluorescence images of differentiated C2C12 cells stably expressing Eevee-S6K, a fluorescence resonance energy transfer (FRET) biosensor of S6 kinase (S6K). Summation of binary images enhanced the contrast between myotubes and reserve cells, permitting detection of a myotube and a myotube center. Using a myotube center instead of a nucleus, individual myotubes could be detected automatically by watershed segmentation. In addition, a background correction using the two-component Gaussian mixture model permitted automatic signal intensity quantification in individual myotubes. Thus, we provide an automatic quantitative segmentation method by combining automatic myotube detection and background correction. Furthermore, this method allowed us to quantify S6K activity in individual myotubes, demonstrating that some of the temporal properties of S6K activity such as peak time and half-life of adaptation show different dose-dependent changes of insulin between cell population and individuals.Key words: time lapse images, cell segmentation, fluorescence resonance energy transfer, C2C12, myotube.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/30047513:text/html},
  keywords     = {Animals, Enzyme Activation, Fluorescence Resonance Energy Transfer, Image Processing, Computer-Assisted, Mice, Muscle Fibers, Skeletal, Optical Imaging, Ribosomal Protein S6 Kinases, Single-Cell Analysis, C2C12, cell segmentation, fluorescence resonance energy transfer, myotube, time lapse images},
  pmid         = {30047513},
}

@InCollection{Fukunaga1993,
  author    = {Fukunaga, Keinosuke},
  date      = {1993-08},
  title     = {Statistical pattern recognition},
  doi       = {10.1142/9789814343138_0002},
  isbn      = {9789810211363},
  pages     = {33--60},
  publisher = {WORLD SCIENTIFIC},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/9789814343138_0002},
  urldate   = {2025-07-31},
  file      = {Full Text PDF:https\://www.worldscientific.com/doi/pdf/10.1142/9789814343138_0002:application/pdf},
  keywords  = {Statistical pattern recognition, classifier, probability of error, hypothesis tests, effect of sample size, nonparametric, Statistical pattern recognition, classifier, probability of error, hypothesis tests, effect of sample size, nonparametric},
}

@Book{ShalevShwartz2014,
  author     = {Shalev-Shwartz, Shai and Ben-David, Shai},
  date       = {2014-05},
  title      = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
  isbn       = {9781139952743},
  language   = {en},
  note       = {Google-Books-ID: Hf6QAwAAQBAJ},
  publisher  = {Cambridge University Press},
  abstract   = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
  file       = {Google Books Link:https\://books.google.de/books?id=Hf6QAwAAQBAJ:text/html},
  keywords   = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Optical Data Processing},
  shorttitle = {Understanding {Machine} {Learning}},
}

@Article{Loog2018,
  author       = {Loog, Marco},
  date         = {2018},
  journaltitle = {Machine Learning Techniques for Space Weather},
  title        = {Supervised classification: {Quite} a brief overview},
  pages        = {113--145},
  url          = {https://www.sciencedirect.com/science/article/pii/B9780128117880000056},
  urldate      = {2025-07-31},
  file         = {Available Version (via Google Scholar):https\://arxiv.org/pdf/1710.09230:application/pdf},
  publisher    = {Elsevier},
  shorttitle   = {Supervised classification},
}

@Article{Kulkarni1998,
  author       = {Kulkarni, S.R. and Lugosi, G. and Venkatesh, S.S.},
  date         = {1998-10},
  journaltitle = {IEEE Transactions on Information Theory},
  title        = {Learning pattern classification-a survey},
  doi          = {10.1109/18.720536},
  issn         = {1557-9654},
  number       = {6},
  pages        = {2178--2206},
  url          = {https://ieeexplore.ieee.org/abstract/document/720536},
  urldate      = {2025-07-31},
  volume       = {44},
  abstract     = {Classical and recent results in statistical pattern recognition and learning theory are reviewed in a two-class pattern classification setting. This basic model best illustrates intuition and analysis techniques while still containing the essential features and serving as a prototype for many applications. Topics discussed include nearest neighbor, kernel, and histogram methods, Vapnik-Chervonenkis theory, and neural networks. The presentation and the large (though nonexhaustive) list of references is geared to provide a useful overview of this field for both specialists and nonspecialists.},
  file         = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=720536&ref=:application/pdf},
  keywords     = {Pattern recognition, Neural networks, Pattern classification, Prototypes, Information theory, Senior members, Nearest neighbor searches, Kernel, Histograms},
}

@Article{Kotsiantis2007,
  author       = {Kotsiantis, Sotiris B. and Zaharakis, Ioannis and Pintelas, P.},
  date         = {2007},
  journaltitle = {Emerging artificial intelligence applications in computer engineering},
  title        = {Supervised machine learning: {A} review of classification techniques},
  number       = {1},
  pages        = {3--24},
  url          = {https://books.google.com/books?hl=de&lr=&id=vLiTXDHr_sYC&oi=fnd&pg=PA3&dq=Kotsiantis+(2007)+Supervised+machine+learning:+A+review+of+classification+techniques&ots=C_pvsyXHon&sig=cptgUhNhe2RQAWGK255Kwyti0R4},
  urldate      = {2025-07-31},
  volume       = {160},
  file         = {Available Version (via Google Scholar):https\://informatica.si/index.php/informatica/article/viewFile/148/140:application/pdf},
  publisher    = {Amsterdam},
  shorttitle   = {Supervised machine learning},
}

@Article{Mutlag2020,
  author       = {Mutlag, Wamidh K. and Ali, Shaker K. and Aydam, Zahoor M. and Taher, Bahaa H.},
  date         = {2020-07},
  journaltitle = {Journal of Physics: Conference Series},
  title        = {Feature {Extraction} {Methods}: {A} {Review}},
  doi          = {10.1088/1742-6596/1591/1/012028},
  issn         = {1742-6596},
  language     = {en},
  number       = {1},
  pages        = {012028},
  url          = {https://dx.doi.org/10.1088/1742-6596/1591/1/012028},
  urldate      = {2025-07-31},
  volume       = {1591},
  abstract     = {Feature extraction is the main core in diagnosis, classification, clustering, recognition, and detection. Many researchers may by interesting in choosing suitable features that used in the applications. In this paper, the most important features methods are collected, and explained each one. The features in this paper are divided into four groups; Geometric features, Statistical features, Texture features, and Color features. It explains the methodology of each method, its equations, and application. In this paper, we made acomparison among them by using two types of image, one type for face images (163 images divided into 113 for training and 50 for testing) and the other for plant images(130 images divided into 100 for training and 30 for testing) to test the features in geometric and textures. Each type of image group shows that each type of images may be used suitable features may differ from other types.},
  file         = {IOP Full Text PDF:https\://iopscience.iop.org/article/10.1088/1742-6596/1591/1/012028/pdf:application/pdf},
  publisher    = {IOP Publishing},
  shorttitle   = {Feature {Extraction} {Methods}},
}

@InCollection{Guyon2006,
  author    = {Guyon, Isabelle and Elisseeff, André},
  date      = {2006},
  title     = {An {Introduction} to {Feature} {Extraction}},
  doi       = {10.1007/978-3-540-35488-8_1},
  editor    = {Guyon, Isabelle and Nikravesh, Masoud and Gunn, Steve and Zadeh, Lotfi A.},
  isbn      = {9783540354888},
  language  = {en},
  location  = {Berlin, Heidelberg},
  pages     = {1--25},
  publisher = {Springer},
  url       = {https://doi.org/10.1007/978-3-540-35488-8_1},
  urldate   = {2025-07-31},
  abstract  = {This chapter introduces the reader to the various aspects of feature extraction covered in this book. Section 1 reviews definitions and notations and proposes a unified view of the feature extraction problem. Section 2 is an overview of the methods and results presented in the book, emphasizing novel contributions. Section 3 provides the reader with an entry point in the field of feature extraction by showing small revealing examples and describing simple but effective algorithms. Finally, Section 4 introduces a more theoretical formalism and points to directions of research and open problems.},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-3-540-35488-8_1.pdf:application/pdf},
}

@InProceedings{Kunaver2005,
  author   = {Kunaver, M. and Tasic, J.F.},
  date     = {2005-11},
  title    = {Image feature extraction - an overview},
  doi      = {10.1109/EURCON.2005.1629889},
  pages    = {183--186},
  url      = {https://ieeexplore.ieee.org/abstract/document/1629889},
  urldate  = {2025-08-04},
  volume   = {1},
  abstract = {The paper presents a short overview over many different techniques for feature extraction. Feature extraction is a very important field of image processing and object recognition. Two different levels of feature extraction are also presented and the connection between them is explained.},
  file     = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=1629889&ref=:application/pdf},
  keywords = {Feature extraction, Colored noise, Shape, Image processing, Image segmentation, Histograms, Object recognition, Helium, Computer vision, Roads, feature extraction, overview, image processing},
}

@Article{Yuan2022,
  author       = {Yuan, Zhecheng and Xue, Zhengrong and Yuan, Bo and Wang, Xueqian and Wu, Yi and Gao, Yang and Xu, Huazhe},
  date         = {2022-12},
  journaltitle = {Advances in Neural Information Processing Systems},
  title        = {Pre-{Trained} {Image} {Encoder} for {Generalizable} {Visual} {Reinforcement} {Learning}},
  language     = {en},
  pages        = {13022--13037},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/548a482d4496ce109cddfbeae5defa7d-Abstract-Conference.html},
  urldate      = {2025-08-04},
  volume       = {35},
  file         = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2022/file/548a482d4496ce109cddfbeae5defa7d-Paper-Conference.pdf:application/pdf},
}

@InProceedings{Bain2021,
  author     = {Bain, Max and Nagrani, Arsha and Varol, Gül and Zisserman, Andrew},
  date       = {2021},
  title      = {Frozen in {Time}: {A} {Joint} {Video} and {Image} {Encoder} for {End}-to-{End} {Retrieval}},
  language   = {en},
  pages      = {1728--1738},
  url        = {https://openaccess.thecvf.com/content/ICCV2021/html/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.html},
  urldate    = {2025-08-04},
  file       = {Full Text PDF:https\://openaccess.thecvf.com/content/ICCV2021/papers/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.pdf:application/pdf},
  shorttitle = {Frozen in {Time}},
}

@InProceedings{Krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  date      = {2012},
  title     = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate   = {2025-08-04},
  volume    = {25},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf:application/pdf},
}

@Article{Lecun1998,
  author       = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date         = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  title        = {Gradient-based learning applied to document recognition},
  doi          = {10.1109/5.726791},
  issn         = {1558-2256},
  number       = {11},
  pages        = {2278--2324},
  url          = {https://ieeexplore.ieee.org/abstract/document/726791},
  urldate      = {2025-08-04},
  volume       = {86},
  abstract     = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  file         = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=726791&ref=:application/pdf},
  keywords     = {Neural networks, Pattern recognition, Machine learning, Optical character recognition software, Character recognition, Feature extraction, Multi-layer neural network, Optical computing, Hidden Markov models, Principal component analysis},
}

@Article{Khan2023,
  author       = {Khan, Asifullah and Rauf, Zunaira and Sohail, Anabia and Khan, Abdul Rehman and Asif, Hifsa and Asif, Aqsa and Farooq, Umair},
  date         = {2023-12},
  journaltitle = {Artificial Intelligence Review},
  title        = {A survey of the vision transformers and their {CNN}-transformer based variants},
  doi          = {10.1007/s10462-023-10595-0},
  issn         = {1573-7462},
  language     = {en},
  number       = {3},
  pages        = {2917--2970},
  url          = {https://doi.org/10.1007/s10462-023-10595-0},
  urldate      = {2025-08-04},
  volume       = {56},
  abstract     = {Vision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs10462-023-10595-0.pdf:application/pdf},
  keywords     = {Computer Vision, Intervision, Object vision, Pattern vision, Transformation Optics, Computer Imaging, Vision, Pattern Recognition and Graphics, Auto encoder, Channel boosting, Computer vision, Convolutional neural networks, Deep learning, Hybrid vision transformers, Image processing, Self-attention, Transformer},
}

@Article{Elngar2021,
  author       = {Elngar, Ahmed A. and Arafa, Mohamed and Fathy, Amar and Moustafa, Basma and Mahmoud, Omar and Shaban, Mohamed and Fawzy, Nehal},
  date         = {2021},
  journaltitle = {Journal of Cybersecurity and Information Management},
  title        = {Image classification based on {CNN}: a survey},
  number       = {1},
  pages        = {18--50},
  url          = {https://www.academia.edu/download/90135273/p2.pdf},
  urldate      = {2025-08-04},
  volume       = {6},
  file         = {Available Version (via Google Scholar):https\://www.academia.edu/download/90135273/p2.pdf:application/pdf},
  shorttitle   = {Image classification based on {CNN}},
}

@Article{Plested2022,
  author       = {Plested, Jo and Gedeon, Tom},
  date         = {2022-05},
  journaltitle = {arXiv preprint arXiv:2205.09904},
  title        = {Deep transfer learning for image classification: a survey},
  doi          = {10.48550/arXiv.2205.09904},
  note         = {arXiv:2205.09904 [cs]},
  url          = {http://arxiv.org/abs/2205.09904},
  urldate      = {2025-08-04},
  abstract     = {Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes in image classification in recent years. It has been consistently demonstrated that best practice for image classification is when large deep models can be trained on abundant labelled data. However there are many real world scenarios where the requirement for large amounts of training data to get the best performance cannot be met. In these scenarios transfer learning can help improve performance. To date there have been no surveys that comprehensively review deep transfer learning as it relates to image classification overall. However, several recent general surveys of deep transfer learning and ones that relate to particular specialised target image classification tasks have been published. We believe it is important for the future progress in the field that all current knowledge is collated and the overarching patterns analysed and discussed. In this survey we formally define deep transfer learning and the problem it attempts to solve in relation to image classification. We survey the current state of the field and identify where recent progress has been made. We show where the gaps in current knowledge are and make suggestions for how to progress the field to fill in these knowledge gaps. We present a new taxonomy of the applications of transfer learning for image classification. This taxonomy makes it easier to see overarching patterns of where transfer learning has been effective and, where it has failed to fulfill its potential. This also allows us to suggest where the problems lie and how it could be used more effectively. We show that under this new taxonomy, many of the applications where transfer learning has been shown to be ineffective or even hinder performance are to be expected when taking into account the source and target datasets and the techniques used.},
  collaborator = {Plested, Jo and Gedeon, Tom},
  file         = {Preprint PDF:http\://arxiv.org/pdf/2205.09904v1:application/pdf},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
  publisher    = {arXiv},
}

@Article{Schmidhuber2015,
  author       = {Schmidhuber, Jürgen},
  date         = {2015-01},
  journaltitle = {Neural Networks},
  title        = {Deep learning in neural networks: {An} overview},
  doi          = {10.1016/j.neunet.2014.09.003},
  issn         = {0893-6080},
  pages        = {85--117},
  url          = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
  urldate      = {2025-08-04},
  volume       = {61},
  abstract     = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0893608014002135/pdfft?download=true:application/pdf},
  keywords     = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
  shorttitle   = {Deep learning in neural networks},
}

@Article{Ghods2019,
  author       = {Ghods, Alireza and Cook, Diane J},
  date         = {2019-09},
  journaltitle = {arXiv preprint arXiv:1909.04791},
  title        = {A {Survey} of {Techniques} {All} {Classifiers} {Can} {Learn} from {Deep} {Networks}: {Models}, {Optimizations}, and {Regularization}},
  doi          = {10.48550/arXiv.1909.04791},
  note         = {arXiv:1909.04791 [cs]},
  url          = {http://arxiv.org/abs/1909.04791},
  urldate      = {2025-08-04},
  abstract     = {Deep neural networks have introduced novel and useful tools to the machine learning community. Other types of classifiers can potentially make use of these tools as well to improve their performance and generality. This paper reviews the current state of the art for deep learning classifier technologies that are being used outside of deep neural networks. Non-network classifiers can employ many components found in deep neural network architectures. In this paper, we review the feature learning, optimization, and regularization methods that form a core of deep network technologies. We then survey non-neural network learning algorithms that make innovative use of these methods to improve classification. Because many opportunities and challenges still exist, we discuss directions that can be pursued to expand the area of deep learning for a variety of classification algorithms.},
  collaborator = {Ghods, Alireza and Cook, Diane J.},
  file         = {Preprint PDF:http\://arxiv.org/pdf/1909.04791v2:application/pdf},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  publisher    = {arXiv},
}

@InProceedings{Kirillov2019a,
  author    = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  date      = {2019},
  title     = {Panoptic feature pyramid networks},
  pages     = {6399--6408},
  url       = {http://openaccess.thecvf.com/content_CVPR_2019/html/Kirillov_Panoptic_Feature_Pyramid_Networks_CVPR_2019_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):http\://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Feature_Pyramid_Networks_CVPR_2019_paper.pdf:application/pdf},
}

@InProceedings{kirillov2019PQ,
  author    = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll{\'a}r, Piotr},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  date      = {2019},
  title     = {Panoptic segmentation},
  pages     = {9404--9413},
  url       = {http://openaccess.thecvf.com/content_CVPR_2019/html/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf:application/pdf},
  year      = {2019},
}

@Article{Yosinski2014,
  author       = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  date         = {2014},
  journaltitle = {Advances in neural information processing systems},
  title        = {How transferable are features in deep neural networks?},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf},
  urldate      = {2025-08-05},
  volume       = {27},
  file         = {Available Version (via Google Scholar):https\://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf:application/pdf},
}

@InProceedings{Tan2021,
  author     = {Tan, Mingxing and Le, Quoc},
  booktitle  = {International conference on machine learning},
  date       = {2021},
  title      = {Efficientnetv2: {Smaller} models and faster training},
  pages      = {10096--10106},
  publisher  = {PMLR},
  url        = {http://proceedings.mlr.press/v139/tan21a.html},
  urldate    = {2025-08-05},
  file       = {Available Version (via Google Scholar):http\://proceedings.mlr.press/v139/tan21a/tan21a.pdf:application/pdf},
  shorttitle = {Efficientnetv2},
}

@Article{Minaee2021,
  author       = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
  date         = {2021},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  title        = {Image segmentation using deep learning: {A} survey},
  number       = {7},
  pages        = {3523--3542},
  url          = {https://ieeexplore.ieee.org/abstract/document/9356353/},
  urldate      = {2025-08-05},
  volume       = {44},
  file         = {Available Version (via Google Scholar):https\://ieeexplore.ieee.org/iel7/34/4359286/09356353.pdf:application/pdf},
  publisher    = {IEEE},
  shorttitle   = {Image segmentation using deep learning},
}

@InProceedings{Long2015,
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2015},
  title     = {Fully convolutional networks for semantic segmentation},
  pages     = {3431--3440},
  url       = {http://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf:application/pdf},
}

@InCollection{Ronneberger2015,
  author     = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date       = {2015},
  title      = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
  doi        = {10.1007/978-3-319-24574-4_28},
  editor     = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  isbn       = {9783319245737 9783319245744},
  language   = {en},
  location   = {Cham},
  pages      = {234--241},
  publisher  = {Springer International Publishing},
  url        = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
  urldate    = {2025-08-05},
  volume     = {9351},
  file       = {Available Version (via Google Scholar):https\://arxiv.org/pdf/1505.04597:application/pdf},
  shorttitle = {U-{Net}},
}

@InProceedings{Arnab2017,
  author    = {Arnab, Anurag and Torr, Philip HS},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2017},
  title     = {Pixelwise instance segmentation with a dynamically instantiated network},
  pages     = {441--450},
  url       = {http://openaccess.thecvf.com/content_cvpr_2017/html/Arnab_Pixelwise_Instance_Segmentation_CVPR_2017_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):http\://openaccess.thecvf.com/content_cvpr_2017/papers/Arnab_Pixelwise_Instance_Segmentation_CVPR_2017_paper.pdf:application/pdf},
}

@InProceedings{Chen2018,
  author     = {Chen, Liang-Chieh and Hermans, Alexander and Papandreou, George and Schroff, Florian and Wang, Peng and Adam, Hartwig},
  booktitle  = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date       = {2018},
  title      = {Masklab: {Instance} segmentation by refining object detection with semantic and direction features},
  pages      = {4013--4022},
  url        = {http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_MaskLab_Instance_Segmentation_CVPR_2018_paper.html},
  urldate    = {2025-08-05},
  file       = {Available Version (via Google Scholar):http\://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_MaskLab_Instance_Segmentation_CVPR_2018_paper.pdf:application/pdf},
  shorttitle = {Masklab},
}

@InProceedings{Winn2006,
  author    = {Winn, John and Shotton, Jamie},
  booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
  date      = {2006},
  title     = {The layout consistent random field for recognizing and segmenting partially occluded objects},
  pages     = {37--44},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/abstract/document/1640739/},
  urldate   = {2025-08-05},
  volume    = {1},
  file      = {Available Version (via Google Scholar):https\://ieeexplore.ieee.org/iel5/10924/34373/01640739.pdf:application/pdf},
}

@InCollection{Hariharan2014,
  author    = {Hariharan, Bharath and Arbeláez, Pablo and Girshick, Ross and Malik, Jitendra},
  date      = {2014},
  title     = {Simultaneous {Detection} and {Segmentation}},
  doi       = {10.1007/978-3-319-10584-0_20},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  isbn      = {9783319105833 9783319105840},
  language  = {en},
  location  = {Cham},
  pages     = {297--312},
  publisher = {Springer International Publishing},
  url       = {http://link.springer.com/10.1007/978-3-319-10584-0_20},
  urldate   = {2025-08-05},
  volume    = {8695},
  copyright = {http://www.springer.com/tdm},
  file      = {Available Version (via Google Scholar):https\://arxiv.org/pdf/1407.1808:application/pdf},
}

@InProceedings{Girshick2014,
  author    = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2014},
  title     = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  pages     = {580--587},
  url       = {http://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf:application/pdf},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2016},
  title     = {Deep residual learning for image recognition},
  pages     = {770--778},
  url       = {http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate   = {2025-08-11},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf:application/pdf},
}

@InProceedings{Liu2022,
  author    = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  date      = {2022},
  title     = {A convnet for the 2020s},
  pages     = {11976--11986},
  url       = {http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html},
  urldate   = {2025-08-11},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf:application/pdf},
}

@InProceedings{Liu2022a,
  author     = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li},
  booktitle  = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  date       = {2022},
  title      = {Swin transformer v2: {Scaling} up capacity and resolution},
  pages      = {12009--12019},
  url        = {http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html},
  urldate    = {2025-08-11},
  file       = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.pdf:application/pdf},
  shorttitle = {Swin transformer v2},
}

@InProceedings{Tan2019,
  author     = {Tan, Mingxing and Le, Quoc},
  booktitle  = {International conference on machine learning},
  date       = {2019},
  title      = {Efficientnet: {Rethinking} model scaling for convolutional neural networks},
  pages      = {6105--6114},
  publisher  = {PMLR},
  url        = {https://proceedings.mlr.press/v97/tan19a.html?ref=ji},
  urldate    = {2025-08-12},
  file       = {Available Version (via Google Scholar):http\://proceedings.mlr.press/v97/tan19a/tan19a.pdf:application/pdf},
  shorttitle = {Efficientnet},
}

@Comment{jabref-meta: databaseType:biblatex;}
