@article{dosovitskiy2020ViT,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{he2022mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@misc{jrfarm_katze,
  author       = {{JR Farm}},
  title        = {Katzenratgeber – Alles Wissenswerte über Katzen},
  year         = 2025,
  howpublished = {\url{https://www.jr-farm.de/ratgeber/tiere/katze}},
  note         = {Accessed: 2025-06-26}
}

@misc{Egan_Katzen,
author = {Egan, Ben and Redden, Alex and {XWAVE} and {SilentAntagonist}},
month = may,
title = {{Dalle3 1 Million+ High Quality Captions}},
url = {https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions},
year = {2024}
}

%nuclei datensatz papers
@article{edlund2021livecell,
  title={LIVECell—A large-scale dataset for label-free live cell segmentation},
  author={Edlund, Christoffer and Jackson, Timothy R and Khalid, Nabeel and Bevan, Nicola and Dale, Timothy and Dengel, Andreas and Ahmed, Sheraz and Trygg, Johan and Sj{\"o}gren, Rickard},
  journal={Nature methods},
  volume={18},
  number={9},
  pages={1038--1045},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{dietler2020YeaZ,
  title={A convolutional neural network segments yeast microscopy images with high accuracy},
  author={Dietler, Nicola and Minder, Matthias and Gligorovski, Vojislav and Economou, Augoustina Maria and Joly, Denis Alain Henri Lucien and Sadeghi, Ahmad and Chan, Chun Hei Michael and Kozi{\'n}ski, Mateusz and Weigert, Martin and Bitbol, Anne-Florence and others},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={5723},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{cutler2022omnipose,
  title={Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation},
  author={Cutler, Kevin J and Stringer, Carsen and Lo, Teresa W and Rappez, Luca and Stroustrup, Nicholas and Brook Peterson, S and Wiggins, Paul A and Mougous, Joseph D},
  journal={Nature methods},
  volume={19},
  number={11},
  pages={1438--1448},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@misc{holden2021deepbacs,
  author       = {Holden, S. and Conduit, M.},
  title        = {{DeepBacs – Bacillus subtilis fluorescence segmentation dataset}},
  year         = {2021},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.5550968},
  url          = {https://doi.org/10.5281/zenodo.5550968},
  note         = {Data set}
}

@article{cspahn2021deepbacs,
  title={DeepBacs: Bacterial image analysis using open-source deep learning approaches},
  author={Spahn, Christoph and Laine, Romain F. and Matos Pereira, Pedro and von Chamier, Lucas and Conduit, Mia and G{\'o}mez-de-Mariscal, Estibaliz and Gomes de Pinho, Mariana and Jacquemet, Guillaume and Holden, S{\'{e}}amus and Heilemann, Mike and Henriques, Ricardo},
  journal={bioRxiv},
  year={2021},
  doi = {10.1101/2021.11.03.467152},
  publisher = {Cold Spring Harbor Laboratory},
  URL = {https://www.biorxiv.org/content/early/2021/11/03/2021.11.03.467152}
}

@article{kumar2017MoNuSeg,
  title={A dataset and a technique for generalized nuclear segmentation for computational pathology},
  author={Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
  journal={IEEE transactions on medical imaging},
  volume={36},
  number={7},
  pages={1550--1560},
  year={2017},
  publisher={IEEE}
}

@article{greenwald2022Tissuenet,
  title={Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning},
  author={Greenwald, Noah F and Miller, Geneva and Moen, Erick and Kong, Alex and Kagel, Adam and Dougherty, Thomas and Fullaway, Christine Camacho and McIntosh, Brianna J and Leow, Ke Xuan and Schwartz, Morgan Sarah and others},
  journal={Nature biotechnology},
  volume={40},
  number={4},
  pages={555--565},
  year={2022},
  publisher={Nature Publishing Group US New York}
}


%Domain adaption und similarity
@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and March, Mario and Lempitsky, Victor},
  journal={Journal of machine learning research},
  volume={17},
  number={59},
  pages={1--35},
  year={2016}
}

@inproceedings{yuan2005domainsimilarity,
  title={An empirical study on language model adaptation using a metric of domain similarity},
  author={Yuan, Wei and Gao, Jianfeng and Suzuki, Hisami},
  booktitle={International Conference on Natural Language Processing},
  pages={957--968},
  year={2005},
  organization={Springer}
}

@inproceedings{ganin2015domain_big,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={International conference on machine learning},
  pages={1180--1189},
  year={2015},
  organization={PMLR}
}

@inproceedings{pinheiro2018domain,
  title={Unsupervised domain adaptation with similarity learning},
  author={Pinheiro, Pedro O},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8004--8013},
  year={2018}
}

@article{peng2017domain,
  title={Visda: The visual domain adaptation challenge},
  author={Peng, Xingchao and Usman, Ben and Kaushik, Neela and Hoffman, Judy and Wang, Dequan and Saenko, Kate},
  journal={arXiv preprint arXiv:1710.06924},
  year={2017}
}

@article{wang2018domain,
  title={Deep visual domain adaptation: A survey},
  author={Wang, Mei and Deng, Weihong},
  journal={Neurocomputing},
  volume={312},
  pages={135--153},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{cai2009similarity,
  title={Efficient algorithm for computing link-based similarity in real world networks},
  author={Cai, Yuanzhe and Cong, Gao and Jia, Xu and Liu, Hongyan and He, Jun and Lu, Jiaheng and Du, Xiaoyong},
  booktitle={2009 Ninth IEEE International Conference on Data Mining},
  pages={734--739},
  year={2009},
  organization={IEEE}
}

@article{koohi2018similarity,
  title={Cross-domain graph based similarity measurement of workflows},
  author={Koohi-Var, Tahereh and Zahedi, Morteza},
  journal={Journal of Big Data},
  volume={5},
  pages={1--16},
  year={2018},
  publisher={Springer}
}

@article{han2022binsimilarity_domain,
  title={Bin similarity-based domain adaptation for fine-grained image classification},
  author={Han, Tianyu and Zhang, Lifeng and Jia, Shixiang},
  journal={International Journal of Intelligent Systems},
  volume={37},
  number={3},
  pages={2319--2334},
  year={2022},
  publisher={Wiley Online Library}
}

@article{zhu2020domain_similarity,
  title={Domain adaptation using class similarity for robust speech recognition},
  author={Zhu, Han and Zhao, Jiangjiang and Ren, Yuling and Wang, Li and Zhang, Pengyuan},
  journal={arXiv preprint arXiv:2011.02782},
  year={2020}
}

@article{bruch2025,
  title={Improving 3D deep learning segmentation with biophysically motivated cell synthesis},
  author={Bruch, Roman and Vitacolonna, Mario and N{\"u}rnberg, Elina and Sauer, Simeon and Rudolf, R{\"u}diger and Reischl, Markus},
  journal={Communications Biology},
  volume={8},
  number={1},
  pages={43},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

%Segmentierung foundation model
@inproceedings{li2023mask,
  title={Mask dino: Towards a unified transformer-based framework for object detection and segmentation},
  author={Li, Feng and Zhang, Hao and Xu, Huaizhe and Liu, Shilong and Zhang, Lei and Ni, Lionel M and Shum, Heung-Yeung},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3041--3050},
  year={2023}
}

@article{isensee2021nnu,
  title={nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},
  author={Isensee, Fabian and Jaeger, Paul F and Kohl, Simon AA and Petersen, Jens and Maier-Hein, Klaus H},
  journal={Nature methods},
  volume={18},
  number={2},
  pages={203--211},
  year={2021},
  publisher={Nature Publishing Group}
}

%Segmentierung model
@article{moen2019DeepcellCaliban,
  title={Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning},
  author={Moen, Erick and Borba, Enrico and Miller, Geneva and Schwartz, Morgan and Bannon, Dylan and Koe, Nora and Camplisson, Isabella and Kyme, Daniel and Pavelchek, Cole and Price, Tyler and others},
  journal={Biorxiv},
  pages={803205},
  year={2019},
  publisher={Cold Spring Harbor Laboratory}
}

@article{bannon2021deepcell,
  title={DeepCell Kiosk: scaling deep learning--enabled cellular image analysis with Kubernetes},
  author={Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba, Enrico and Kudo, Takamasa and Greenwald, Noah and Vijayakumar, Vibha and Chang, Brian and Pao, Edward and Osterman, Erik and others},
  journal={Nature methods},
  volume={18},
  number={1},
  pages={43--45},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{greenwald2022deepcell,
  title={Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning},
  author={Greenwald, Noah F and Miller, Geneva and Moen, Erick and Kong, Alex and Kagel, Adam and Dougherty, Thomas and Fullaway, Christine Camacho and McIntosh, Brianna J and Leow, Ke Xuan and Schwartz, Morgan Sarah and others},
  journal={Nature biotechnology},
  volume={40},
  number={4},
  pages={555--565},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@article{van2016deepcell,
  title={Deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments},
  author={Van Valen, David A and Kudo, Takamasa and Lane, Keara M and Macklin, Derek N and Quach, Nicolas T and DeFelice, Mialy M and Maayan, Inbal and Tanouchi, Yu and Ashley, Euan A and Covert, Markus W},
  journal={PLoS computational biology},
  volume={12},
  number={11},
  pages={e1005177},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}


%Segmentierung foundation model
@inproceedings{jain2023oneformer,
  title={Oneformer: One transformer to rule universal image segmentation},
  author={Jain, Jitesh and Li, Jiachen and Chiu, Mang Tik and Hassani, Ali and Orlov, Nikita and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2989--2998},
  year={2023}
}

%Segmentierung foundation model
@article{zou2023segment,
  title={Segment everything everywhere all at once},
  author={Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={19769--19782},
  year={2023}
}

%Segmentierung foundation model
@inproceedings{wang2021max,
  title={Max-deeplab: End-to-end panoptic segmentation with mask transformers},
  author={Wang, Huiyu and Zhu, Yukun and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5463--5474},
  year={2021}
}


%Image foundation model, nicht ganz für Segmentierung gedacht
@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International conference on machine learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}

@article{stringer2021cellpose,
  title={Cellpose: a generalist algorithm for cellular segmentation},
  author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
  journal={Nature methods},
  volume={18},
  number={1},
  pages={100--106},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{pachitariu2025samcellpose,
  title={Cellpose-SAM: superhuman generalization for cellular segmentation},
  author={Pachitariu, Marius and Rariden, Michael and Stringer, Carsen},
  journal={bioRxiv},
  pages={2025--04},
  year={2025},
  publisher={Cold Spring Harbor Laboratory}
}

@article{vandeloo2025samfine,
  title={SAMCell: Generalized Label-Free Biological Cell Segmentation with Segment Anything},
  author={VandeLoo, Alexandra D and Malta, Nathan J and Aponte, Emilio and van Zyl, Caitlin and Xu, Danfei and Forest, Craig R},
  journal={bioRxiv},
  year={2025}
}

@article{archit2025samfine,
  title={Segment anything for microscopy},
  author={Archit, Anwai and Freckmann, Luca and Nair, Sushmita and Khalid, Nabeel and Hilt, Paul and Rajashekar, Vikas and Freitag, Marei and Teuber, Carolin and Buckley, Genevieve and von Haaren, Sebastian and others},
  journal={Nature Methods},
  pages={1--13},
  year={2025},
  publisher={Nature Publishing Group US New York}
}

@article{israel2023samfine,
  title={A foundation model for cell segmentation},
  author={Israel, Uriah and Marks, Markus and Dilip, Rohit and Li, Qilin and Schwartz, Morgan and Pradhan, Elora and Pao, Edward and Li, Shenyi and Pearson-Goulart, Alexander and Perona, Pietro and others},
  journal={arXiv preprint arXiv:2311.11004},
  year={2023}
}

@inproceedings{kirillov2023sam,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4015--4026},
  year={2023}
}

@article{dippel2022segmentation,
  title={Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder},
  author={Dippel, Jonas and Lenga, Matthias and Goerttler, Thomas and Obermayer, Klaus and H{\"o}hne, Johannes},
  journal={arXiv preprint arXiv:2207.14508},
  year={2022}
}

@article{bommasani2021foundationmodels,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{li2024PartPQ,
  title={Panoptic-PartFormer++: A unified and decoupled view for panoptic part segmentation},
  author={Li, Xiangtai and Xu, Shilin and Yang, Yibo and Yuan, Haobo and Cheng, Guangliang and Tong, Yunhai and Lin, Zhouchen and Yang, Ming-Hsuan and Tao, Dacheng},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2024},
  publisher={IEEE}
}

@inproceedings{fu2018cycleGAN,
  title={Three dimensional fluorescence microscopy image synthesis and segmentation},
  author={Fu, Chichen and Lee, Soonam and Joon Ho, David and Han, Shuo and Salama, Paul and Dunn, Kenneth W and Delp, Edward J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={2221--2229},
  year={2018}
}

@article{Kromp2020_Dataset,
  author = {Kromp, Florian and Bozsaky, Eva and Rifatbegovic, Fikret and Fischer, Lukas and Ambros, Magdalena and Berneder, Maria and Weiss, Tamara and Lazic, Daria and Doerr, Wolfgang and Hanbury, Allan and Beiske, Klaus and Ambros, Peter F. and Ambros, Inge M. and Taschner-Mandl, Sabine},
  journal = {Nature Scientific Data},
  title = {An annotated fluorescence image dataset for training nuclear segmentation methods},
  year = {2020},
  volume = {7},
  number = {262},
  pages = {1--8},
  doi = {10.1038/s41597-020-00608-w}
}

@article{chen20223_Dataset,
  title={3d ground truth annotations of nuclei in 3d microscopy volumes},
  author={Chen, Alain and Wu, Liming and Winfree, Seth and Dunn, Kenneth W and Salama, Paul and Delp, Edward J},
  journal={bioRxiv},
  pages={2022--09},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{ulman2017cellTrackingChallenge,
  title={An objective comparison of cell-tracking algorithms},
  author={Ulman, Vladim{\'\i}r and Ma{\v{s}}ka, Martin and Magnusson, Klas EG and Ronneberger, Olaf and Haubold, Carsten and Harder, Nathalie and Matula, Pavel and Matula, Petr and Svoboda, David and Radojevic, Miroslav and others},
  journal={Nature methods},
  volume={14},
  number={12},
  pages={1141--1152},
  year={2017},
  publisher={Nature Publishing Group UK London}
}











@book{Boyd2009,
	author		= "Stephen Boyd and Lieven Vandenberghe",
	title		= "Convex {O}ptimization",
	edition		= "7th ed.",
	year		= "2009",
	publisher	= "Cambridge University Press",
	hyphenation     = "english",
}  

@online{EMVA1288,
	author		= "EMVA",
	title		= "{EMVA} {S}tandard 1288",
	subtitle	= "Standard for Characerization of Image Sensors and Cameras. Release 3.1.",
	url		= "http://www.emva.org/standards-technology/emva-1288/",
	version		= "3.1",
	date		= "2016",
	month		= "12",
	year		= "2016",
	hyphenation	= "ngerman",
} 

@online{Hashemi2018,
	title		= {Asymmetric Similarity Loss Function to Balance Precision and Recall in Highly Unbalanced Deep Medical Image Segmentation},
	author		= {Hashemi, S. R. and Salehi, S. S. M. and Erdogmus, D. and Prabhu, S. P. and Warfield S. K. and Gholipour, A.},
	year		= {2018},
	archivePrefix	= "arXiv",
	eprint		= {1803.11078v3},
	hyphenation     = "ngerman",
}

@book{Jaehne2012,
	author		= "Bernd Jähne",
	title		= "Digitale Bildverarbeitung und Bildgewinnung",
	edition		= "7. Auflage",
	year		= "2012",
	publisher	= "Springer",
	doi		= "10.1007/978-3-642-04952-1",
	hyphenation	= "ngerman",
}

@thesis{Scherr2017,
	author		= "Tim Scherr",
	title		= "Gradient-Based Surface Reconstruction and the Application to Wind Waves",
	type		= "Master's Thesis",
	institution	= "Ruprecht-Karls University Heidelberg",
	date		= "2017",
	doi		= "10.11588/heidok.00023653",
	hyphenation	= "english",
}

@thesis{Scherr2017-2,
	author		= "Tim Scherr",
	title		= "Gradient-Based Surface Reconstruction and the Application to Wind Waves",
	type		= "Master's Thesis",
	institution	= "Ruprecht-Karls University Heidelberg",
	date		= "2017",
	doi		= "10.11588/heidok.00023653",
	hyphenation	= "ngerman",
}

@article{Schott2018,
	author 		= "Schott, B. AND Traub, M. AND Schlagenhauf, C. AND Takamiya, M. AND Antritter, T. AND Bartschat, A. AND Löffler, K. AND Blessing, D. AND Otte, J. C. AND Kobitski, A. Y. AND Nienhaus, G. U. AND Strähle, U. AND Mikut, R. AND Stegmaier, J.",
	journal		= "PLOS Computational Biology",
	publisher	= "Public Library of Science",
	title		= "Embryo{M}iner: {A} {N}ew {F}ramework for {I}nteractive {K}nowledge {D}iscovery in {L}arge-{S}cale {C}ell {T}racking {D}ata of {D}eveloping {E}mbryos",
	year		= "2018",
	volume		= "14",
	pages		= "1-18",
	doi		= "10.1371/journal.pcbi.1006128",
	hyphenation	= "english",
}

@InProceedings{Szegedy2015,
	author		= {Szegedy, C. and Liu, W. and Jia, Y. and Sermanet, P. and Reed, S. and Anguelov, D. and Erhan, D. and Vanhoucke, V. and Rabinovich, A.},
	title		= {Going Deeper With Convolutions},
	booktitle	= {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages		= {1-9}, 
	year		= {2015},
	doi		= {10.1109/CVPR.2015.7298594}, 
	hyphenation	= "ngerman",
}  

@InProceedings{Radford2021,
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date      = {2021-07},
  title     = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
  language  = {en},
  pages     = {8748--8763},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v139/radford21a.html},
  urldate   = {2025-07-10},
  abstract  = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  file      = {Full Text PDF:http\://proceedings.mlr.press/v139/radford21a/radford21a.pdf:application/pdf;Supplementary PDF:http\://proceedings.mlr.press/v139/radford21a/radford21a-supp.pdf:application/pdf},
  issn      = {2640-3498},
}

@Article{Weisrock2024,
  author       = {Weisrock, Antoine and Wüst, Rebecca and Olenic, Maria and Lecomte-Grosbras, Pauline and Thorrez, Lieven},
  date         = {2024-10},
  journaltitle = {Tissue Engineering Part A},
  title        = {{MyoFInDer}: {An} {AI}-{Based} {Tool} for {Myotube} {Fusion} {Index} {Determination}},
  doi          = {10.1089/ten.tea.2024.0049},
  issn         = {1937-3341},
  number       = {19-20},
  pages        = {652--661},
  url          = {https://www.liebertpub.com/doi/10.1089/ten.tea.2024.0049},
  urldate      = {2025-07-16},
  volume       = {30},
  file         = {Full Text PDF:https\://www.liebertpub.com/doi/pdf/10.1089/ten.tea.2024.0049:application/pdf},
  publisher    = {Mary Ann Liebert, Inc., publishers},
  shorttitle   = {{MyoFInDer}},
}

@TechReport{Lair2025,
  author      = {Lair, Benjamin and Cazorla, Clément and Lobeto, Alicia and Labour, Axel and Laurens, Claire and Rustan, Arild C. and Weiss, Pierre and Flores-Flores, Remy and Moro, Cedric},
  date        = {2025-02},
  institution = {bioRxiv},
  title       = {{MyoFuse}: {A} fully {AI}-based workflow for automated quantification of skeletal muscle cell fusion in vitro},
  chapter     = {New Results},
  doi         = {10.1101/2025.02.17.638596},
  language    = {en},
  note        = {Type: article},
  pages       = {2025.02.17.638596},
  url         = {https://www.biorxiv.org/content/10.1101/2025.02.17.638596v1},
  urldate     = {2025-07-16},
  abstract    = {Background The myogenic fusion index (FI) is commonly used in skeletal muscle cell culture to assess the ability of myoblasts to form myotubes, as the ratio of myoblast nuclei fused with myotubes over the total number of myoblasts. The manual quantification of the FI from 2D microscopy images is tedious and biased, thus several automated methods have been developed. However, they still face challenges such as efficient nucleus segmentation and classification of fused and isolated myoblast nuclei. Here, we developed a novel workflow entirely based on AI for fully automated and unbiased quantification of the FI.
Results Using current methods, we show that myoblast nuclei located above or below myotubes can significantly corrupt accurate FI computation. To circumvent this issue, we developed MyoFuse which enables an accurate and high-throughput segmentation and classification of myonuclei. It comprises a nuclei segmentation step using Cellpose, followed by a classification network trained with Svetlana. MyoFuse demonstrated strong accuracy when tested against manual annotation in mouse C2C12 and human primary myotubes. The trained classifier is able to differentiate myotube nuclei from myoblast nuclei based on myotube cytoplasm staining only. Experimental comparisons also highlighted that the previously developed methods lead to a significant overestimation of the FI.
Conclusion In summary, we underscore the lack of accuracy of traditional methods for automated FI quantification. MyoFuse enables a direct and accurate segmentation of nuclei even in nuclei clusters frequently observed in myotubes. This workflow thus offers a new and more reliable method to evaluate the FI. It also limits the selection bias by processing large images.},
  copyright   = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file        = {Full Text PDF:https\://www.biorxiv.org/content/biorxiv/early/2025/02/17/2025.02.17.638596.full.pdf:application/pdf},
  shorttitle  = {{MyoFuse}},
}

@Article{Jeong2025,
  author       = {Jeong, Kyungchang and Park, Sanghun and Jo, Gyuchan and Seo, Hanbit and Choi, Nayoung and Jang, Soyoung and Park, Gyutae and Seo, Young-Duk and Brad Kim, Yuan H. and Jeong, Ji-Hoon and Hyun, Sang-Hwan and Choi, Jungseok and Lee, Euijong},
  date         = {2025-03},
  journaltitle = {Computers in Biology and Medicine},
  title        = {{SEPO}-{FI}: {Deep}-learning based software to calculate fusion index of muscle cells},
  doi          = {10.1016/j.compbiomed.2025.109706},
  issn         = {0010-4825},
  pages        = {109706},
  url          = {https://www.sciencedirect.com/science/article/pii/S0010482525000563},
  urldate      = {2025-07-16},
  volume       = {186},
  abstract     = {The fusion index is a critical metric for quantitatively assessing the transformation of in vitro muscle cells into myotubes in the biological and medical fields. Traditional methods for calculating this index manually involve the labor-intensive counting of numerous muscle cell nuclei in images, which necessitates determining whether each nucleus is located inside or outside the myotubes, leading to significant inter-observer variation. To address these challenges, this study proposes a three-stage process that integrates the strengths of pattern recognition and deep-learning to automatically calculate the fusion index. The experimental results demonstrate that the proposed process achieves significantly higher performance in cell nuclei detection and classification, with an F1-score of 0.953, whereas traditional object detection methods achieve less than 0.5. In addition, the fusion index obtained using the proposed method is closely aligned with the human-assessed values, showing minimal discrepancy and strong agreement with human evaluations. This process is incorporated into the development of “SEPO-FI” as public software, automating cell detection and classification to enable effective fusion index calculation and broaden access to this methodology within the scientific community.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0010482525000563/pdfft?download=true:application/pdf},
  keywords     = {Fusion index calculation, Computer vision, Pattern recognition, Deep-learning, Cell detection and classification, Convolutional neural network},
  shorttitle   = {{SEPO}-{FI}},
}

@article{lewis1917behavior,
  title={Behavior of cross striated muscle in tissue cultures},
  author={Lewis, Warren H and Lewis, Margaret R},
  journal={American Journal of Anatomy},
  volume={22},
  number={2},
  pages={169--194},
  year={1917},
  publisher={Wiley Subscription Services, Inc., A Wiley Company Hoboken}
}

@Article{Pogogeff1946,
  author       = {Pogogeff, Irene A. and Murray, Margaret R.},
  date         = {1946},
  journaltitle = {The Anatomical Record},
  title        = {Form and behavior of adult mammalian skeletal muscle in vitro},
  doi          = {10.1002/ar.1090950308},
  issn         = {1097-0185},
  language     = {en},
  number       = {3},
  pages        = {321--335},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ar.1090950308},
  urldate      = {2025-07-16},
  volume       = {95},
  copyright    = {Copyright © 1946 Wiley-Liss, Inc.},
  file         = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ar.1090950308:application/pdf},
}

@Article{Enwere2014,
  author       = {Enwere, Emeka and LaCasse, Eric and Adam, Nadine and Korneluk, Robert},
  date         = {2014-02},
  journaltitle = {Frontiers in Immunology},
  title        = {Role of the {TWEAK}-{Fn14}-{cIAP1}-{NF}-κ{B} {Signaling} {Axis} in the {Regulation} of {Myogenesis} and {Muscle} {Homeostasis}},
  doi          = {10.3389/fimmu.2014.00034},
  pages        = {34},
  volume       = {5},
  abstract     = {Mammalian skeletal muscle maintains a robust regenerative capacity throughout life, largely due to the presence of a stem cell population known as “satellite cells” in the muscle milieu. In normal conditions, these cells remain quiescent; they are activated upon injury to become myoblasts, which proliferate extensively and eventually differentiate and fuse to form new multinucleated muscle fibers. Recent findings have identified some of the factors, including the cytokine TNFα-like weak inducer of apoptosis (TWEAK), which govern these cells’ decisions to proliferate, differentiate, or fuse. In this review, we will address the functions of TWEAK, its receptor Fn14, and the associated signal transduction molecule, the cellular inhibitor of apoptosis 1 (cIAP1), in the regulation of myogenesis. TWEAK signaling can activate the canonical NF-κB signaling pathway, which promotes myoblast proliferation and inhibits myogenesis. In addition, TWEAK activates the non-canonical NF-κB pathway, which, in contrast, promotes myogenesis by increasing myoblast fusion. Both pathways are regulated by cIAP1, which is an essential component of downstream signaling mediated by TWEAK and similar cytokines. This review will focus on the seemingly contradictory roles played by TWEAK during muscle regeneration, by highlighting the interplay between the two NF-κB pathways under physiological and pathological conditions. We will also discuss how myogenesis is negatively affected by chronic conditions, which affect homeostasis of the skeletal muscle environment.},
  file         = {Full Text PDF:https\://www.researchgate.net/journal/Frontiers-in-Immunology-1664-3224/publication/260254398_Role_of_the_TWEAK-Fn14-cIAP1-NF-kB_Signaling_Axis_in_the_Regulation_of_Myogenesis_and_Muscle_Homeostasis/links/66544fd022a7f16b4f4ed833/Role-of-the-TWEAK-Fn14-cIAP1-NF-kB-Signaling-Axis-in-the-Regulation-of-Myogenesis-and-Muscle-Homeostasis.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/260254398_Role_of_the_TWEAK-Fn14-cIAP1-NF-kB_Signaling_Axis_in_the_Regulation_of_Myogenesis_and_Muscle_Homeostasis:},
}

@Article{Scharner2011,
  author       = {Scharner, Juergen and Zammit, Peter S},
  date         = {2011-08},
  journaltitle = {Skeletal Muscle},
  title        = {The muscle satellite cell at 50: the formative years},
  doi          = {10.1186/2044-5040-1-28},
  issn         = {2044-5040},
  pages        = {28},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3177780/},
  urldate      = {2025-07-16},
  volume       = {1},
  abstract     = {In February 1961, Alexander Mauro described a cell 'wedged' between the plasma membrane of the muscle fibre and the surrounding basement membrane. He postulated that it could be a dormant myoblast, poised to repair muscle when needed. In the same month, Bernard Katz also reported a cell in a similar location on muscle spindles, suggesting that it was associated with development and growth of intrafusal muscle fibres. Both Mauro and Katz used the term 'satellite cell' in relation to their discoveries. Today, the muscle satellite cell is widely accepted as the resident stem cell of skeletal muscle, supplying myoblasts for growth, homeostasis and repair., Since 2011 marks both the 50th anniversary of the discovery of the satellite cell, and the launch of Skeletal Muscle, it seems an opportune moment to summarise the seminal events in the history of research into muscle regeneration. We start with the 19th-century pioneers who showed that muscle had a regenerative capacity, through to the descriptions from the mid-20th century of the underlying cellular mechanisms. The journey of the satellite cell from electron microscope curio, to its gradual acceptance as a bona fide myoblast precursor, is then charted: work that provided the foundations for our understanding of the role of the satellite cell. Finally, the rapid progress in the age of molecular biology is briefly discussed, and some ongoing debates on satellite cell function highlighted.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC3177780/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC3177780/pdf/2044-5040-1-28.pdf:application/pdf},
  pmcid        = {PMC3177780},
  pmid         = {21849021},
  shorttitle   = {The muscle satellite cell at 50},
}

@Book{Gilbert2014,
  author    = {Gilbert, Scott F},
  date      = {2014},
  title     = {Developmental biology},
  edition   = {10},
  language  = {english},
  location  = {Sunderland, Massachusetts},
  publisher = {Sinauer Associates},
}

@Article{Brunetti2021,
  author       = {Brunetti, Jessica and Koenig, Stéphane and Monnier, Arthur and Frieden, Maud},
  date         = {2021-05},
  journaltitle = {Skeletal Muscle},
  title        = {Nanopattern surface improves cultured human myotube maturation},
  doi          = {10.1186/s13395-021-00268-3},
  issn         = {2044-5040},
  number       = {1},
  pages        = {12},
  url          = {https://doi.org/10.1186/s13395-021-00268-3},
  urldate      = {2025-07-16},
  volume       = {11},
  abstract     = {In vitro maturation of human primary myoblasts using 2D culture remains a challenging process and leads to immature fibers with poor internal organization and function. This would however represent a valuable system to study muscle physiology or pathophysiology from patient myoblasts, at a single-cell level.},
  file         = {Full Text PDF:https\://skeletalmusclejournal.biomedcentral.com/counter/pdf/10.1186/s13395-021-00268-3:application/pdf},
  keywords     = {Human primary myoblasts, Cell alignment, Myotube maturation, Acetylcholine receptor clusters, Ca2+ signals},
}

@Article{NogalesGadea2010,
  author       = {Nogales-Gadea, Gisela and Mormeneo, Emma and García-Consuegra, Inés and Rubio, Juan C. and Orozco, Anna and Arenas, Joaquin and Martín, Miguel A. and Lucia, Alejandro and Gómez-Foix, Anna M. and Martí, Ramon and Andreu, Antoni L.},
  date         = {2010-10},
  journaltitle = {PLOS ONE},
  title        = {Expression of {Glycogen} {Phosphorylase} {Isoforms} in {Cultured} {Muscle} from {Patients} with {McArdle}'s {Disease} {Carrying} the p.{R771PfsX33} {PYGM} {Mutation}},
  doi          = {10.1371/journal.pone.0013164},
  issn         = {1932-6203},
  language     = {en},
  number       = {10},
  pages        = {e13164},
  url          = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0013164},
  urldate      = {2025-07-16},
  volume       = {5},
  abstract     = {Background Mutations in the PYGM gene encoding skeletal muscle glycogen phosphorylase (GP) cause a metabolic disorder known as McArdle's disease. Previous studies in muscle biopsies and cultured muscle cells from McArdle patients have shown that PYGM mutations abolish GP activity in skeletal muscle, but that the enzyme activity reappears when muscle cells are in culture. The identification of the GP isoenzyme that accounts for this activity remains controversial. Methodology/Principal Findings In this study we present two related patients harbouring a novel PYGM mutation, p.R771PfsX33. In the patients' skeletal muscle biopsies, PYGM mRNA levels were ∼60\% lower than those observed in two matched healthy controls; biochemical analysis of a patient muscle biopsy resulted in undetectable GP protein and GP activity. A strong reduction of the PYGM mRNA was observed in cultured muscle cells from patients and controls, as compared to the levels observed in muscle tissue. In cultured cells, PYGM mRNA levels were negligible regardless of the differentiation stage. After a 12 day period of differentiation similar expression of the brain and liver isoforms were observed at the mRNA level in cells from patients and controls. Total GP activity (measured with AMP) was not different either; however, the active GP activity and immunoreactive GP protein levels were lower in patients' cell cultures. GP immunoreactivity was mainly due to brain and liver GP but muscle GP seemed to be responsible for the differences. Conclusions/Significance These results indicate that in both patients' and controls' cell cultures, unlike in skeletal muscle tissue, most of the protein and GP activities result from the expression of brain GP and liver GP genes, although there is still some activity resulting from the expression of the muscle GP gene. More research is necessary to clarify the differential mechanisms of metabolic adaptations that McArdle cultures undergo in vitro.},
  file         = {Full Text PDF:https\://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0013164&type=printable:application/pdf},
  keywords     = {Skeletal muscles, Glycogens, Cell cultures, Muscle proteins, Muscle differentiation, Muscle cells, Biopsy, Frameshift mutation},
  publisher    = {Public Library of Science},
}

@Article{Chua2019,
  author       = {Chua, Min‐Wen Jason and Yildirim, Ege Deniz and Tan, Jun‐Hao Elwin and Chua, Yan‐Jiang Benjamin and Low, Suet‐Mei Crystal and Ding, Suet Lee Shirley and Li, Chun‐wei and Jiang, Zongmin and Teh, Bin Tean and Yu, Kang and Shyh‐Chang, Ng},
  date         = {2019-03},
  journaltitle = {Cell Proliferation},
  title        = {Assessment of different strategies for scalable production and proliferation of human myoblasts},
  doi          = {10.1111/cpr.12602},
  issn         = {0960-7722},
  number       = {3},
  pages        = {e12602},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6536385/},
  urldate      = {2025-07-16},
  volume       = {52},
  abstract     = {Objectives
Myoblast transfer therapy (MTT) is a technique to replace muscle satellite cells with genetically repaired or healthy myoblasts, to treat muscular dystrophies. However, clinical trials with human myoblasts were ineffective, showing almost no benefit with MTT. One important obstacle is the rapid senescence of human myoblasts. The main purpose of our study was to compare the various methods for scalable generation of proliferative human myoblasts.

Methods
We compared the immortalization of primary myoblasts with hTERT, cyclin D1 and CDK4R24C, two chemically defined methods for deriving myoblasts from pluripotent human embryonic stem cells (hESCs), and introduction of viral MyoD into hESC‐myoblasts.

Results
Our results show that, while all the strategies above are suboptimal at generating bona fide human myoblasts that can both proliferate and differentiate robustly, chemically defined hESC‐monolayer‐myoblasts show the most promise in differentiation potential.

Conclusions
Further efforts to optimize the chemically defined differentiation of hESC‐monolayer‐myoblasts would be the most promising strategy for the scalable generation of human myoblasts, for applications in MTT and high‐throughput drug screening.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC6536385/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC6536385/pdf/CPR-52-e12602.pdf:application/pdf},
  pmcid        = {PMC6536385},
  pmid         = {30891802},
}

@Article{Shahini2018,
  author       = {Shahini, Aref and Vydiam, Kalyan and Choudhury, Debanik and Rajabian, Nika and Nguyen, Thy and Lei, Pedro and Andreadis, Stelios T.},
  date         = {2018-07},
  journaltitle = {Stem cell research},
  title        = {Efficient and high yield isolation of myoblasts from skeletal muscle},
  doi          = {10.1016/j.scr.2018.05.017},
  issn         = {1873-5061},
  pages        = {122--129},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6090567/},
  urldate      = {2025-07-16},
  volume       = {30},
  abstract     = {Skeletal muscle (SkM) regeneration relies on the activity of myogenic progenitors that reside beneath the basal lamina of myofibers. Here, we describe a protocol for the isolation of the SkM progenitors from young and old mice by exploiting their outgrowth potential from SkM explants on matrigel coated dishes in the presence of high serum, chicken embryo extract and basic fibroblast growth factor. Compared to other protocols, this method yields a higher number of myoblasts (10–20 million) by enabling the outgrowth of these cells from tissue fragments. The majority of outgrowth cells ({\textasciitilde}90\%) were positive for myogenic markers such as α7-integrin, MyoD, and Desmin. The myogenic cell population could be purified to 98\% with one round of pre-plating on collagen coated dishes, where differential attachment of fibroblasts and other non-myogenic progenitors separates them from myoblasts. Moreover, the combination of high serum medium and matrigel coating provided a proliferation advantage to myogenic cells, which expanded rapidly ({\textasciitilde}24 h population doubling), while non-myogenic cells diminished over time, thereby eliminating the need for further purification steps such as FACS sorting. Finally, myogenic progenitors gave rise to multinucleated myotubes that exhibited sarcomeres and spontaneous beating in the culture dish.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC6090567/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC6090567/pdf/nihms1500572.pdf:application/pdf},
  pmcid        = {PMC6090567},
  pmid         = {29879622},
}

@Article{Nolan2024,
  author       = {Nolan, Andy and Heaton, Robert A. and Adamova, Petra and Cole, Paige and Turton, Nadia and Gillham, Scott H. and Owens, Daniel J. and Sexton, Darren W.},
  date         = {2024-05},
  journaltitle = {Cytometry. Part A: The Journal of the International Society for Analytical Cytology},
  title        = {Fluorescent characterization of differentiated myotubes using flow cytometry},
  doi          = {10.1002/cyto.a.24822},
  issn         = {1552-4930},
  language     = {eng},
  number       = {5},
  pages        = {332--344},
  volume       = {105},
  abstract     = {Flow cytometry is routinely used in the assessment of skeletal muscle progenitor cell (myoblast) populations. However, a full gating strategy, inclusive of difficult to interpret forward and side scatter data, which documents cytometric analysis of differentiated myoblasts (myotubes) has not been reported. Beyond changes in size and shape, there are substantial metabolic and protein changes in myotubes allowing for their potential identification within heterogenous cell suspensions. To establish the utility of flow cytometry for determination of myoblasts and myotubes, C2C12 murine cell populations were assessed for cell morphology and metabolic reprogramming. Laser scatter, both forward (FSC; size) and side (SSC; granularity), measured cell morphology, while mitochondrial mass, reactive oxygen species (ROS) generation and DNA content were quantified using the fluorescent probes, MitoTracker green, CM-H2DCFDA and Vybrant DyeCycle, respectively. Immunophenotyping for myosin heavy chain (MyHC) was utilized to confirm myotube differentiation. Cellular viability was determined using Annexin V/propidium iodide dual labelling. Fluorescent microscopy was employed to visualize fluorescence and morphology. Myotube and myoblast populations were resolvable through non-intuitive interpretation of laser scatter-based morphology assessment and mitochondrial mass and activity assessment. Myotubes appeared to have similar sizes to the myoblasts based on laser scatter but exhibited greater mitochondrial mass (159\%, p {\textless} 0.0001), ROS production (303\%, p {\textless} 0.0001), DNA content (18\%, p {\textless} 0.001) and expression of MyHC (147\%, p {\textless} 0.001) compared to myoblasts. Myotube sub-populations contained a larger viable cluster of cells which were unable to be fractionated from myoblast populations and a smaller population cluster which likely contains apoptotic bodies. Imaging of differentiated myoblasts that had transited through the flow cytometer revealed the presence of intact, 'rolled-up' myotubes, which would alter laser scatter properties and potential transit through the laser beam. Our results indicate that myotubes can be analyzed successfully using flow cytometry. Increased mitochondrial mass, ROS and DNA content are key features that correlate with MyHC expression but due to myotubes 'rolling up' during flow cytometric analysis, laser scatter determination of size is not positively correlated; a phenomenon observed with some size determination particles and related to surface properties of said particles. We also note a greater heterogeneity of myotubes compared to myoblasts as evidenced by the 2 distinct sub-populations. We suggest that acoustic focussing may prove effective in identifying myotube sub populations compared to traditional hydrodynamic focussing.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/38092660:text/html},
  keywords     = {Flow Cytometry, Muscle Fibers, Skeletal, Animals, Mice, Cell Differentiation, Reactive Oxygen Species, Cell Line, Myoblasts, Mitochondria, Fluorescent Dyes, Myosin Heavy Chains, Cell Survival, C2C12, flow cytometry, myoblasts, myotubes},
  pmid         = {38092660},
}

@Article{Agley2012,
  author       = {Agley, Chibeza C. and Velloso, Cristiana P. and Lazarus, Norman R. and Harridge, Stephen D. R.},
  date         = {2012-06},
  journaltitle = {Journal of Histochemistry and Cytochemistry},
  title        = {An {Image} {Analysis} {Method} for the {Precise} {Selection} and {Quantitation} of Fluorescently {Labeled} {Cellular} {Constituents}},
  doi          = {10.1369/0022155412442897},
  issn         = {0022-1554},
  number       = {6},
  pages        = {428--438},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3393072/},
  urldate      = {2025-07-16},
  volume       = {60},
  abstract     = {The accurate measurement of the morphological characteristics of cells with nonuniform
conformations presents difficulties. We report here a straightforward method using
immunofluorescent staining and the commercially available imaging program Adobe Photoshop,
which allows objective and precise information to be gathered on irregularly shaped cells.
We have applied this measurement technique to the analysis of human muscle cells and their
immunologically marked intracellular constituents, as these cells are prone to adopting a
highly branched phenotype in culture. Use of this method can be used to overcome many of
the long-standing limitations of conventional approaches for quantifying muscle cell size
in vitro. In addition, wider applications of Photoshop as a quantitative and
semiquantitative tool in immunocytochemistry are explored.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC3393072/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC3393072/pdf/10.1369_0022155412442897.pdf:application/pdf},
  pmcid        = {PMC3393072},
  pmid         = {22511600},
}

@Article{Velica2011,
  author       = {Veliça, Pedro and Bunce, Chris M.},
  date         = {2011-09},
  journaltitle = {Muscle \& Nerve},
  title        = {A quick, simple and unbiased method to quantify {C2C12} myogenic differentiation},
  doi          = {10.1002/mus.22056},
  issn         = {1097-4598},
  language     = {eng},
  number       = {3},
  pages        = {366--370},
  volume       = {44},
  abstract     = {INTRODUCTION: C2C12 myoblasts undergo in vitro myogenesis to form protein-rich multinucleated myotubes. Determining the fraction of total nuclei incorporated into myotubes is a commonly used method to quantify the extent of differentiation, but it is labor-intensive and susceptible to operator bias.
METHODS: We have developed a simple method to quantify myotube formation using micrographs of Jenner-Giemsa-stained C2C12 cultures. Because myotubes are darkly stained by Jenner-Giemsa dyes, the extent of myotube formation correlates with an increase in pixels attributed to the darkest tones. Thus, image histograms were obtained from photographs using ImageJ software, and the sum of the darkest tones was used as a measure of myotube density.
RESULTS: Measurements of myotube density mirrored those of fusion index during C2C12 differentiation and after treatment with prostaglandin D(2) , an inhibitor of C2C12 myogenesis.
CONCLUSIONS: We propose this inexpensive, quick, and unbiased method to quantify C2C12 differentiation as a complement of the fusion index analysis.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/21996796:text/html},
  keywords     = {Animals, Cell Differentiation, Cell Line, Image Processing, Computer-Assisted, Mice, Microscopy, Muscle Development, Muscle Fibers, Skeletal, Muscle, Skeletal, Myoblasts, Skeletal, Software},
  pmid         = {21996796},
}

@Article{Noe2022,
  author       = {Noë, Simon and Corvelyn, Marlies and Willems, Sarah and Costamagna, Domiziana and Aerts, Jean-Marie and Van Campenhout, Anja and Desloovere, Kaat},
  date         = {2022-06},
  journaltitle = {Skeletal Muscle},
  title        = {The {Myotube} {Analyzer}: how to assess myogenic features in muscle stem cells},
  doi          = {10.1186/s13395-022-00297-6},
  issn         = {2044-5040},
  language     = {en},
  number       = {1},
  pages        = {12},
  url          = {https://doi.org/10.1186/s13395-022-00297-6},
  urldate      = {2025-07-16},
  volume       = {12},
  abstract     = {The analysis of in vitro cultures of human adult muscle stem cells obtained from biopsies delineates the potential of skeletal muscles and may help to understand altered muscle morphology in patients. In these analyses, the fusion index is a commonly used quantitative metric to assess the myogenic potency of the muscle stem cells. Since the fusion index only partly describes myogenic potency, we developed the Myotube Analyzer tool, which combines the definition of the fusion index with extra features of myonuclei and myotubes obtained from satellite cell cultures.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1186%2Fs13395-022-00297-6.pdf:application/pdf},
  keywords     = {Electromyography, Muscle stem cells, Muscle, Muscle Physiology, Myosin, Skeletal Muscle, Cerebral palsy, Satellite cell cultures, Fusion index, Image analysis},
  shorttitle   = {The {Myotube} {Analyzer}},
}

@Article{Abdelmoez2020,
  author       = {Abdelmoez, Ahmed M. and Sardón Puig, Laura and Smith, Jonathon A. B. and Gabriel, Brendan M. and Savikj, Mladen and Dollet, Lucile and Chibalin, Alexander V. and Krook, Anna and Zierath, Juleen R. and Pillon, Nicolas J.},
  date         = {2020-03},
  journaltitle = {American Journal of Physiology - Cell Physiology},
  title        = {Comparative profiling of skeletal muscle models reveals heterogeneity of transcriptome and metabolism},
  doi          = {10.1152/ajpcell.00540.2019},
  issn         = {0363-6143},
  number       = {3},
  pages        = {C615--C626},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7099524/},
  urldate      = {2025-07-16},
  volume       = {318},
  abstract     = {Rat L6, mouse C2C12, and primary human skeletal muscle cells (HSMCs) are commonly used to study biological processes in skeletal muscle, and experimental data on these models are abundant. However, consistently matched experimental data are scarce, and comparisons between the different cell types and adult tissue are problematic. We hypothesized that metabolic differences between these cellular models may be reflected at the mRNA level. Publicly available data sets were used to profile mRNA levels in myotubes and skeletal muscle tissues. L6, C2C12, and HSMC myotubes were assessed for proliferation, glucose uptake, glycogen synthesis, mitochondrial activity, and substrate oxidation, as well as the response to in vitro contraction. Transcriptomic profiling revealed that mRNA of genes coding for actin and myosin was enriched in C2C12, whereas L6 myotubes had the highest levels of genes encoding glucose transporters and the five complexes of the mitochondrial electron transport chain. Consistently, insulin-stimulated glucose uptake and oxidative capacity were greatest in L6 myotubes. Insulin-induced glycogen synthesis was highest in HSMCs, but C2C12 myotubes had higher baseline glucose oxidation. All models responded to electrical pulse stimulation-induced glucose uptake and gene expression but in a slightly different manner. Our analysis reveals a great degree of heterogeneity in the transcriptomic and metabolic profiles of L6, C2C12, or primary human myotubes. Based on these distinct signatures, we provide recommendations for the appropriate use of these models depending on scientific hypotheses and biological relevance.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC7099524/:text/html},
  pmcid        = {PMC7099524},
  pmid         = {31825657},
}

@Article{Inoue2018,
  author       = {Inoue, Haruki and Kunida, Katsuyuki and Matsuda, Naoki and Hoshino, Daisuke and Wada, Takumi and Imamura, Hiromi and Noji, Hiroyuki and Kuroda, Shinya},
  date         = {2018-08},
  journaltitle = {Cell Structure and Function},
  title        = {Automatic {Quantitative} {Segmentation} of {Myotubes} {Reveals} {Single}-cell {Dynamics} of {S6} {Kinase} {Activation}},
  doi          = {10.1247/csf.18012},
  issn         = {1347-3700},
  language     = {eng},
  number       = {2},
  pages        = {153--169},
  volume       = {43},
  abstract     = {Automatic cell segmentation is a powerful method for quantifying signaling dynamics at single-cell resolution in live cell fluorescence imaging. Segmentation methods for mononuclear and round shape cells have been developed extensively. However, a segmentation method for elongated polynuclear cells, such as differentiated C2C12 myotubes, has yet to be developed. In addition, myotubes are surrounded by undifferentiated reserve cells, making it difficult to identify background regions and subsequent quantification. Here we developed an automatic quantitative segmentation method for myotubes using watershed segmentation of summed binary images and a two-component Gaussian mixture model. We used time-lapse fluorescence images of differentiated C2C12 cells stably expressing Eevee-S6K, a fluorescence resonance energy transfer (FRET) biosensor of S6 kinase (S6K). Summation of binary images enhanced the contrast between myotubes and reserve cells, permitting detection of a myotube and a myotube center. Using a myotube center instead of a nucleus, individual myotubes could be detected automatically by watershed segmentation. In addition, a background correction using the two-component Gaussian mixture model permitted automatic signal intensity quantification in individual myotubes. Thus, we provide an automatic quantitative segmentation method by combining automatic myotube detection and background correction. Furthermore, this method allowed us to quantify S6K activity in individual myotubes, demonstrating that some of the temporal properties of S6K activity such as peak time and half-life of adaptation show different dose-dependent changes of insulin between cell population and individuals.Key words: time lapse images, cell segmentation, fluorescence resonance energy transfer, C2C12, myotube.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/30047513:text/html},
  keywords     = {Animals, Enzyme Activation, Fluorescence Resonance Energy Transfer, Image Processing, Computer-Assisted, Mice, Muscle Fibers, Skeletal, Optical Imaging, Ribosomal Protein S6 Kinases, Single-Cell Analysis, C2C12, cell segmentation, fluorescence resonance energy transfer, myotube, time lapse images},
  pmid         = {30047513},
}

@InCollection{Fukunaga1993,
  author    = {Fukunaga, Keinosuke},
  date      = {1993-08},
  title     = {Statistical pattern recognition},
  doi       = {10.1142/9789814343138_0002},
  isbn      = {9789810211363},
  pages     = {33--60},
  publisher = {WORLD SCIENTIFIC},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/9789814343138_0002},
  urldate   = {2025-07-31},
  file      = {Full Text PDF:https\://www.worldscientific.com/doi/pdf/10.1142/9789814343138_0002:application/pdf},
  keywords  = {Statistical pattern recognition, classifier, probability of error, hypothesis tests, effect of sample size, nonparametric, Statistical pattern recognition, classifier, probability of error, hypothesis tests, effect of sample size, nonparametric},
}

@Book{ShalevShwartz2014,
  author     = {Shalev-Shwartz, Shai and Ben-David, Shai},
  date       = {2014-05},
  title      = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
  isbn       = {9781139952743},
  language   = {en},
  note       = {Google-Books-ID: Hf6QAwAAQBAJ},
  publisher  = {Cambridge University Press},
  abstract   = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
  file       = {Google Books Link:https\://books.google.de/books?id=Hf6QAwAAQBAJ:text/html},
  keywords   = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Optical Data Processing},
  shorttitle = {Understanding {Machine} {Learning}},
}

@Article{Loog2018,
  author       = {Loog, Marco},
  date         = {2018},
  journaltitle = {Machine Learning Techniques for Space Weather},
  title        = {Supervised classification: {Quite} a brief overview},
  pages        = {113--145},
  url          = {https://www.sciencedirect.com/science/article/pii/B9780128117880000056},
  urldate      = {2025-07-31},
  file         = {Available Version (via Google Scholar):https\://arxiv.org/pdf/1710.09230:application/pdf},
  publisher    = {Elsevier},
  shorttitle   = {Supervised classification},
}

@Article{Kulkarni1998,
  author       = {Kulkarni, S.R. and Lugosi, G. and Venkatesh, S.S.},
  date         = {1998-10},
  journaltitle = {IEEE Transactions on Information Theory},
  title        = {Learning pattern classification-a survey},
  doi          = {10.1109/18.720536},
  issn         = {1557-9654},
  number       = {6},
  pages        = {2178--2206},
  url          = {https://ieeexplore.ieee.org/abstract/document/720536},
  urldate      = {2025-07-31},
  volume       = {44},
  abstract     = {Classical and recent results in statistical pattern recognition and learning theory are reviewed in a two-class pattern classification setting. This basic model best illustrates intuition and analysis techniques while still containing the essential features and serving as a prototype for many applications. Topics discussed include nearest neighbor, kernel, and histogram methods, Vapnik-Chervonenkis theory, and neural networks. The presentation and the large (though nonexhaustive) list of references is geared to provide a useful overview of this field for both specialists and nonspecialists.},
  file         = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=720536&ref=:application/pdf},
  keywords     = {Pattern recognition, Neural networks, Pattern classification, Prototypes, Information theory, Senior members, Nearest neighbor searches, Kernel, Histograms},
}

@Article{Kotsiantis2007,
  author       = {Kotsiantis, Sotiris B. and Zaharakis, Ioannis and Pintelas, P.},
  date         = {2007},
  journaltitle = {Emerging artificial intelligence applications in computer engineering},
  title        = {Supervised machine learning: {A} review of classification techniques},
  number       = {1},
  pages        = {3--24},
  url          = {https://books.google.com/books?hl=de&lr=&id=vLiTXDHr_sYC&oi=fnd&pg=PA3&dq=Kotsiantis+(2007)+Supervised+machine+learning:+A+review+of+classification+techniques&ots=C_pvsyXHon&sig=cptgUhNhe2RQAWGK255Kwyti0R4},
  urldate      = {2025-07-31},
  volume       = {160},
  file         = {Available Version (via Google Scholar):https\://informatica.si/index.php/informatica/article/viewFile/148/140:application/pdf},
  publisher    = {Amsterdam},
  shorttitle   = {Supervised machine learning},
}

@Article{Mutlag2020,
  author       = {Mutlag, Wamidh K. and Ali, Shaker K. and Aydam, Zahoor M. and Taher, Bahaa H.},
  date         = {2020-07},
  journaltitle = {Journal of Physics: Conference Series},
  title        = {Feature {Extraction} {Methods}: {A} {Review}},
  doi          = {10.1088/1742-6596/1591/1/012028},
  issn         = {1742-6596},
  language     = {en},
  number       = {1},
  pages        = {012028},
  url          = {https://dx.doi.org/10.1088/1742-6596/1591/1/012028},
  urldate      = {2025-07-31},
  volume       = {1591},
  abstract     = {Feature extraction is the main core in diagnosis, classification, clustering, recognition, and detection. Many researchers may by interesting in choosing suitable features that used in the applications. In this paper, the most important features methods are collected, and explained each one. The features in this paper are divided into four groups; Geometric features, Statistical features, Texture features, and Color features. It explains the methodology of each method, its equations, and application. In this paper, we made acomparison among them by using two types of image, one type for face images (163 images divided into 113 for training and 50 for testing) and the other for plant images(130 images divided into 100 for training and 30 for testing) to test the features in geometric and textures. Each type of image group shows that each type of images may be used suitable features may differ from other types.},
  file         = {IOP Full Text PDF:https\://iopscience.iop.org/article/10.1088/1742-6596/1591/1/012028/pdf:application/pdf},
  publisher    = {IOP Publishing},
  shorttitle   = {Feature {Extraction} {Methods}},
}

@InCollection{Guyon2006,
  author    = {Guyon, Isabelle and Elisseeff, André},
  date      = {2006},
  title     = {An {Introduction} to {Feature} {Extraction}},
  doi       = {10.1007/978-3-540-35488-8_1},
  editor    = {Guyon, Isabelle and Nikravesh, Masoud and Gunn, Steve and Zadeh, Lotfi A.},
  isbn      = {9783540354888},
  language  = {en},
  location  = {Berlin, Heidelberg},
  pages     = {1--25},
  publisher = {Springer},
  url       = {https://doi.org/10.1007/978-3-540-35488-8_1},
  urldate   = {2025-07-31},
  abstract  = {This chapter introduces the reader to the various aspects of feature extraction covered in this book. Section 1 reviews definitions and notations and proposes a unified view of the feature extraction problem. Section 2 is an overview of the methods and results presented in the book, emphasizing novel contributions. Section 3 provides the reader with an entry point in the field of feature extraction by showing small revealing examples and describing simple but effective algorithms. Finally, Section 4 introduces a more theoretical formalism and points to directions of research and open problems.},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-3-540-35488-8_1.pdf:application/pdf},
}

@InProceedings{Kunaver2005,
  author   = {Kunaver, M. and Tasic, J.F.},
  date     = {2005-11},
  title    = {Image feature extraction - an overview},
  doi      = {10.1109/EURCON.2005.1629889},
  pages    = {183--186},
  url      = {https://ieeexplore.ieee.org/abstract/document/1629889},
  urldate  = {2025-08-04},
  volume   = {1},
  abstract = {The paper presents a short overview over many different techniques for feature extraction. Feature extraction is a very important field of image processing and object recognition. Two different levels of feature extraction are also presented and the connection between them is explained.},
  file     = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=1629889&ref=:application/pdf},
  keywords = {Feature extraction, Colored noise, Shape, Image processing, Image segmentation, Histograms, Object recognition, Helium, Computer vision, Roads, feature extraction, overview, image processing},
}

@Article{Yuan2022,
  author       = {Yuan, Zhecheng and Xue, Zhengrong and Yuan, Bo and Wang, Xueqian and Wu, Yi and Gao, Yang and Xu, Huazhe},
  date         = {2022-12},
  journaltitle = {Advances in Neural Information Processing Systems},
  title        = {Pre-{Trained} {Image} {Encoder} for {Generalizable} {Visual} {Reinforcement} {Learning}},
  language     = {en},
  pages        = {13022--13037},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/548a482d4496ce109cddfbeae5defa7d-Abstract-Conference.html},
  urldate      = {2025-08-04},
  volume       = {35},
  file         = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2022/file/548a482d4496ce109cddfbeae5defa7d-Paper-Conference.pdf:application/pdf},
}

@InProceedings{Bain2021,
  author     = {Bain, Max and Nagrani, Arsha and Varol, Gül and Zisserman, Andrew},
  date       = {2021},
  title      = {Frozen in {Time}: {A} {Joint} {Video} and {Image} {Encoder} for {End}-to-{End} {Retrieval}},
  language   = {en},
  pages      = {1728--1738},
  url        = {https://openaccess.thecvf.com/content/ICCV2021/html/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.html},
  urldate    = {2025-08-04},
  file       = {Full Text PDF:https\://openaccess.thecvf.com/content/ICCV2021/papers/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.pdf:application/pdf},
  shorttitle = {Frozen in {Time}},
}

@InProceedings{Krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  date      = {2012},
  title     = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate   = {2025-08-04},
  volume    = {25},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf:application/pdf},
}

@Article{Lecun1998,
  author       = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date         = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  title        = {Gradient-based learning applied to document recognition},
  doi          = {10.1109/5.726791},
  issn         = {1558-2256},
  number       = {11},
  pages        = {2278--2324},
  url          = {https://ieeexplore.ieee.org/abstract/document/726791},
  urldate      = {2025-08-04},
  volume       = {86},
  abstract     = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  file         = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=726791&ref=:application/pdf},
  keywords     = {Neural networks, Pattern recognition, Machine learning, Optical character recognition software, Character recognition, Feature extraction, Multi-layer neural network, Optical computing, Hidden Markov models, Principal component analysis},
}

@Article{Khan2023,
  author       = {Khan, Asifullah and Rauf, Zunaira and Sohail, Anabia and Khan, Abdul Rehman and Asif, Hifsa and Asif, Aqsa and Farooq, Umair},
  date         = {2023-12},
  journaltitle = {Artificial Intelligence Review},
  title        = {A survey of the vision transformers and their {CNN}-transformer based variants},
  doi          = {10.1007/s10462-023-10595-0},
  issn         = {1573-7462},
  language     = {en},
  number       = {3},
  pages        = {2917--2970},
  url          = {https://doi.org/10.1007/s10462-023-10595-0},
  urldate      = {2025-08-04},
  volume       = {56},
  abstract     = {Vision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs10462-023-10595-0.pdf:application/pdf},
  keywords     = {Computer Vision, Intervision, Object vision, Pattern vision, Transformation Optics, Computer Imaging, Vision, Pattern Recognition and Graphics, Auto encoder, Channel boosting, Computer vision, Convolutional neural networks, Deep learning, Hybrid vision transformers, Image processing, Self-attention, Transformer},
}

@Article{Elngar2021,
  author       = {Elngar, Ahmed A. and Arafa, Mohamed and Fathy, Amar and Moustafa, Basma and Mahmoud, Omar and Shaban, Mohamed and Fawzy, Nehal},
  date         = {2021},
  journaltitle = {Journal of Cybersecurity and Information Management},
  title        = {Image classification based on {CNN}: a survey},
  number       = {1},
  pages        = {18--50},
  url          = {https://www.academia.edu/download/90135273/p2.pdf},
  urldate      = {2025-08-04},
  volume       = {6},
  file         = {Available Version (via Google Scholar):https\://www.academia.edu/download/90135273/p2.pdf:application/pdf},
  shorttitle   = {Image classification based on {CNN}},
}

@Article{Plested2022,
  author       = {Plested, Jo and Gedeon, Tom},
  date         = {2022-05},
  journaltitle = {arXiv preprint arXiv:2205.09904},
  title        = {Deep transfer learning for image classification: a survey},
  doi          = {10.48550/arXiv.2205.09904},
  note         = {arXiv:2205.09904 [cs]},
  url          = {http://arxiv.org/abs/2205.09904},
  urldate      = {2025-08-04},
  abstract     = {Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes in image classification in recent years. It has been consistently demonstrated that best practice for image classification is when large deep models can be trained on abundant labelled data. However there are many real world scenarios where the requirement for large amounts of training data to get the best performance cannot be met. In these scenarios transfer learning can help improve performance. To date there have been no surveys that comprehensively review deep transfer learning as it relates to image classification overall. However, several recent general surveys of deep transfer learning and ones that relate to particular specialised target image classification tasks have been published. We believe it is important for the future progress in the field that all current knowledge is collated and the overarching patterns analysed and discussed. In this survey we formally define deep transfer learning and the problem it attempts to solve in relation to image classification. We survey the current state of the field and identify where recent progress has been made. We show where the gaps in current knowledge are and make suggestions for how to progress the field to fill in these knowledge gaps. We present a new taxonomy of the applications of transfer learning for image classification. This taxonomy makes it easier to see overarching patterns of where transfer learning has been effective and, where it has failed to fulfill its potential. This also allows us to suggest where the problems lie and how it could be used more effectively. We show that under this new taxonomy, many of the applications where transfer learning has been shown to be ineffective or even hinder performance are to be expected when taking into account the source and target datasets and the techniques used.},
  collaborator = {Plested, Jo and Gedeon, Tom},
  file         = {Preprint PDF:http\://arxiv.org/pdf/2205.09904v1:application/pdf},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
  publisher    = {arXiv},
}

@Article{Schmidhuber2015,
  author       = {Schmidhuber, Jürgen},
  date         = {2015-01},
  journaltitle = {Neural Networks},
  title        = {Deep learning in neural networks: {An} overview},
  doi          = {10.1016/j.neunet.2014.09.003},
  issn         = {0893-6080},
  pages        = {85--117},
  url          = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
  urldate      = {2025-08-04},
  volume       = {61},
  abstract     = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0893608014002135/pdfft?download=true:application/pdf},
  keywords     = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
  shorttitle   = {Deep learning in neural networks},
}

@Article{Ghods2019,
  author       = {Ghods, Alireza and Cook, Diane J},
  date         = {2019-09},
  journaltitle = {arXiv preprint arXiv:1909.04791},
  title        = {A {Survey} of {Techniques} {All} {Classifiers} {Can} {Learn} from {Deep} {Networks}: {Models}, {Optimizations}, and {Regularization}},
  doi          = {10.48550/arXiv.1909.04791},
  note         = {arXiv:1909.04791 [cs]},
  url          = {http://arxiv.org/abs/1909.04791},
  urldate      = {2025-08-04},
  abstract     = {Deep neural networks have introduced novel and useful tools to the machine learning community. Other types of classifiers can potentially make use of these tools as well to improve their performance and generality. This paper reviews the current state of the art for deep learning classifier technologies that are being used outside of deep neural networks. Non-network classifiers can employ many components found in deep neural network architectures. In this paper, we review the feature learning, optimization, and regularization methods that form a core of deep network technologies. We then survey non-neural network learning algorithms that make innovative use of these methods to improve classification. Because many opportunities and challenges still exist, we discuss directions that can be pursued to expand the area of deep learning for a variety of classification algorithms.},
  collaborator = {Ghods, Alireza and Cook, Diane J.},
  file         = {Preprint PDF:http\://arxiv.org/pdf/1909.04791v2:application/pdf},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  publisher    = {arXiv},
}

@InProceedings{Kirillov2019a,
  author    = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  date      = {2019},
  title     = {Panoptic feature pyramid networks},
  pages     = {6399--6408},
  url       = {http://openaccess.thecvf.com/content_CVPR_2019/html/Kirillov_Panoptic_Feature_Pyramid_Networks_CVPR_2019_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):http\://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Feature_Pyramid_Networks_CVPR_2019_paper.pdf:application/pdf},
}

@InProceedings{kirillov2019PQ,
  author    = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll{\'a}r, Piotr},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  date      = {2019},
  title     = {Panoptic segmentation},
  pages     = {9404--9413},
  url       = {http://openaccess.thecvf.com/content_CVPR_2019/html/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf:application/pdf},
  year      = {2019},
}

@Article{Yosinski2014,
  author       = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  date         = {2014},
  journaltitle = {Advances in neural information processing systems},
  title        = {How transferable are features in deep neural networks?},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf},
  urldate      = {2025-08-05},
  volume       = {27},
  file         = {Available Version (via Google Scholar):https\://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf:application/pdf},
}

@InProceedings{Tan2021,
  author     = {Tan, Mingxing and Le, Quoc},
  booktitle  = {International conference on machine learning},
  date       = {2021},
  title      = {Efficientnetv2: {Smaller} models and faster training},
  pages      = {10096--10106},
  publisher  = {PMLR},
  url        = {http://proceedings.mlr.press/v139/tan21a.html},
  urldate    = {2025-08-05},
  file       = {Available Version (via Google Scholar):http\://proceedings.mlr.press/v139/tan21a/tan21a.pdf:application/pdf},
  shorttitle = {Efficientnetv2},
}

@Article{Minaee2021,
  author       = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
  date         = {2021},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  title        = {Image segmentation using deep learning: {A} survey},
  number       = {7},
  pages        = {3523--3542},
  url          = {https://ieeexplore.ieee.org/abstract/document/9356353/},
  urldate      = {2025-08-05},
  volume       = {44},
  file         = {Available Version (via Google Scholar):https\://ieeexplore.ieee.org/iel7/34/4359286/09356353.pdf:application/pdf},
  publisher    = {IEEE},
  shorttitle   = {Image segmentation using deep learning},
}

@InProceedings{Long2015,
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2015},
  title     = {Fully convolutional networks for semantic segmentation},
  pages     = {3431--3440},
  url       = {http://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf:application/pdf},
}

@InCollection{Ronneberger2015,
  author     = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date       = {2015},
  title      = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
  doi        = {10.1007/978-3-319-24574-4_28},
  editor     = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  isbn       = {9783319245737 9783319245744},
  language   = {en},
  location   = {Cham},
  pages      = {234--241},
  publisher  = {Springer International Publishing},
  url        = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
  urldate    = {2025-08-05},
  volume     = {9351},
  file       = {Available Version (via Google Scholar):https\://arxiv.org/pdf/1505.04597:application/pdf},
  shorttitle = {U-{Net}},
}

@InProceedings{Arnab2017,
  author    = {Arnab, Anurag and Torr, Philip HS},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2017},
  title     = {Pixelwise instance segmentation with a dynamically instantiated network},
  pages     = {441--450},
  url       = {http://openaccess.thecvf.com/content_cvpr_2017/html/Arnab_Pixelwise_Instance_Segmentation_CVPR_2017_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):http\://openaccess.thecvf.com/content_cvpr_2017/papers/Arnab_Pixelwise_Instance_Segmentation_CVPR_2017_paper.pdf:application/pdf},
}

@InProceedings{Chen2018,
  author     = {Chen, Liang-Chieh and Hermans, Alexander and Papandreou, George and Schroff, Florian and Wang, Peng and Adam, Hartwig},
  booktitle  = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date       = {2018},
  title      = {Masklab: {Instance} segmentation by refining object detection with semantic and direction features},
  pages      = {4013--4022},
  url        = {http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_MaskLab_Instance_Segmentation_CVPR_2018_paper.html},
  urldate    = {2025-08-05},
  file       = {Available Version (via Google Scholar):http\://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_MaskLab_Instance_Segmentation_CVPR_2018_paper.pdf:application/pdf},
  shorttitle = {Masklab},
}

@InProceedings{Winn2006,
  author    = {Winn, John and Shotton, Jamie},
  booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
  date      = {2006},
  title     = {The layout consistent random field for recognizing and segmenting partially occluded objects},
  pages     = {37--44},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/abstract/document/1640739/},
  urldate   = {2025-08-05},
  volume    = {1},
  file      = {Available Version (via Google Scholar):https\://ieeexplore.ieee.org/iel5/10924/34373/01640739.pdf:application/pdf},
}

@InCollection{Hariharan2014,
  author    = {Hariharan, Bharath and Arbeláez, Pablo and Girshick, Ross and Malik, Jitendra},
  date      = {2014},
  title     = {Simultaneous {Detection} and {Segmentation}},
  doi       = {10.1007/978-3-319-10584-0_20},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  isbn      = {9783319105833 9783319105840},
  language  = {en},
  location  = {Cham},
  pages     = {297--312},
  publisher = {Springer International Publishing},
  url       = {http://link.springer.com/10.1007/978-3-319-10584-0_20},
  urldate   = {2025-08-05},
  volume    = {8695},
  copyright = {http://www.springer.com/tdm},
  file      = {Available Version (via Google Scholar):https\://arxiv.org/pdf/1407.1808:application/pdf},
}

@InProceedings{Girshick2014,
  author    = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2014},
  title     = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  pages     = {580--587},
  url       = {http://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
  urldate   = {2025-08-05},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf:application/pdf},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2016},
  title     = {Deep residual learning for image recognition},
  pages     = {770--778},
  url       = {http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate   = {2025-08-11},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf:application/pdf},
}

@InProceedings{Liu2022,
  author    = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  date      = {2022},
  title     = {A convnet for the 2020s},
  pages     = {11976--11986},
  url       = {http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html},
  urldate   = {2025-08-11},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf:application/pdf},
}

@InProceedings{Liu2022a,
  author     = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li},
  booktitle  = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  date       = {2022},
  title      = {Swin transformer v2: {Scaling} up capacity and resolution},
  pages      = {12009--12019},
  url        = {http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html},
  urldate    = {2025-08-11},
  file       = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.pdf:application/pdf},
  shorttitle = {Swin transformer v2},
}

@Article{Gupta2019,
  author       = {Gupta, Suyog and Tan, Mingxing},
  date         = {2019},
  journaltitle = {Google AI Blog},
  title        = {{EfficientNet}-{EdgeTPU}: {Creating} accelerator-optimized neural networks with {AutoML}},
  number       = {1},
  volume       = {2},
  shorttitle   = {{EfficientNet}-{EdgeTPU}},
}

@InProceedings{Sandler2018,
  author     = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle  = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date       = {2018},
  title      = {Mobilenetv2: {Inverted} residuals and linear bottlenecks},
  pages      = {4510--4520},
  url        = {http://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html},
  urldate    = {2025-08-18},
  file       = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf:application/pdf},
  shorttitle = {Mobilenetv2},
}

@InProceedings{Tan2019,
  author     = {Tan, Mingxing and Le, Quoc},
  booktitle  = {International conference on machine learning},
  date       = {2019},
  title      = {Efficientnet: {Rethinking} model scaling for convolutional neural networks},
  pages      = {6105--6114},
  publisher  = {PMLR},
  url        = {https://proceedings.mlr.press/v97/tan19a.html?ref=ji},
  urldate    = {2025-08-12},
  file       = {Available Version (via Google Scholar):http\://proceedings.mlr.press/v97/tan19a/tan19a.pdf:application/pdf},
  shorttitle = {Efficientnet},
}

@InProceedings{He2015,
  author    = {He, Kaiming and Sun, Jian},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2015},
  title     = {Convolutional neural networks at constrained time cost},
  pages     = {5353--5360},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/He_Convolutional_Neural_Networks_2015_CVPR_paper.html},
  urldate   = {2025-08-18},
  file      = {Available Version (via Google Scholar):https\://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf:application/pdf},
}

@article{srivastava2015highway,
  title={Highway networks},
  author={Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1505.00387},
  year={2015}
}

@Article{Russakovsky2015,
  author       = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date         = {2015-12},
  journaltitle = {International Journal of Computer Vision},
  title        = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
  doi          = {10.1007/s11263-015-0816-y},
  issn         = {0920-5691, 1573-1405},
  language     = {en},
  number       = {3},
  pages        = {211--252},
  url          = {http://link.springer.com/10.1007/s11263-015-0816-y},
  urldate      = {2025-08-18},
  volume       = {115},
  file         = {Available Version (via Google Scholar):https\://arxiv.org/pdf/1409.0575:application/pdf},
}

@InProceedings{Kornblith2019,
  author    = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2019-06},
  title     = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
  pages     = {2661--2671},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/html/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.html},
  urldate   = {2025-08-18},
  file      = {Full Text PDF:http\://openaccess.thecvf.com/content_CVPR_2019/papers/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.pdf:application/pdf},
}

@Article{Beyer2020,
  author       = {Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
  date         = {2020-06},
  journaltitle = {arXiv preprint arXiv:2006.07159},
  title        = {Are we done with {ImageNet}?},
  doi          = {10.48550/arXiv.2006.07159},
  note         = {arXiv:2006.07159 [cs]},
  url          = {http://arxiv.org/abs/2006.07159},
  urldate      = {2025-08-18},
  abstract     = {Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition.},
  annotation   = {Comment: All five authors contributed equally. New labels at https://github.com/google-research/reassessed-imagenet},
  collaborator = {Beyer, Lucas and Hénaff, Olivier J. and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, Aäron van den},
  file         = {Preprint PDF:http\://arxiv.org/pdf/2006.07159v1:application/pdf},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  publisher    = {arXiv},
}

@InProceedings{Recht2019,
  author    = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle = {International conference on machine learning},
  date      = {2019},
  title     = {Do imagenet classifiers generalize to imagenet?},
  pages     = {5389--5400},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/v97/recht19a.html},
  urldate   = {2025-08-18},
  file      = {Available Version (via Google Scholar):http\://proceedings.mlr.press/v97/recht19a/recht19a.pdf:application/pdf},
}

@InProceedings{You2018,
  author    = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
  date      = {2018-08},
  title     = {{ImageNet} {Training} in {Minutes}},
  doi       = {10.1145/3225058.3225069},
  isbn      = {9781450365109},
  language  = {en},
  location  = {Eugene OR USA},
  pages     = {1--10},
  publisher = {ACM},
  url       = {https://dl.acm.org/doi/10.1145/3225058.3225069},
  urldate   = {2025-08-18},
  file      = {Available Version (via Google Scholar):https\://dl.acm.org/doi/pdf/10.1145/3225058.3225069:application/pdf},
}

@InProceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  title     = {Rectified linear units improve restricted boltzmann machines},
  pages     = {807--814},
  year      = {2010},
}

@Article{Hendrycks2016,
  author  = {Hendrycks, Dan and Gimpel, Kevin},
  title   = {Gaussian error linear units (gelus)},
  journal = {arXiv preprint arXiv:1606.08415},
  year    = {2016},
}

@Article{Ioffe2017,
  author       = {Ioffe, Sergey},
  date         = {2017},
  journaltitle = {Advances in neural information processing systems},
  title        = {Batch renormalization: {Towards} reducing minibatch dependence in batch-normalized models},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/c54e7837e0cd0ced286cb5995327d1ab-Abstract.html},
  urldate      = {2025-08-18},
  volume       = {30},
  file         = {Available Version (via Google Scholar):https\://proceedings.neurips.cc/paper/2017/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf:application/pdf},
  shorttitle   = {Batch renormalization},
}

@Article{Ba2016,
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  title   = {Layer normalization},
  journal = {arXiv preprint arXiv:1607.06450},
  year    = {2016},
}

 
@InProceedings{liu2021,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  title     = {Swin transformer: Hierarchical vision transformer using shifted windows},
  pages     = {10012--10022},
  year      = {2021},
}

 
@Article{Bridle1989,
  author       = {Bridle, John},
  date         = {1989},
  journaltitle = {Advances in neural information processing systems},
  title        = {Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters},
  url          = {https://proceedings.neurips.cc/paper/1989/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html},
  urldate      = {2025-08-19},
  volume       = {2},
  file         = {Available Version (via Google Scholar):https\://proceedings.neurips.cc/paper/1989/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf:application/pdf},
}

@InProceedings{Kinga2015,
  author       = {Kinga, Diederik and Adam, Jimmy Ba and others},
  booktitle    = {International conference on learning representations (ICLR)},
  title        = {A method for stochastic optimization},
  number       = {6},
  organization = {California;},
  volume       = {5},
  year         = {2015},
}

@Article{Sukhbaatar2014,
  author  = {Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
  title   = {Training convolutional networks with noisy labels},
  journal = {arXiv preprint arXiv:1406.2080},
  year    = {2014},
}

@Article{Gu2018,
  author       = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei},
  date         = {2018},
  journaltitle = {Pattern recognition},
  title        = {Recent advances in convolutional neural networks},
  pages        = {354--377},
  url          = {https://www.sciencedirect.com/science/article/pii/S0031320317304120},
  urldate      = {2025-08-27},
  volume       = {77},
  publisher    = {Elsevier},
}

@InProceedings{Zeiler2014,
  author    = {Zeiler, Matthew D. and Fergus, Rob},
  booktitle = {Computer {Vision} – {ECCV} 2014},
  date      = {2014},
  title     = {Visualizing and {Understanding} {Convolutional} {Networks}},
  doi       = {10.1007/978-3-319-10590-1_53},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  isbn      = {9783319105901},
  language  = {en},
  location  = {Cham},
  pages     = {818--833},
  publisher = {Springer International Publishing},
  abstract  = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-3-319-10590-1_53.pdf:application/pdf},
  keywords  = {Input Image, Training Image, Convolutional Neural Network, Stochastic Gradient Descent, Pixel Space},
}

@InProceedings{Zeiler2010,
  author   = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
  date     = {2010-06},
  title    = {Deconvolutional networks},
  doi      = {10.1109/CVPR.2010.5539957},
  note     = {ISSN: 1063-6919},
  pages    = {2528--2535},
  url      = {https://ieeexplore.ieee.org/document/5539957},
  urldate  = {2025-08-27},
  abstract = {Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.},
  file     = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=5539957&ref=:application/pdf},
  issn     = {1063-6919},
  keywords = {Convolution, Decoding, Image edge detection, Image representation, Filters, Computer architecture, Robustness, Object recognition, Feature extraction, Image restoration},
}

@InProceedings{Mostajabi2015,
  author    = {Mostajabi, Mohammadreza and Yadollahpour, Payman and Shakhnarovich, Gregory},
  booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  date      = {2015},
  title     = {Feedforward semantic segmentation with zoom-out features},
  pages     = {3376--3385},
  url       = {http://openaccess.thecvf.com/content_cvpr_2015/html/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.html},
  urldate   = {2025-08-27},
  file      = {Available Version (via Google Scholar):https\://openaccess.thecvf.com/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf:application/pdf},
}

@InProceedings{Noh2015,
  author   = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  date     = {2015-12},
  title    = {Learning {Deconvolution} {Network} for {Semantic} {Segmentation}},
  doi      = {10.1109/ICCV.2015.178},
  note     = {ISSN: 2380-7504},
  pages    = {1520--1528},
  url      = {https://ieeexplore.ieee.org/document/7410535},
  urldate  = {2025-08-27},
  abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
  file     = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=7410535&ref=:application/pdf},
  issn     = {2380-7504},
  keywords = {Deconvolution, Semantics, Image segmentation, Visualization, Feature extraction, Shape, Image reconstruction},
}

@InCollection{Abay2019,
  author    = {Abay, Nazmiye Ceren and Zhou, Yan and Kantarcioglu, Murat and Thuraisingham, Bhavani and Sweeney, Latanya},
  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  date      = {2019},
  title     = {Privacy {Preserving} {Synthetic} {Data} {Release} {Using} {Deep} {Learning}},
  doi       = {10.1007/978-3-030-10925-7_31},
  editor    = {Berlingerio, Michele and Bonchi, Francesco and Gärtner, Thomas and Hurley, Neil and Ifrim, Georgiana},
  isbn      = {9783030109240 9783030109257},
  language  = {en},
  location  = {Cham},
  pages     = {510--526},
  publisher = {Springer International Publishing},
  url       = {https://link.springer.com/10.1007/978-3-030-10925-7_31},
  urldate   = {2025-08-27},
  volume    = {11051},
  file      = {Available Version (via Google Scholar):https\://www.researchgate.net/profile/Nazmiye-Abay/publication/330460051_Privacy_Preserving_Synthetic_Data_Release_Using_Deep_Learning/links/5c4b856ba6fdccd6b5c977a6/Privacy-Preserving-Synthetic-Data-Release-Using-Deep-Learning.pdf:application/pdf},
}

@Article{Raghunathan2021,
  author       = {Raghunathan, Trivellore E.},
  date         = {2021-03},
  journaltitle = {Annual Review of Statistics and Its Application},
  title        = {Synthetic {Data}},
  doi          = {10.1146/annurev-statistics-040720-031848},
  issn         = {2326-8298, 2326-831X},
  language     = {en},
  number       = {1},
  pages        = {129--140},
  url          = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-040720-031848},
  urldate      = {2025-08-27},
  volume       = {8},
  abstract     = {Demand for access to data, especially data collected using public funds, is ever growing. At the same time, concerns about the disclosure of the identities of and sensitive information about the respondents providing the data are making the data collectors limit the access to data. Synthetic data sets, generated to emulate certain key information found in the actual data and provide the ability to draw valid statistical inferences, are an attractive framework to afford widespread access to data for analysis while mitigating privacy and confidentiality concerns. The goal of this article is to provide a review of various approaches for generating and analyzing synthetic data sets, inferential justification, limitations of the approaches, and directions for future research.},
  file         = {Available Version (via Google Scholar):https\://scholar.google.com/scholar?output=instlink&q=info\:GGDhDY8d0HcJ\:scholar.google.com/&hl=de&as_sdt=0,5&scillfp=16882053589501831235&oi=lle:},
}

@Book{Nikolenko2021,
  author    = {Nikolenko, Sergey I and others},
  title     = {Synthetic data for deep learning},
  publisher = {Springer},
  volume    = {174},
  year      = {2021},
}

@InProceedings{Choi2017,
  author    = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
  booktitle = {Machine learning for healthcare conference},
  date      = {2017},
  title     = {Generating multi-label discrete patient records using generative adversarial networks},
  pages     = {286--305},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/v68/choi17a},
  urldate   = {2025-08-27},
  file      = {Available Version (via Google Scholar):http\://proceedings.mlr.press/v68/choi17a/choi17a.pdf:application/pdf},
}

@Article{Lu2023,
  author  = {Lu, Yingzhou and Shen, Minjie and Wang, Huazheng and Wang, Xiao and van Rechem, Capucine and Fu, Tianfan and Wei, Wenqi},
  title   = {Machine learning for synthetic data generation: a review},
  journal = {arXiv preprint arXiv:2302.04062},
  year    = {2023},
}

@Article{Jain1999,
  author       = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
  date         = {1999-09},
  journaltitle = {ACM Comput. Surv.},
  title        = {Data clustering: a review},
  doi          = {10.1145/331499.331504},
  issn         = {0360-0300},
  number       = {3},
  pages        = {264--323},
  url          = {https://dl.acm.org/doi/10.1145/331499.331504},
  urldate      = {2025-08-28},
  volume       = {31},
  abstract     = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
  file         = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/331499.331504:application/pdf},
  shorttitle   = {Data clustering},
}

@Article{Xu2005,
  author       = {Xu, Rui and Wunsch, D.},
  date         = {2005-05},
  journaltitle = {IEEE Transactions on Neural Networks},
  title        = {Survey of clustering algorithms},
  doi          = {10.1109/TNN.2005.845141},
  issn         = {1941-0093},
  number       = {3},
  pages        = {645--678},
  url          = {https://ieeexplore.ieee.org/abstract/document/1427769},
  urldate      = {2025-08-28},
  volume       = {16},
  abstract     = {Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.},
  file         = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=1427769&ref=:application/pdf},
  keywords     = {Clustering algorithms, Machine learning algorithms, Data analysis, Humans, Statistics, Computer science, Machine learning, Application software, Traveling salesman problems, Bioinformatics, Adaptive resonance theory (ART), clustering, clustering algorithm, cluster validation, neural networks, proximity, self-organizing feature map (SOFM)},
}

@Article{Zahn1972,
  author       = {Zahn, Charles T. and Roskies, Ralph Z.},
  date         = {1972-03},
  journaltitle = {IEEE Transactions on Computers},
  title        = {Fourier {Descriptors} for {Plane} {Closed} {Curves}},
  doi          = {10.1109/TC.1972.5008949},
  issn         = {1557-9956},
  number       = {3},
  pages        = {269--281},
  url          = {https://ieeexplore.ieee.org/abstract/document/5008949},
  urldate      = {2025-08-28},
  volume       = {C-21},
  abstract     = {A method for the analysis and synthesis of closed curves in the plane is developed using the Fourier descriptors FD's of Cosgriff [1]. A curve is represented parametrically as a function of arc length by the accumulated change in direction of the curve since the starting point. This function is expanded in a Fourier series and the coefficients are arranged in the amplitude/phase-angle form. It is shown that the amplitudes are pure form invariants as well as are certain simple functions of phase angles. Rotational and axial symmetry are related directly to simple properties of the Fourier descriptors. An analysis of shape similarity or symmetry can be based on these relationships; also closed symmetric curves can be synthesized from almost arbitrary Fourier descriptors. It is established that the Fourier series expansion is optimal and unique with respect to obtaining coefficients insensitive to starting point. Several examples are provided to indicate the usefulness of Fourier descriptors as features for shape discrimination and a number of interesting symmetric curves are generated by computer and plotted out.},
  file         = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=5008949&ref=:application/pdf},
  keywords     = {Shape, Harmonic analysis, Fourier series, Shape measurement, Data mining, Phase measurement, Probability density function, Boundary functions, curvature functions, form-invariant features, Fourier descriptors, image processing, intrinsic functions, pattern analysis, pattern recognition, pattern symmetry, pattern synthesis, planar curves, shape description},
}

@Article{Kuhl1982,
  author       = {Kuhl, Frank P and Giardina, Charles R},
  date         = {1982-03},
  journaltitle = {Computer Graphics and Image Processing},
  title        = {Elliptic {Fourier} features of a closed contour},
  doi          = {10.1016/0146-664X(82)90034-X},
  issn         = {0146-664X},
  number       = {3},
  pages        = {236--258},
  url          = {https://www.sciencedirect.com/science/article/pii/0146664X8290034X},
  urldate      = {2025-08-28},
  volume       = {18},
  abstract     = {A direct procedure for obtaining the Fourier coefficients of a chain-encoded contour is presented. Advantages of the procedure are that it does not require integration or the use of fast Fourier transform techniques, and that bounds on the accuracy of the image contour reconstruction are easy to specify. Elliptic properties of the Fourier coefficients are shown and used for a convenient and intuitively pleasing procedure of normalizing a Fourier contour representation. Extension of the contour representation to arbitrary objects at arbitrary aspect angle is discussed. The procedures have direct application to a variety of pattern recognition problems that involve analysis of well-defined image contours.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/0146664X8290034X/pdf?md5=6ae010ab3f41bfa63fbf7f2f52729025&pid=1-s2.0-0146664X8290034X-main.pdf&isDTMRedir=Y:application/pdf},
}

@Article{Hotelling1933,
  author       = {Hotelling, H.},
  date         = {1933},
  journaltitle = {Journal of Educational Psychology},
  title        = {Analysis of a complex of statistical variables into principal components},
  doi          = {10.1037/h0071325},
  issn         = {1939-2176},
  number       = {6},
  pages        = {417--441},
  volume       = {24},
  abstract     = {The problem is stated in detail, a method of analysis is derived and its geometrical meaning shown, methods of solution are illustrated and certain derivative problems are discussed. (To be concluded in October issue.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords     = {Statistical Analysis, Statistical Variables},
  location     = {US},
  publisher    = {Warwick \& York},
}

@Article{Pearson1901,
  author    = {Pearson, Karl},
  title     = {LIII. On lines and planes of closest fit to systems of points in space},
  number    = {11},
  pages     = {559--572},
  volume    = {2},
  journal   = {The London, Edinburgh, and Dublin philosophical magazine and journal of science},
  publisher = {Taylor \& Francis},
  year      = {1901},
}

@InProceedings{Delalleau2005,
  author    = {Delalleau, Olivier and Bengio, Yoshua and Le Roux, Nicolas},
  booktitle = {International {Workshop} on {Artificial} {Intelligence} and {Statistics}},
  date      = {2005},
  title     = {Efficient non-parametric function induction in semi-supervised learning},
  pages     = {96--103},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/r5/delalleau05a/delalleau05a.pdf},
  urldate   = {2025-08-28},
  file      = {Available Version (via Google Scholar):http\://proceedings.mlr.press/r5/delalleau05a/delalleau05a.pdf:application/pdf},
}

@InProceedings{Zhou2003,
  author    = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas and Weston, Jason and Schölkopf, Bernhard},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  date      = {2003},
  title     = {Learning with {Local} and {Global} {Consistency}},
  publisher = {MIT Press},
  url       = {https://proceedings.neurips.cc/paper/2003/hash/87682805257e619d49b8e0dfdc14affa-Abstract.html},
  urldate   = {2025-08-28},
  volume    = {16},
  abstract  = {We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive in- ference. A principled approach to semi-supervised learning is to design a classifying function which is suf(cid:2)ciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of clas- si(cid:2)cation problems and demonstrates effective use of unlabeled data.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2003/file/87682805257e619d49b8e0dfdc14affa-Paper.pdf:application/pdf},
}

@InProceedings{Yarowsky1995,
  author    = {Yarowsky, David},
  booktitle = {Proceedings of the 33rd annual meeting on {Association} for {Computational} {Linguistics}},
  date      = {1995-06},
  title     = {Unsupervised word sense disambiguation rivaling supervised methods},
  doi       = {10.3115/981658.981684},
  location  = {USA},
  pages     = {189--196},
  publisher = {Association for Computational Linguistics},
  series    = {{ACL} '95},
  url       = {https://dl.acm.org/doi/10.3115/981658.981684},
  urldate   = {2025-08-28},
  abstract  = {This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96\%.},
  file      = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.3115/981658.981684:application/pdf},
}

@Book{Smola1998,
  author    = {Smola, Alexander J. and Schölkopf, Bernhard},
  date      = {1998},
  title     = {Learning with kernels},
  publisher = {Citeseer},
  url       = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=1655f14b601fe6ff80ca935091e3cd78c9733d86},
  urldate   = {2025-08-28},
  volume    = {4},
  file      = {Available Version (via Google Scholar):https\://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=1655f14b601fe6ff80ca935091e3cd78c9733d86:application/pdf},
}

@PhdThesis{Schoelkopf1997,
  author      = {Schölkopf, Bernhard},
  date        = {1997},
  institution = {Oldenbourg München, Germany},
  title       = {Support vector learning},
  type        = {{PhD} {Thesis}},
  url         = {https://pure.mpg.de/rest/items/item_1794215/component/file_3214422/content},
  urldate     = {2025-08-28},
  file        = {Available Version (via Google Scholar):https\://pure.mpg.de/rest/items/item_1794215/component/file_3214422/content:application/pdf},
}

@Article{Lowe1988,
  author       = {Lowe, David and Broomhead, D.},
  date         = {1988},
  journaltitle = {Complex systems},
  title        = {Multivariable functional interpolation and adaptive networks},
  number       = {3},
  pages        = {321--355},
  url          = {http://wpmedia.wolfram.com/uploads/sites/13/2018/02/02-3-5.pdf},
  urldate      = {2025-08-28},
  volume       = {2},
  file         = {Available Version (via Google Scholar):http\://wpmedia.wolfram.com/uploads/sites/13/2018/02/02-3-5.pdf:application/pdf},
}

@InProceedings{Boser1992,
  author    = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  booktitle = {Proceedings of the fifth annual workshop on {Computational} learning theory},
  date      = {1992-07},
  title     = {A training algorithm for optimal margin classifiers},
  doi       = {10.1145/130385.130401},
  isbn      = {9780897914970},
  location  = {New York, NY, USA},
  pages     = {144--152},
  publisher = {Association for Computing Machinery},
  series    = {{COLT} '92},
  url       = {https://dl.acm.org/doi/10.1145/130385.130401},
  urldate   = {2025-08-28},
  abstract  = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  file      = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/130385.130401:application/pdf},
}

 
@Article{Hughes1968,
  author       = {Hughes, G.},
  date         = {1968-01},
  journaltitle = {IEEE Transactions on Information Theory},
  title        = {On the mean accuracy of statistical pattern recognizers},
  doi          = {10.1109/TIT.1968.1054102},
  issn         = {1557-9654},
  number       = {1},
  pages        = {55--63},
  url          = {https://ieeexplore.ieee.org/abstract/document/1054102},
  urldate      = {2025-08-29},
  volume       = {14},
  abstract     = {The overall mean recognition probability (mean accuracy) of a pattern classifier is calculated and numerically plotted as a function of the pattern measurement complexity n and design data set sizem. Utilized is the well-known probabilistic model of a two-class, discrete-measurement pattern environment (no Gaussian or statistical independence assumptions are made). The minimum-error recognition rule (Bayes) is used, with the unknown pattern environment probabilities estimated from the data relative frequencies. In calculating the mean accuracy over all such environments, only three parameters remain in the final equation:n, m, and the prior probabilityp\_cof either of the pattern classes. With a fixed design pattern sample, recognition accuracy can first increase as the number of measurements made on a pattern increases, but decay with measurement complexity higher than some optimum value. Graphs of the mean accuracy exhibit both an optimal and a maximum acceptable value ofnfor fixedmandp\_c. A four-place tabulation of the optimumnand maximum mean accuracy values is given for equally likely classes andmranging from2to1000. The penalty exacted for the generality of the analysis is the use of the mean accuracy itself as a recognizer optimality criterion. Namely, one necessarily always has some particular recognition problem at hand whose Bayes accuracy will be higher or lower than the mean over all recognition problems having fixedn, m, andp\_c.},
  keywords     = {Pattern recognition, Probability, Bayes methods, Complexity theory, Standards, Vectors, Numerical models},
}

 
@Article{Dippel2022,
  author  = {Dippel, Jonas and Lenga, Matthias and Goerttler, Thomas and Obermayer, Klaus and H{\"o}hne, Johannes},
  title   = {Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder},
  journal = {arXiv preprint arXiv:2207.14508},
  year    = {2022},
}

@Book{Marr2010,
  author    = {Marr, David},
  title     = {Vision: A computational investigation into the human representation and processing of visual information},
  publisher = {MIT press},
  year      = {2010},
}

@Article{Marr1978,
  author    = {Marr, David and Nishihara, Herbert Keith},
  title     = {Representation and recognition of the spatial organization of three-dimensional shapes},
  number    = {1140},
  pages     = {269--294},
  volume    = {200},
  journal   = {Proceedings of the Royal Society of London. Series B. Biological Sciences},
  publisher = {The Royal Society London},
  year      = {1978},
}

@Article{Moore2021,
  author       = {Moore, Josh and Allan, Chris and Besson, Sébastien and Burel, Jean-Marie and Diel, Erin and Gault, David and Kozlowski, Kevin and Lindner, Dominik and Linkert, Melissa and Manz, Trevor and Moore, Will and Pape, Constantin and Tischer, Christian and Swedlow, Jason R.},
  date         = {2021-12},
  journaltitle = {Nature Methods},
  title        = {{OME}-{NGFF}: a next-generation file format for expanding bioimaging data-access strategies},
  doi          = {10.1038/s41592-021-01326-w},
  issn         = {1548-7105},
  language     = {en},
  number       = {12},
  pages        = {1496--1498},
  url          = {https://www.nature.com/articles/s41592-021-01326-w},
  urldate      = {2025-09-03},
  volume       = {18},
  abstract     = {The rapid pace of innovation in biological imaging and the diversity of its applications have prevented the establishment of a community-agreed standardized data format. We propose that complementing established open formats such as OME-TIFF and HDF5 with a next-generation file format such as Zarr will satisfy the majority of use cases in bioimaging. Critically, a common metadata format used in all these vessels can deliver truly findable, accessible, interoperable and reusable bioimaging data.},
  copyright    = {2021 The Author(s)},
  file         = {Full Text PDF:https\://www.nature.com/articles/s41592-021-01326-w.pdf:application/pdf},
  keywords     = {Computational platforms and environments, Data publication and archiving},
  publisher    = {Nature Publishing Group},
  shorttitle   = {{OME}-{NGFF}},
}

@Article{Li2023,
  author       = {Li, Xuesong and Wu, Yicong and Su, Yijun and Rey-Suarez, Ivan and Matthaeus, Claudia and Updegrove, Taylor B. and Wei, Zhuang and Zhang, Lixia and Sasaki, Hideki and Li, Yue and Guo, Min and Giannini, John P. and Vishwasrao, Harshad D. and Chen, Jiji and Lee, Shih-Jong J. and Shao, Lin and Liu, Huafeng and Ramamurthi, Kumaran S. and Taraska, Justin W. and Upadhyaya, Arpita and La Riviere, Patrick and Shroff, Hari},
  date         = {2023},
  journaltitle = {Nature Biotechnology},
  title        = {Three-dimensional structured illumination microscopy with enhanced axial resolution},
  doi          = {10.1038/s41587-022-01651-1},
  issn         = {1087-0156},
  number       = {9},
  pages        = {1307--1319},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10497409/},
  urldate      = {2025-09-03},
  volume       = {41},
  abstract     = {The axial resolution of three-dimensional structured illumination microscopy (3D SIM) is limited to ∼300 nm. Here we present two distinct, complementary methods to improve axial resolution in 3D SIM with minimal or no modification to the optical system. We show that placing a mirror directly opposite the sample enables four-beam interference with higher spatial frequency content than 3D SIM illumination, offering near-isotropic imaging with ∼120-nm lateral and 160-nm axial resolution. We also developed a deep learning method achieving ∼120-nm isotropic resolution. This method can be combined with denoising to facilitate volumetric imaging spanning dozens of timepoints. We demonstrate the potential of these advances by imaging a variety of cellular samples, delineating the nanoscale distribution of vimentin and microtubule filaments, observing the relative positions of caveolar coat proteins and lysosomal markers and visualizing cytoskeletal dynamics within T cells in the early stages of immune synapse formation., Enhanced axial resolution for 3D SIM is achieved with deep learning or four-beam interference.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC10497409/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC10497409/pdf/41587_2022_Article_1651.pdf:application/pdf},
  pmcid        = {PMC10497409},
  pmid         = {36702897},
}

@InProceedings{Pan2023,
  author       = {Pan, Mingjie and Gan, Yulu and Zhou, Fangxu and Liu, Jiaming and Zhang, Ying and Wang, Aimin and Zhang, Shanghang and Li, Dawei},
  booktitle    = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  title        = {DiffuseIR: diffusion models for isotropic reconstruction of 3D microscopic images},
  organization = {Springer},
  pages        = {323--332},
  year         = {2023},
}

@Article{Bagheri2022,
  author       = {Bagheri, Neda and Carpenter, Anne E. and Lundberg, Emma and Plant, Anne L. and Horwitz, Rick},
  date         = {2022-01},
  journaltitle = {Molecular cell},
  title        = {The new era of quantitative cell imaging—challenges and opportunities},
  doi          = {10.1016/j.molcel.2021.12.024},
  issn         = {1097-2765},
  number       = {2},
  pages        = {241--247},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10339817/},
  urldate      = {2025-09-03},
  volume       = {82},
  abstract     = {Quantitative optical microscopy—an emerging, transformative approach to single-cell biology—has seen dramatic methodological advancements over the past few years. However, its impact has been hampered by challenges in the areas of data generation, management, and analysis. Here we outline these technical and cultural challenges and provide our perspective on the trajectory of this field, ushering in a new era of quantitative, data-driven microscopy. We also contrast it to the three decades of enormous advances in the field of genomics that have significantly enhanced the reproducibility and wider adoption of a plethora of genomic approaches.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC10339817/:text/html;PubMed Central Full Text PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC10339817/pdf/nihms-1911499.pdf:application/pdf},
  pmcid        = {PMC10339817},
  pmid         = {35063094},
}

@Article{Liu2021,
  author       = {Liu, Zhichao and Jin, Luhong and Chen, Jincheng and Fang, Qiuyu and Ablameyko, Sergey and Yin, Zhaozheng and Xu, Yingke},
  date         = {2021-07},
  journaltitle = {Computers in Biology and Medicine},
  title        = {A survey on applications of deep learning in microscopy image analysis},
  doi          = {10.1016/j.compbiomed.2021.104523},
  issn         = {0010-4825},
  pages        = {104523},
  url          = {https://www.sciencedirect.com/science/article/pii/S0010482521003176},
  urldate      = {2025-09-03},
  volume       = {134},
  abstract     = {Advanced microscopy enables us to acquire quantities of time-lapse images to visualize the dynamic characteristics of tissues, cells or molecules. Microscopy images typically vary in signal-to-noise ratios and include a wealth of information which require multiple parameters and time-consuming iterative algorithms for processing. Precise analysis and statistical quantification are often needed for the understanding of the biological mechanisms underlying these dynamic image sequences, which has become a big challenge in the field. As deep learning technologies develop quickly, they have been applied in bioimage processing more and more frequently. Novel deep learning models based on convolution neural networks have been developed and illustrated to achieve inspiring outcomes. This review article introduces the applications of deep learning algorithms in microscopy image analysis, which include image classification, region segmentation, object tracking and super-resolution reconstruction. We also discuss the drawbacks of existing deep learning-based methods, especially on the challenges of training datasets acquisition and evaluation, and propose the potential solutions. Furthermore, the latest development of augmented intelligent microscopy that based on deep learning technology may lead to revolution in biomedical research.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0010482521003176/pdfft?download=true:application/pdf},
  keywords     = {Deep learning, Neural network, Image processing, Super-resolution microscopy},
}

@Article{Hirabayashi2024,
  author       = {Hirabayashi, Yu and Iga, Haruka and Ogawa, Hiroki and Tokuta, Shinnosuke and Shimada, Yusuke and Yamamoto, Akiyasu},
  date         = {2024-03},
  journaltitle = {npj Computational Materials},
  title        = {Deep learning for three-dimensional segmentation of electron microscopy images of complex ceramic materials},
  doi          = {10.1038/s41524-024-01226-5},
  issn         = {2057-3960},
  language     = {en},
  number       = {1},
  pages        = {46},
  url          = {https://www.nature.com/articles/s41524-024-01226-5},
  urldate      = {2025-09-03},
  volume       = {10},
  abstract     = {The microstructure is a critical factor governing the functionality of ceramic materials. Meanwhile, microstructural analysis of electron microscopy images of polycrystalline ceramics, which are geometrically complex and composed of countless crystal grains with porosity and secondary phases, has generally been performed manually by human experts. Objective pixel-based analysis (semantic segmentation) with high accuracy is a simple but critical step for quantifying microstructures. In this study, we apply neural network-based semantic segmentation to secondary electron images of polycrystalline ceramics obtained by three-dimensional (3D) imaging. The deep-learning-based models (e.g., fully convolutional network and U-Net) by employing a dataset based on a 3D scanning electron microscopy with a focused ion beam is found to be able to recognize defect structures characteristic of polycrystalline materials in some cases due to artifacts in electron microscopy imaging. Owing to the training images with improved depth accuracy, the accuracy evaluation function, intersection over union (IoU) values, reaches 94.6\% for U-Net. These IoU values are among the highest for complex ceramics, where the 3D spatial distribution of phases is difficult to locate from a 2D image. Moreover, we employ the learned model to successfully reconstruct a 3D microstructure consisting of giga-scale voxel data in a few minutes. The resolution of a single voxel is 20 nm, which is higher than that obtained using a typical X-ray computed tomography. These results suggest that deep learning with datasets that learn depth information is essential in 3D microstructural quantifying polycrystalline ceramic materials. Additionally, developing improved segmentation models and datasets will pave the way for data assimilation into operando analysis and numerical simulations of in situ microstructures obtained experimentally and for application to process informatics.},
  copyright    = {2024 The Author(s)},
  file         = {Full Text PDF:https\://www.nature.com/articles/s41524-024-01226-5.pdf:application/pdf},
  keywords     = {Chemical engineering, Scanning electron microscopy, Synthesis and processing},
  publisher    = {Nature Publishing Group},
}

@Article{James2023,
  author       = {James, Jim and Pruyne, Nathan and Stan, Tiberiu and Schwarting, Marcus and Yeom, Jiwon and Hong, Seungbum and Voorhees, Peter and Blaiszik, Ben and Foster, Ian},
  date         = {2023-01},
  journaltitle = {Computational Materials Science},
  title        = {Segmentation of tomography datasets using {3D} convolutional neural networks},
  doi          = {10.1016/j.commatsci.2022.111847},
  issn         = {0927-0256},
  pages        = {111847},
  url          = {https://www.sciencedirect.com/science/article/pii/S0927025622005584},
  urldate      = {2025-09-03},
  volume       = {216},
  abstract     = {Dendritic microstructures are ubiquitous in nature and are the primary solidification morphologies in metallic materials. Techniques such as X-ray computed tomography (XCT) have provided new insights into dendritic phase transformation phenomena. However, manual identification of dendritic morphologies in microscopy data can be both labor intensive and potentially ambiguous. The analysis of 3D datasets is particularly challenging due to their large sizes (terabytes) and the presence of artifacts scattered within the imaged volumes. In this study, we trained 3D convolutional neural networks (CNNs) to segment 3D datasets. Three CNN architectures were investigated, including a new version of FCDenseNet which we extended to 3D. We show that using hyperparameter optimization (HPO) and fine-tuning techniques, both 2D and 3D CNN architectures outperform the previous state of the art. The 3D U-Net architecture trained in this study produced the best segmentations according to quantitative metrics (intersection-over-union of 95.56\% and a boundary displacement error of 0.58 pixels), while 3D FCDense produced the smoothest boundaries and best segmentations according to visual inspection. The trained 3D CNNs are able to segment entire 852 × 852 × 250 voxel 3D volumes in only ∼60 s, thus hastening the progress towards a deeper understanding of phase transformation phenomena such as dendritic solidification.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0927025622005584/pdfft?download=true:application/pdf},
  keywords     = {Artificial neural networks, X-ray computed tomography, Dendritic formation, Solidification microstructure, 3D image analysis},
}

@Article{Chan2020,
  author       = {Chan, Henry and Cherukara, Mathew and Loeffler, Troy D. and Narayanan, Badri and Sankaranarayanan, Subramanian K. R. S.},
  date         = {2020-01},
  journaltitle = {npj Computational Materials},
  title        = {Machine learning enabled autonomous microstructural characterization in {3D} samples},
  doi          = {10.1038/s41524-019-0267-z},
  issn         = {2057-3960},
  language     = {en},
  number       = {1},
  pages        = {1},
  url          = {https://www.nature.com/articles/s41524-019-0267-z},
  urldate      = {2025-09-03},
  volume       = {6},
  abstract     = {We introduce an unsupervised machine learning (ML) based technique for the identification and characterization of microstructures in three-dimensional (3D) samples obtained from molecular dynamics simulations, particle tracking data, or experiments. Our technique combines topology classification, image processing, and clustering algorithms, and can handle a wide range of microstructure types including grains in polycrystalline materials, voids in porous systems, and structures from self/directed assembly in soft-matter complex solutions. Our technique does not require a priori microstructure description of the target system and is insensitive to disorder such as extended defects in polycrystals arising from line and plane defects. We demonstrate quantitively that our technique provides unbiased microstructural information such as precise quantification of grains and their size distributions in 3D polycrystalline samples, characterizes features such as voids and porosity in 3D polymeric samples and micellar size distribution in 3D complex fluids. To demonstrate the efficacy of our ML approach, we benchmark it against a diverse set of synthetic data samples representing nanocrystalline metals, polymers and complex fluids as well as experimentally published characterization data. Our technique is computationally efficient and provides a way to quickly identify, track, and quantify complex microstructural features that impact the observed material behavior.},
  copyright    = {2020 This is a U.S Government work and not under copyright protection in the U.S; foreign copyright protection may apply},
  file         = {Full Text PDF:https\://www.nature.com/articles/s41524-019-0267-z.pdf:application/pdf},
  keywords     = {Characterization and analytical techniques, Theory and computation},
  publisher    = {Nature Publishing Group},
}

@Article{Midgley2003,
  author       = {Midgley, P. A. and Weyland, M.},
  date         = {2003-09},
  journaltitle = {Ultramicroscopy},
  title        = {{3D} electron microscopy in the physical sciences: the development of {Z}-contrast and {EFTEM} tomography},
  doi          = {10.1016/S0304-3991(03)00105-0},
  issn         = {0304-3991},
  number       = {3},
  pages        = {413--431},
  series       = {Proceedings of the {International} {Workshop} on {Strategies} and {Advances} in {Atomic} {Level} {Spectroscopy} and {Analysis}},
  url          = {https://www.sciencedirect.com/science/article/pii/S0304399103001050},
  urldate      = {2025-09-03},
  volume       = {96},
  abstract     = {The rapid advances in nanotechnology and the ever decreasing size of features in the microelectronics industry brings with it the need for advanced characterisation with high spatial resolution in two and three dimensions. Stereo microscopy allows some insight into the three-dimensional nature of an object but for true quantitative analysis, one has to turn to tomography as a way to reconstruct a three-dimensional object from a series of two-dimensional projections (images). X-ray tomography allow structures to be imaged at relatively large length scales, atom probe tomography at the atomic level. Electron tomography offers an intermediate resolution (of about 1nm) with a field of view of hundreds of nm making it ideal for the characterisation of many nanoscale devices. Whilst electron tomography has been used in the biological sciences for more than 30 years, it is only now being applied to the physical sciences. In this paper, we review the status of electron tomography, describe the basis behind the technique and some of the practicalities of recording and analysing data for tomographic reconstruction, particularly in regard to solving three-dimensional problems that are encountered in materials science at the nanometre level. We present examples of how STEM dark-field imaging and energy-filtered TEM can be used successfully to examine nearly all types of specimens likely to be encountered by the physical scientist.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0304399103001050/pdfft?download=true:application/pdf},
  keywords     = {Electron tomography, Three-dimensional reconstruction, Energy-filtered imaging, STEM HAADF imaging},
  shorttitle   = {{3D} electron microscopy in the physical sciences},
}

@Article{Schermelleh2008,
  author       = {Schermelleh, Lothar and Carlton, Peter M. and Haase, Sebastian and Shao, Lin and Winoto, Lukman and Kner, Peter and Burke, Brian and Cardoso, M. Cristina and Agard, David A. and Gustafsson, Mats G. L. and Leonhardt, Heinrich and Sedat, John W.},
  date         = {2008-06},
  journaltitle = {Science (New York, N.Y.)},
  title        = {Subdiffraction multicolor imaging of the nuclear periphery with {3D} structured illumination microscopy},
  doi          = {10.1126/science.1156947},
  issn         = {1095-9203},
  language     = {eng},
  number       = {5881},
  pages        = {1332--1336},
  volume       = {320},
  abstract     = {Fluorescence light microscopy allows multicolor visualization of cellular components with high specificity, but its utility has until recently been constrained by the intrinsic limit of spatial resolution. We applied three-dimensional structured illumination microscopy (3D-SIM) to circumvent this limit and to study the mammalian nucleus. By simultaneously imaging chromatin, nuclear lamina, and the nuclear pore complex (NPC), we observed several features that escape detection by conventional microscopy. We could resolve single NPCs that colocalized with channels in the lamin network and peripheral heterochromatin. We could differentially localize distinct NPC components and detect double-layered invaginations of the nuclear envelope in prophase as previously seen only by electron microscopy. Multicolor 3D-SIM opens new and facile possibilities to analyze subcellular structures beyond the diffraction limit of the emitted light.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/18535242:text/html},
  keywords     = {Animals, Cell Line, Cell Nucleus, Chromatin, Fluorescent Dyes, Heterochromatin, Imaging, Three-Dimensional, Indoles, Interphase, Lamins, Mice, Microscopy, Confocal, Microscopy, Fluorescence, Myoblasts, Nuclear Envelope, Nuclear Lamina, Nuclear Pore, Optics and Photonics},
  pmcid        = {PMC2916659},
  pmid         = {18535242},
}

@Article{Peng2015,
  author       = {Peng, Hong-Shang and T. Chiu, Daniel},
  date         = {2015},
  journaltitle = {Chemical Society Reviews},
  title        = {Soft fluorescent nanomaterials for biological and biomedical imaging},
  doi          = {10.1039/C4CS00294F},
  language     = {en},
  number       = {14},
  pages        = {4699--4722},
  url          = {https://pubs.rsc.org/en/content/articlelanding/2015/cs/c4cs00294f},
  urldate      = {2025-09-03},
  volume       = {44},
  file         = {Full Text PDF:https\://pubs.rsc.org/en/content/articlepdf/2015/cs/c4cs00294f:application/pdf},
  publisher    = {Royal Society of Chemistry},
}

@Article{Ali2012,
  author       = {Ali, Rehan and Gooding, Mark and Szilágyi, Tünde and Vojnovic, Borivoj and Christlieb, Martin and Brady, Michael},
  date         = {2012-07},
  journaltitle = {Machine Vision and Applications},
  title        = {Automatic segmentation of adherent biological cell boundaries and nuclei from brightfield microscopy images},
  doi          = {10.1007/s00138-011-0337-9},
  issn         = {1432-1769},
  language     = {en},
  number       = {4},
  pages        = {607--621},
  url          = {https://doi.org/10.1007/s00138-011-0337-9},
  urldate      = {2025-09-03},
  volume       = {23},
  abstract     = {The detection and segmentation of adherent eukaryotic cells from brightfield microscopy images represent challenging tasks in the image analysis field. This paper presents a free and open-source image analysis package which fully automates the tasks of cell detection, cell boundary segmentation, and nucleus segmentation in brightfield images. The package also performs image registration between brightfield and fluorescence images. The algorithms were evaluated on a variety of biological cell lines and compared against manual and fluorescence-based ground truths. When tested on HT1080 and HeLa cells, the cell detection step was able to correctly identify over 80\% of cells, whilst the cell boundary segmentation step was able to segment over 75\% of the cell body pixels, and the nucleus segmentation step was able to correctly identify nuclei in over 75\% of the cells. The algorithms for cell detection and nucleus segmentation are novel to the field, whilst the cell boundary segmentation algorithm is contrast-invariant, which makes it more robust on these low-contrast images. Together, this suite of algorithms permit brightfield microscopy image processing without the need for additional fluorescence images. Finally our sephaCe application, which is available at http://www.sephace.com, provides a novel method for integrating these methods with any motorised microscope, thus facilitating the adoption of these techniques in biological research labs.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs00138-011-0337-9.pdf:application/pdf},
  keywords     = {Segmentation, Registration, Cell detection, Level sets, Monogenic signal, Continuous intrinsic dimensionality},
}

@Article{Smith1981,
  author       = {Smith, P. R.},
  date         = {1981-01},
  journaltitle = {Ultramicroscopy},
  title        = {Bilinear interpolation of digital images},
  doi          = {10.1016/0304-3991(81)90061-9},
  issn         = {0304-3991},
  number       = {2},
  pages        = {201--204},
  url          = {https://www.sciencedirect.com/science/article/pii/0304399181900619},
  urldate      = {2025-09-05},
  volume       = {6},
  abstract     = {The application of the method of three-point bilinear interpolation is shown to generate a smoothly interpolated image, free from erroneous substructure generated by the interpolation scheme itself. Three-point interpolation is therefore to be preferred to the standard four-point bilinear scheme when images are prepared for film writing.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/0304399181900619/pdf?md5=7165c8ac5bee24d32362a6e1fbde40c1&pid=1-s2.0-0304399181900619-main.pdf&isDTMRedir=Y:application/pdf},
}

@InProceedings{Ioffe2015,
  author     = {Ioffe, Sergey and Szegedy, Christian},
  booktitle  = {International conference on machine learning},
  date       = {2015},
  title      = {Batch normalization: {Accelerating} deep network training by reducing internal covariate shift},
  pages      = {448--456},
  publisher  = {pmlr},
  url        = {http://proceedings.mlr.press/v37/ioffe15.html},
  urldate    = {2025-09-05},
  file       = {Available Version (via Google Scholar):http\://proceedings.mlr.press/v37/ioffe15.pdf:application/pdf},
  shorttitle = {Batch normalization},
}

@InProceedings{Santurkar2018,
  author    = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  date      = {2018},
  title     = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/905056c1ac1dad141560467e0a99e1cf-Abstract.html},
  urldate   = {2025-09-05},
  volume    = {31},
  abstract  = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs).
Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood.
The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift".
In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm.
Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother.
This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf:application/pdf},
}

@InProceedings{Xu2019,
  author    = {Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  date      = {2019},
  title     = {Understanding and {Improving} {Layer} {Normalization}},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/2f4fe03d77724a7217006e5d16728874-Abstract.html},
  urldate   = {2025-09-05},
  volume    = {32},
  abstract  = {Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm.  Many of previous studies believe that the success of LayerNorm comes from  forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version  of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. 
To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm  on seven out of eight datasets.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf:application/pdf},
}

@InCollection{Ghosh2020,
  author    = {Ghosh, Soumyadeep and Singh, Richa and Vatsa, Mayank and Ratha, Nalini and Patel, Vishal M.},
  date      = {2020},
  title     = {Domain {Adaptation} for {Visual} {Understanding}},
  doi       = {10.1007/978-3-030-30671-7_1},
  editor    = {Singh, Richa and Vatsa, Mayank and Patel, Vishal M. and Ratha, Nalini},
  isbn      = {9783030306717},
  language  = {en},
  location  = {Cham},
  pages     = {1--15},
  publisher = {Springer International Publishing},
  url       = {https://doi.org/10.1007/978-3-030-30671-7_1},
  urldate   = {2025-09-05},
  abstract  = {Advances in visual understanding in the last two decades have been aided by exemplary progress in machine learning and deep learningDeep learning methods. One of the principal issues of modern classifiers is generalization toward unseen testing data which may have a distribution different to that of the training set. Further, classifiers need to be adapted to scenarios where training data is made available online. Domain adaptationDomain adaptation based machine learning algorithms cater to these specific scenarios where the classifiers are updated for inclusivity and generalizability. Such methods need to encompass the covariate shift so that the trained model gives appreciable performance on the testing data. In this chapter, we categorize, illustrate, and analyze different domain adaptationDomain adaptation based machine learning algorithms for visual understanding.},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-3-030-30671-7_1.pdf:application/pdf},
  keywords  = {Domain adaptation, Deep learning, Image classification},
}

@InProceedings{Mao2023,
  author     = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  date       = {2023-07},
  title      = {Cross-{Entropy} {Loss} {Functions}: {Theoretical} {Analysis} and {Applications}},
  language   = {en},
  pages      = {23803--23828},
  publisher  = {PMLR},
  url        = {https://proceedings.mlr.press/v202/mao23b.html},
  urldate    = {2025-09-17},
  abstract   = {Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first 𝐻HH-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set 𝐻HH used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, smooth adversarial comp-sum losses, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit 𝐻HH-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.},
  file       = {Full Text PDF:https\://proceedings.mlr.press/v202/mao23b/mao23b.pdf:application/pdf},
  issn       = {2640-3498},
  shorttitle = {Cross-{Entropy} {Loss} {Functions}},
}

@InProceedings{GordonRodriguez2020,
  author     = {Gordon-Rodriguez, Elliott and Loaiza-Ganem, Gabriel and Pleiss, Geoff and Cunningham, John Patrick},
  date       = {2020-02},
  title      = {Uses and {Abuses} of the {Cross}-{Entropy} {Loss}: {Case} {Studies} in {Modern} {Deep} {Learning}},
  language   = {en},
  pages      = {1--10},
  publisher  = {PMLR},
  url        = {https://proceedings.mlr.press/v137/gordon-rodriguez20a.html},
  urldate    = {2025-09-17},
  abstract   = {Modern deep learning is primarily an experimental science, in which empirical advances occasionally come at the expense of probabilistic rigor. Here we focus on one such example; namely the use of the categorical cross-entropy loss to model data that is not strictly categorical, but rather takes values on the simplex. This practice is standard in neural network architectures with label smoothing and actor-mimic reinforcement learning, amongst others. Drawing on the recently discovered continuous-categorical distribution, we propose probabilistically-inspired alternatives to these models, providing an approach that is more principled and theoretically appealing. Through careful experimentation, including an ablation study, we identify the potential for outperformance in these models, thereby highlighting the importance of a proper probabilistic treatment, as well as illustrating some of the failure modes thereof.},
  file       = {Full Text PDF:http\://proceedings.mlr.press/v137/gordon-rodriguez20a/gordon-rodriguez20a.pdf:application/pdf;Supplementary PDF:http\://proceedings.mlr.press/v137/gordon-rodriguez20a/gordon-rodriguez20a-supp.pdf:application/pdf},
  issn       = {2640-3498},
  shorttitle = {Uses and {Abuses} of the {Cross}-{Entropy} {Loss}},
}

@InProceedings{Zhang2018,
  author    = {Zhang, Zhilu and Sabuncu, Mert},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  date      = {2018},
  title     = {Generalized {Cross} {Entropy} {Loss} for {Training} {Deep} {Neural} {Networks} with {Noisy} {Labels}},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/f2925f97bc13ad2852a7a551802feea0-Abstract.html},
  urldate   = {2025-09-17},
  volume    = {31},
  abstract  = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and large-scale datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2018/file/f2925f97bc13ad2852a7a551802feea0-Paper.pdf:application/pdf},
}

@InProceedings{Goldberger2017,
  author    = {Goldberger, Jacob and Ben-Reuven, Ehud},
  booktitle = {International conference on learning representations},
  date      = {2017},
  title     = {Training deep neural-networks using a noise adaptation layer},
  url       = {https://openreview.net/forum?id=H12GRgcxg},
  urldate   = {2025-09-17},
  file      = {Available Version (via Google Scholar):https\://openreview.net/pdf?id=H12GRgcxg:application/pdf},
}

@InProceedings{Han2018,
  author     = {Han, Bo and Yao, Jiangchao and Niu, Gang and Zhou, Mingyuan and Tsang, Ivor and Zhang, Ya and Sugiyama, Masashi},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  date       = {2018},
  title      = {Masking: {A} {New} {Perspective} of {Noisy} {Supervision}},
  publisher  = {Curran Associates, Inc.},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/aee92f16efd522b9326c25cc3237ac15-Abstract.html},
  urldate    = {2025-09-17},
  volume     = {31},
  abstract   = {It is important to learn various types of classifiers given training data with noisy labels. Noisy labels, in the most popular noise model hitherto, are corrupted from ground-truth labels by an unknown noise transition matrix. Thus, by estimating this matrix, classifiers can escape from overfitting those noisy labels. However, such estimation is practically difficult, due to either the indirect nature of two-step approaches, or not big enough data to afford end-to-end approaches. In this paper, we propose a human-assisted approach called ''Masking'' that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. To this end, we derive a structure-aware probabilistic model incorporating a structure prior, and solve the challenges from structure extraction and structure alignment. Thanks to Masking, we only estimate unmasked noise transition probabilities and the burden of estimation is tremendously reduced. We conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise structures as well as the industrial-level Clothing1M with agnostic noise structure, and the results show that Masking can improve the robustness of classifiers significantly.},
  file       = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2018/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf:application/pdf},
  shorttitle = {Masking},
}

@InProceedings{Hendrycks2018,
  author    = {Hendrycks, Dan and Mazeika, Mantas and Wilson, Duncan and Gimpel, Kevin},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  date      = {2018},
  title     = {Using {Trusted} {Data} to {Train} {Deep} {Networks} on {Labels} {Corrupted} by {Severe} {Noise}},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/ad554d8c3b06d6b97ee76a2448bd7913-Abstract.html},
  urldate   = {2025-09-17},
  volume    = {31},
  abstract  = {The growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling for large datasets, non-expert labeling, and label corruption by data poisoning adversaries. In the latter case, corruptions may be arbitrarily bad, even so bad that a classifier predicts the wrong labels with high confidence. To protect against such sources of noise, we leverage the fact that a small set of clean labels is often easy to procure. We demonstrate that robustness to label noise up to severe strengths can be achieved by using a set of trusted data with clean labels, and propose a loss correction that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper_files/paper/2018/file/ad554d8c3b06d6b97ee76a2448bd7913-Paper.pdf:application/pdf},
}

@InProceedings{Selvaraju2017,
  author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
  month     = {Oct},
  year      = {2017},
}

@Article{Maaten2008,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html},
  volume  = {9},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
}

@Article{Cai2022,
  author       = {Cai, T. Tony and Ma, Rong},
  date         = {2022},
  journaltitle = {Journal of Machine Learning Research},
  title        = {Theoretical {Foundations} of t-{SNE} for {Visualizing} {High}-{Dimensional} {Clustered} {Data}},
  issn         = {1533-7928},
  number       = {301},
  pages        = {1--54},
  url          = {http://jmlr.org/papers/v23/21-0524.html},
  urldate      = {2025-10-31},
  volume       = {23},
  abstract     = {This paper investigates the theoretical foundations of the t-distributed stochastic neighbor embedding (t-SNE) algorithm, a popular nonlinear dimension reduction and data visualization method. A novel theoretical framework for the analysis of t-SNE based on the gradient descent approach is presented. For the early exaggeration stage of t-SNE, we show its asymptotic equivalence to power iterations based on the underlying graph Laplacian, characterize its limiting behavior, and uncover its deep connection to Laplacian spectral clustering, and fundamental principles including early stopping as implicit regularization. The results explain the intrinsic mechanism and the empirical benefits of such a computational strategy. For the embedding stage of t-SNE, we characterize the kinematics of the low-dimensional map throughout the iterations, and identify an amplification phase, featuring the intercluster repulsion and the expansive behavior of the low-dimensional map, and a stabilization phase. The general theory explains the fast convergence rate and the exceptional empirical performance of t-SNE for visualizing clustered data, brings forth the interpretations of the t-SNE visualizations, and provides theoretical guidance for applying t-SNE and selecting its tuning parameters in various applications.},
  file         = {Full Text PDF:http\://jmlr.org/papers/volume23/21-0524/21-0524.pdf:application/pdf},
}

@Article{Avesta2023,
  author       = {Avesta, Arman and Hossain, Sajid and Lin, MingDe and Aboian, Mariam and Krumholz, Harlan M. and Aneja, Sanjay},
  date         = {2023-02},
  journaltitle = {Bioengineering},
  title        = {Comparing {3D}, 2.{5D}, and {2D} {Approaches} to {Brain} {Image} {Auto}-{Segmentation}},
  doi          = {10.3390/bioengineering10020181},
  issn         = {2306-5354},
  language     = {en},
  number       = {2},
  pages        = {181},
  url          = {https://www.mdpi.com/2306-5354/10/2/181},
  urldate      = {2025-10-31},
  volume       = {10},
  abstract     = {Deep-learning methods for auto-segmenting brain images either segment one slice of the image (2D), five consecutive slices of the image (2.5D), or an entire volume of the image (3D). Whether one approach is superior for auto-segmenting brain images is not known. We compared these three approaches (3D, 2.5D, and 2D) across three auto-segmentation models (capsule networks, UNets, and nnUNets) to segment brain structures. We used 3430 brain MRIs, acquired in a multi-institutional study, to train and test our models. We used the following performance metrics: segmentation accuracy, performance with limited training data, required computational memory, and computational speed during training and deployment. The 3D, 2.5D, and 2D approaches respectively gave the highest to lowest Dice scores across all models. 3D models maintained higher Dice scores when the training set size was decreased from 3199 MRIs down to 60 MRIs. 3D models converged 20\% to 40\% faster during training and were 30\% to 50\% faster during deployment. However, 3D models require 20 times more computational memory compared to 2.5D or 2D models. This study showed that 3D models are more accurate, maintain better performance with limited training data, and are faster to train and deploy. However, 3D models require more computational memory compared to 2.5D or 2D models.},
  copyright    = {http://creativecommons.org/licenses/by/3.0/},
  file         = {Full Text PDF:https\://www.mdpi.com/2306-5354/10/2/181/pdf?version=1675231805:application/pdf},
  keywords     = {auto-segmentation, deep learning, neuroimaging, magnetic resonance imaging},
  publisher    = {Multidisciplinary Digital Publishing Institute},
}

@InProceedings{Hung2024,
  author    = {Hung, Alex Ling Yu and Zheng, Haoxin and Zhao, Kai and Du, Xiaoxi and Pang, Kaifeng and Miao, Qi and Raman, Steven S and Terzopoulos, Demetri and Sung, Kyunghyun},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  title     = {Csam: A 2.5 d cross-slice attention module for anisotropic volumetric medical image segmentation},
  pages     = {5923--5932},
  year      = {2024},
}

@Article{Yang2021,
  author       = {Yang, Jiancheng and Huang, Xiaoyang and He, Yi and Xu, Jingwei and Yang, Canqian and Xu, Guozheng and Ni, Bingbing},
  date         = {2021-08},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  title        = {Reinventing 2D Convolutions for 3D Images},
  doi          = {10.1109/jbhi.2021.3049452},
  number       = {8},
  pages        = {3009-3018},
  volume       = {25},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@InBook{Arnab2021,
  author    = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lucic, Mario and Schmid, Cordelia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  date      = {2021-10},
  title     = {ViViT: A Video Vision Transformer},
  doi       = {10.1109/iccv48922.2021.00676},
  pages     = {6816-6826},
  publisher = {IEEE},
}

@Article{Wang2025,
  author       = {Wang, Yaoli and Deng, Yaojun and Zheng, Yuanjin and Chattopadhyay, Pratik and Wang, Lipo},
  date         = {2025-01},
  journaltitle = {Technologies},
  title        = {Vision {Transformers} for {Image} {Classification}: {A} {Comparative} {Survey}},
  doi          = {10.3390/technologies13010032},
  issn         = {2227-7080},
  language     = {en},
  number       = {1},
  pages        = {32},
  url          = {https://www.mdpi.com/2227-7080/13/1/32},
  urldate      = {2025-10-31},
  volume       = {13},
  abstract     = {Transformers were initially introduced for natural language processing, leveraging the self-attention mechanism. They require minimal inductive biases in their design and can function effectively as set-based architectures. Additionally, transformers excel at capturing long-range dependencies and enabling parallel processing, which allows them to outperform traditional models, such as long short-term memory (LSTM) networks, on sequence-based tasks. In recent years, transformers have been widely adopted in computer vision, driving remarkable advancements in the field. Previous surveys have provided overviews of transformer applications across various computer vision tasks, such as object detection, activity recognition, and image enhancement. In this survey, we focus specifically on image classification. We begin with an introduction to the fundamental concepts of transformers and highlight the first successful Vision Transformer (ViT). Building on the ViT, we review subsequent improvements and optimizations introduced for image classification tasks. We then compare the strengths and limitations of these transformer-based models against classic convolutional neural networks (CNNs) through experiments. Finally, we explore key challenges and potential future directions for image classification transformers.},
  copyright    = {http://creativecommons.org/licenses/by/3.0/},
  file         = {Full Text PDF:https\://www.mdpi.com/2227-7080/13/1/32/pdf?version=1736778259:application/pdf},
  keywords     = {computer vision, pattern recognition, artificial intelligence, machine learning},
  publisher    = {Multidisciplinary Digital Publishing Institute},
  shorttitle   = {Vision {Transformers} for {Image} {Classification}},
}

@Article{Avesta2023a,
  author       = {Avesta, A. and Hui, Y. and Aboian, M. and Duncan, J. and Krumholz, H.M. and Aneja, S.},
  date         = {2023-04},
  journaltitle = {American Journal of Neuroradiology},
  title        = {3D Capsule Networks for Brain Image Segmentation},
  doi          = {10.3174/ajnr.a7845},
  issn         = {1936-959X},
  number       = {5},
  pages        = {562--568},
  volume       = {44},
  publisher    = {American Society of Neuroradiology (ASNR)},
}

@Article{Karimi2024,
  author       = {Karimi, Hamed and Hamghalam, Mohammad},
  date         = {2024-03},
  journaltitle = {Multimedia Tools and Applications},
  title        = {Segmentation of {3D} {MRI} {Using} {2D} {Convolutional} {Neural} {Networks} in {Infants}’ {Brain}},
  doi          = {10.1007/s11042-023-16790-z},
  issn         = {1573-7721},
  language     = {en},
  number       = {11},
  pages        = {33511--33526},
  url          = {https://doi.org/10.1007/s11042-023-16790-z},
  urldate      = {2025-10-31},
  volume       = {83},
  abstract     = {Magnetic resonance (MR) imaging of the human brain poses significant challenges when it comes to segmenting the white matter, gray matter, and cerebrospinal fluid. This study presents a novel 2D model for segmenting 3D MR scans, utilizing 3D features, while maintaining a reduced number of model parameters compared to traditional 3D deep neural network models. The proposed method addresses the intensity contrast issue between white and gray matter in six- to nine-month-old infants by leveraging consecutive concatenation slices as a three-channel input image. Additionally, the combination of T1 and T2 weighted MR images for each patient reduces model complexity. Specifically, our study presents a 2D model capable of effectively segmenting MR images of the human brain, especially when there is a close contrast between white matter and gray matter. The combination of 3D features and a reduced parameter count improves segmentation accuracy. Our findings suggest the potential of our proposed method for the diagnosis of possible brain abnormalities. This will pave the way for more accurate and efficient medical image analysis in neuroimaging. To evaluate the effectiveness of our approach, an extensive evaluation was conducted on the iSeg-2017 datasets. The results demonstrated substantial improvements in the segmentation accuracy compared to other 2D techniques, especially in limited annotation settings. The proposed method achieved impressive Dice scores of 0.803, 0.817, and 0.907 for white matter, gray matter, and cerebrospinal fluid, respectively. Accordingly, these results demonstrate the efficiency of our 2D model in accurately segmenting brain tissue in MR images.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs11042-023-16790-z.pdf:application/pdf},
  keywords     = {Convolutional neural networks, Magnetic resonance imaging, Segmentation, 2D architecture, Infants’ Brain},
}

@Article{Zhang2022,
  author       = {Zhang, Yichi and Liao, Qingcheng and Ding, Le and Zhang, Jicong},
  date         = {2022-07},
  journaltitle = {Computerized Medical Imaging and Graphics},
  title        = {Bridging 2D and 3D segmentation networks for computation-efficient volumetric medical image segmentation: An empirical study of 2.5D solutions},
  doi          = {10.1016/j.compmedimag.2022.102088},
  pages        = {102088},
  volume       = {99},
  publisher    = {Elsevier BV},
}

@Article{Taha2015,
  author       = {Taha, Abdel Aziz and Hanbury, Allan},
  date         = {2015-08},
  journaltitle = {BMC Medical Imaging},
  title        = {Metrics for evaluating {3D} medical image segmentation: analysis, selection, and tool},
  doi          = {10.1186/s12880-015-0068-x},
  issn         = {1471-2342},
  number       = {1},
  pages        = {29},
  url          = {https://doi.org/10.1186/s12880-015-0068-x},
  urldate      = {2025-10-31},
  volume       = {15},
  abstract     = {Medical Image segmentation is an important image processing step. Comparing images to evaluate the quality of segmentation is an essential part of measuring progress in this research area. Some of the challenges in evaluating medical segmentation are: metric selection, the use in the literature of multiple definitions for certain metrics, inefficiency of the metric calculation implementations leading to difficulties with large volumes, and lack of support for fuzzy segmentation by existing metrics.},
  file         = {Full Text PDF:https\://bmcmedimaging.biomedcentral.com/counter/pdf/10.1186/s12880-015-0068-x:application/pdf},
  keywords     = {Evaluation metrics, Evaluation tool, Medical volume segmentation, Metric selection},
  shorttitle   = {Metrics for evaluating {3D} medical image segmentation},
}

@Article{Couturier2024,
  author       = {Couturier, Nathalie and Hörner, Sarah and Nürnberg, Elina and Joazeiro, Claudio and Hafner, Mathias and Rudolf, Rüdiger},
  date         = {2024-06-20},
  journaltitle = {Frontiers in Cell and Developmental Biology},
  title        = {Aberrant evoked calcium signaling and nAChR cluster morphology in a SOD1 D90A hiPSC-derived neuromuscular model},
  doi          = {10.3389/fcell.2024.1429759},
  pages        = {1429759},
  volume       = {12},
  day          = {20},
  month        = {6},
  publisher    = {Frontiers Media SA},
  year         = {2024},
}

@InProceedings{Carreira2017,
  author     = {Carreira, Joao and Zisserman, Andrew},
  date       = {2017},
  title      = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
  pages      = {6299--6308},
  url        = {https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html},
  urldate    = {2025-11-03},
  file       = {Full Text PDF:http\://openaccess.thecvf.com/content_cvpr_2017/papers/Carreira_Quo_Vadis_Action_CVPR_2017_paper.pdf:application/pdf},
  shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
}

@InProceedings{Tran2015,
  author  = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  date    = {2015},
  title   = {Learning {Spatiotemporal} {Features} {With} {3D} {Convolutional} {Networks}},
  pages   = {4489--4497},
  url     = {https://openaccess.thecvf.com/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html},
  urldate = {2025-11-03},
  file    = {Full Text PDF:http\://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf:application/pdf},
}

@InProceedings{Tran2018,
  author  = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  date    = {2018},
  title   = {A {Closer} {Look} at {Spatiotemporal} {Convolutions} for {Action} {Recognition}},
  pages   = {6450--6459},
  url     = {https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html},
  urldate = {2025-11-03},
  file    = {Full Text PDF:http\://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_A_Closer_Look_CVPR_2018_paper.pdf:application/pdf},
}

@InProceedings{Feichtenhofer2020,
  author     = {Feichtenhofer, Christoph},
  date       = {2020},
  title      = {{X3D}: {Expanding} {Architectures} for {Efficient} {Video} {Recognition}},
  pages      = {203--213},
  url        = {https://openaccess.thecvf.com/content_CVPR_2020/html/Feichtenhofer_X3D_Expanding_Architectures_for_Efficient_Video_Recognition_CVPR_2020_paper.html},
  urldate    = {2025-11-03},
  file       = {Full Text PDF:http\://openaccess.thecvf.com/content_CVPR_2020/papers/Feichtenhofer_X3D_Expanding_Architectures_for_Efficient_Video_Recognition_CVPR_2020_paper.pdf:application/pdf},
  shorttitle = {{X3D}},
}

@InProceedings{Wang2018,
  author  = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  date    = {2018},
  title   = {Non-{Local} {Neural} {Networks}},
  pages   = {7794--7803},
  url     = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html},
  urldate = {2025-11-03},
  file    = {Full Text PDF:http\://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf:application/pdf},
}

@Article{Bertasius2021,
  author       = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  date         = {2021},
  journaltitle = {Icml},
  title        = {Is space-time attention all you need for video understanding?},
  number       = {3},
  volume       = {2},
}

@Article{Belaid2009,
  author       = {Belaid, Lamia Jaafar and Mourou, Walid},
  date         = {2009},
  journaltitle = {Image Analysis and Stereology},
  title        = {{IMAGE} {SEGMENTATION}: {A} {WATERSHED} {TRANSFORMATION} {ALGORITHM}},
  doi          = {10.5566/ias.v28.p93-102},
  issn         = {1854-5165},
  language     = {en},
  number       = {2},
  pages        = {93--102},
  url          = {https://www.ias-iss.org/ojs/IAS/article/view/852},
  urldate      = {2025-11-05},
  volume       = {28},
  abstract     = {The goal of this work is to present a new method for image segmentation using mathematicalmorphology. The approach used is based on the watershed transformation. In order to avoid an oversegmentation, we propose to adapt the topological gradient method. The watershed transformation combined with a fast algorithm based on the topological gradient approach gives good results. The numerical tests obtained illustrate the efficiency of our approach for image segmentation.},
  copyright    = {Copyright (c) 2014 Image Analysis \& Stereology},
  file         = {Full Text PDF:https\://www.ias-iss.org/ojs/IAS/article/download/852/755:application/pdf},
  keywords     = {image segmentation, mathematical morphology, topological asymptotic expansion, topological gradient, watershed transformation},
  shorttitle   = {{IMAGE} {SEGMENTATION}},
}

@Comment{jabref-meta: databaseType:biblatex;}
